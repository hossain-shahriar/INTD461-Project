{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "102a7ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: CPU | Model: meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# 01 - Setup: device, imports, seeds, paths, model\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# ---- Single toggle: \"cpu\" or \"gpu\"\n",
    "RUN_DEVICE = \"cpu\"   # change to \"gpu\" if you have CUDA\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "if RUN_DEVICE.lower() == \"gpu\" and torch.cuda.is_available():\n",
    "    SELECT_DEVICE = \"cuda\"\n",
    "else:\n",
    "    SELECT_DEVICE = \"cpu\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "device = torch.device(SELECT_DEVICE)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "if device.type == \"cpu\":\n",
    "    torch.set_num_threads(max(1, os.cpu_count() // 2))\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "\n",
    "# Paths\n",
    "EXCEL_PATH = Path(\"Punjabi Dataset.xlsx\")\n",
    "OUT_DIR = Path(\"outputs_punjabi_metaphor\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "MAX_NEW_TOKENS = 1\n",
    "DO_SAMPLE = False\n",
    "\n",
    "print(f\"Using device: {device.type.upper()} | Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d475e147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (50, 5)\n",
      "Columns: ['ID', 'Metaphor_Used', 'Prev_Sentence', 'Target_Sentence', 'Final_Sentence']\n",
      "\n",
      "First 3 rows:\n",
      "   ID         Metaphor_Used                    Prev_Sentence  \\\n",
      "0   1  ਕਾਰ ਵਿਚ ਅੱਗ ਲੱਗ ਜਾਣਾ      ਚਾਹ ਕਾਫ਼ੀ ਠੰਢੀ ਹੋ ਚੁੱਕੀ ਸੀ।   \n",
      "1   2          ਕੂਏਂ ਦਾ ਮੱਛੀ  ਰਾਤ ਨੂੰ ਕੁੱਤਾ ਬਹੁਤ ਭੌਂਕਦਾ ਰਿਹਾ।   \n",
      "2   3          ਅੱਗ ਲੱਗ ਜਾਣਾ      ਚਾਹ ਕਾਫ਼ੀ ਠੰਢੀ ਹੋ ਚੁੱਕੀ ਸੀ।   \n",
      "\n",
      "                                     Target_Sentence  \\\n",
      "0  ਕਾਰ ਵਿਚ ਅੱਗ ਲੱਗ ਜਾਣਾ ਸੱਚਮੁੱਚ ਹੋਇਆ ਸੀ, ਜਿਵੇਂ ਕਿ...   \n",
      "1  ਕੂਏਂ ਦਾ ਮੱਛੀ ਸਿਰਫ਼ ਰੂਪਕ ਸੀ, ਪਰ ਇਸਦਾ ਮਤਲਬ ਕਾਫ਼ੀ...   \n",
      "2  ਅੱਗ ਲੱਗ ਜਾਣਾ ਸਿਰਫ਼ ਰੂਪਕ ਸੀ, ਪਰ ਇਸਦਾ ਮਤਲਬ ਕਾਫ਼ੀ...   \n",
      "\n",
      "                    Final_Sentence  \n",
      "0   ਫਿਰ ਉਸਨੇ ਘਰ ਜਾਣ ਦਾ ਫੈਸਲਾ ਕੀਤਾ।  \n",
      "1   ਫਿਰ ਸਾਰੇ ਆਪਣੇ ਕੰਮ ਵਿੱਚ ਲੱਗ ਗਏ।  \n",
      "2  ਉਸ ਤੋਂ ਬਾਅਦ ਮਾਹੌਲ ਸ਼ਾਂਤ ਹੋ ਗਿਆ।  \n",
      "\n",
      "Data types:\n",
      "ID                  int64\n",
      "Metaphor_Used      object\n",
      "Prev_Sentence      object\n",
      "Target_Sentence    object\n",
      "Final_Sentence     object\n",
      "dtype: object\n",
      "\n",
      "Label distribution:\n",
      "Label\n",
      "0    25\n",
      "1    25\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data ready for classification!\n"
     ]
    }
   ],
   "source": [
    "# 02 - Load and inspect data\n",
    "\n",
    "# Load the Punjabi metaphor dataset\n",
    "df = pd.read_excel(EXCEL_PATH)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df.head(3))\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "\n",
    "# For this dataset, we'll treat \"Metaphor_Used\" as the key metaphor expression\n",
    "# We need to assign binary labels (0=literal, 1=metaphorical)\n",
    "# Since we don't have ground truth labels, let's create a binary target based on sentence analysis\n",
    "\n",
    "# Create a simple feature: if the target sentence explicitly says \"रूपक\" (metaphor in Hindi)\n",
    "# or similar indicators, mark as 1 (metaphorical), else 0 (literal)\n",
    "\n",
    "df['Label'] = 0  # Default to literal\n",
    "# Mark as metaphorical if sentence mentions it's a metaphor\n",
    "df.loc[df['Target_Sentence'].str.contains('रूपक|ਰੂਪਕ|metaphor', case=False, na=False), 'Label'] = 1\n",
    "\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['Label'].value_counts())\n",
    "print(f\"\\nData ready for classification!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "331586b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Logged in to Hugging Face\n",
      "Loading tokenizer for meta-llama/Llama-3.2-3B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1073dec911748eba879da64ed5840a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model and tokenizer loaded!\n"
     ]
    }
   ],
   "source": [
    "# 03 - Load model and tokenizer\n",
    "\n",
    "import gc\n",
    "from huggingface_hub import login as hf_login\n",
    "from getpass import getpass\n",
    "\n",
    "# Optional HF login for gated models\n",
    "hf_token = os.getenv(\"HF_TOKEN\", \"\").strip()\n",
    "if not hf_token:\n",
    "    try:\n",
    "        hf_token = getpass(\"Enter HuggingFace token (or press Enter to skip): \").strip()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if hf_token:\n",
    "    try:\n",
    "        hf_login(token=hf_token)\n",
    "        print(\"✓ Logged in to Hugging Face\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: HF login failed: {e}\")\n",
    "\n",
    "_token_arg = {\"token\": hf_token} if hf_token else {}\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"Loading tokenizer for {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    "    **_token_arg\n",
    ")\n",
    "\n",
    "# Load model\n",
    "print(f\"Loading model...\")\n",
    "if device.type == \"cuda\":\n",
    "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        **_token_arg\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float32,\n",
    "        device_map={\"\": \"cpu\"},\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        **_token_arg\n",
    "    )\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Ensure pad token\n",
    "if tokenizer.pad_token_id is None:\n",
    "    if tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    else:\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"✓ Model and tokenizer loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e4bd99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prompt:\n",
      "Is the Punjabi metaphor/idiom used metaphorically (figuratively) in the sentence below? Respond with only '1' for metaphorical/figurative or '0' for literal.\n",
      "\n",
      "Metaphor: ਕਾਰ ਵਿਚ ਅੱਗ ਲੱਗ ਜਾਣਾ\n",
      "\n",
      "Previous Sentence: ਚਾਹ ਕਾਫ਼ੀ ਠੰਢੀ ਹੋ ਚੁੱਕੀ ਸੀ।\n",
      "Target Sentence: ਕਾਰ ਵਿਚ ਅੱਗ ਲੱਗ ਜਾਣਾ ਸੱਚਮੁੱਚ ਹੋਇਆ ਸੀ, ਜਿਵੇਂ ਕ...\n"
     ]
    }
   ],
   "source": [
    "# 04 - Prompt templates\n",
    "\n",
    "INSTR_ZS = (\n",
    "    \"Is the Punjabi metaphor/idiom used metaphorically (figuratively) in the sentence below? \"\n",
    "    \"Respond with only '1' for metaphorical/figurative or '0' for literal.\"\n",
    ")\n",
    "\n",
    "def make_prompt(metaphor: str, prev_sent: str, target_sent: str, next_sent: str) -> str:\n",
    "    return (\n",
    "        f\"{INSTR_ZS}\\n\\n\"\n",
    "        f\"Metaphor: {metaphor}\\n\\n\"\n",
    "        f\"Previous Sentence: {prev_sent}\\n\"\n",
    "        f\"Target Sentence: {target_sent}\\n\"\n",
    "        f\"Next Sentence: {next_sent}\\n\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "\n",
    "# Test prompt\n",
    "test_row = df.iloc[0]\n",
    "test_prompt = make_prompt(\n",
    "    test_row['Metaphor_Used'],\n",
    "    test_row['Prev_Sentence'],\n",
    "    test_row['Target_Sentence'],\n",
    "    test_row['Final_Sentence']\n",
    ")\n",
    "print(\"Sample prompt:\")\n",
    "print(test_prompt[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf5fd981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID for '0': 15\n",
      "Token ID for '1': 16\n",
      "Classification function ready!\n"
     ]
    }
   ],
   "source": [
    "# 05 - Classification function\n",
    "\n",
    "# Cache token IDs for \"0\" and \"1\"\n",
    "_id0 = None\n",
    "_id1 = None\n",
    "\n",
    "def _candidate_token_id_for_digit(d: str) -> Optional[int]:\n",
    "    ids = tokenizer.encode(d, add_special_tokens=False)\n",
    "    if len(ids) == 1: return ids[0]\n",
    "    ids = tokenizer.encode(\" \" + d, add_special_tokens=False)\n",
    "    if len(ids) == 1: return ids[0]\n",
    "    ids = tokenizer.encode(d + \"\\n\", add_special_tokens=False)\n",
    "    if len(ids) == 1: return ids[0]\n",
    "    return None\n",
    "\n",
    "def _init_digit_ids():\n",
    "    global _id0, _id1\n",
    "    if _id0 is None: _id0 = _candidate_token_id_for_digit(\"0\")\n",
    "    if _id1 is None: _id1 = _candidate_token_id_for_digit(\"1\")\n",
    "\n",
    "_init_digit_ids()\n",
    "print(f\"Token ID for '0': {_id0}\")\n",
    "print(f\"Token ID for '1': {_id1}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def classify_prompt(prompt: str) -> int:\n",
    "    \"\"\"Classify a single prompt by comparing logits of '0' and '1' tokens\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    logits = model(**inputs).logits\n",
    "    next_logits = logits[0, -1, :]\n",
    "    \n",
    "    if _id0 is not None and _id1 is not None:\n",
    "        logit0 = next_logits[_id0].item()\n",
    "        logit1 = next_logits[_id1].item()\n",
    "        return 1 if logit1 >= logit0 else 0\n",
    "    \n",
    "    # Fallback: generate 1 token and check content\n",
    "    gen = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=DO_SAMPLE,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    text = tokenizer.decode(gen[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True)\n",
    "    return 1 if \"1\" in text else 0\n",
    "\n",
    "print(\"Classification function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "532571e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting classification on 50 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ffb61e7b5e4452af5ac0e77acd6fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifying:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Classification complete!\n",
      "Predictions: {1: 50}\n"
     ]
    }
   ],
   "source": [
    "# 06 - Batch classification with progress\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def classify_batch(df_batch: pd.DataFrame) -> List[int]:\n",
    "    \"\"\"Classify a batch of samples\"\"\"\n",
    "    predictions = []\n",
    "    for _, row in tqdm(df_batch.iterrows(), total=len(df_batch), desc=\"Classifying\"):\n",
    "        prompt = make_prompt(\n",
    "            row['Metaphor_Used'],\n",
    "            row['Prev_Sentence'],\n",
    "            row['Target_Sentence'],\n",
    "            row['Final_Sentence']\n",
    "        )\n",
    "        pred = classify_prompt(prompt)\n",
    "        predictions.append(pred)\n",
    "    return predictions\n",
    "\n",
    "# Run classification on the full dataset\n",
    "print(f\"Starting classification on {len(df)} samples...\")\n",
    "y_pred = classify_batch(df)\n",
    "y_true = df['Label'].tolist()\n",
    "\n",
    "print(f\"\\n✓ Classification complete!\")\n",
    "print(f\"Predictions: {pd.Series(y_pred).value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f262086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Classification Results ===\n",
      "Macro F1 Score: 0.6667\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Literal     0.0000    0.0000    0.0000        25\n",
      "Metaphorical     0.5000    1.0000    0.6667        25\n",
      "\n",
      "    accuracy                         0.5000        50\n",
      "   macro avg     0.2500    0.5000    0.3333        50\n",
      "weighted avg     0.2500    0.5000    0.3333        50\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0 25]\n",
      " [ 0 25]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Results saved to outputs_punjabi_metaphor/punjabi_metaphor_predictions.csv\n",
      "\n",
      "Accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# 07 - Evaluation\n",
    "\n",
    "f1 = f1_score(y_true, y_pred, average=\"binary\")\n",
    "print(f\"\\n=== Classification Results ===\")\n",
    "print(f\"Macro F1 Score: {f1:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Literal', 'Metaphorical'], digits=4))\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Save results\n",
    "results_df = df.copy()\n",
    "results_df['Prediction'] = y_pred\n",
    "results_df['Correct'] = (results_df['Label'] == results_df['Prediction']).astype(int)\n",
    "\n",
    "results_path = OUT_DIR / \"punjabi_metaphor_predictions.csv\"\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"\\n✓ Results saved to {results_path}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nAccuracy: {results_df['Correct'].mean():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
