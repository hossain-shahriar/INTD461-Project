{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be3e8a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: CPU | Model: Qwen/Qwen2.5-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# 01 - Setup: device, imports, seeds, paths, model\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# ---- Single toggle: \"cpu\" or \"gpu\"\n",
    "RUN_DEVICE = \"cpu\"   # change to \"gpu\" if you have CUDA\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "\n",
    "if RUN_DEVICE.lower() == \"gpu\" and torch.cuda.is_available():\n",
    "    SELECT_DEVICE = \"cuda\"\n",
    "else:\n",
    "    SELECT_DEVICE = \"cpu\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "device = torch.device(SELECT_DEVICE)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "if device.type == \"cpu\":\n",
    "    torch.set_num_threads(max(1, os.cpu_count() // 2))\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "\n",
    "# Paths\n",
    "EXCEL_PATH = Path(\"Punjabi Dataset.xlsx\")\n",
    "OUT_DIR = Path(\"outputs_punjabi_metaphor\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model - UPDATED TO QWEN\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"  # <-- Changed\n",
    "# MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "MAX_NEW_TOKENS = 1\n",
    "DO_SAMPLE = False\n",
    "print(f\"Using device: {device.type.upper()} | Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b68c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (50, 5)\n",
      "Columns: ['ID', 'Metaphor_Used', 'Prev_Sentence', 'Target_Sentence', 'Final_Sentence']\n",
      "\n",
      "First 3 rows:\n",
      "   ID         Metaphor_Used                    Prev_Sentence  \\\n",
      "0   1  ਕਾਰ ਵਿਚ ਅੱਗ ਲੱਗ ਜਾਣਾ      ਚਾਹ ਕਾਫ਼ੀ ਠੰਢੀ ਹੋ ਚੁੱਕੀ ਸੀ।   \n",
      "1   2          ਕੂਏਂ ਦਾ ਮੱਛੀ  ਰਾਤ ਨੂੰ ਕੁੱਤਾ ਬਹੁਤ ਭੌਂਕਦਾ ਰਿਹਾ।   \n",
      "2   3          ਅੱਗ ਲੱਗ ਜਾਣਾ      ਚਾਹ ਕਾਫ਼ੀ ਠੰਢੀ ਹੋ ਚੁੱਕੀ ਸੀ।   \n",
      "\n",
      "                                     Target_Sentence  \\\n",
      "0  ਕਾਰ ਵਿਚ ਅੱਗ ਲੱਗ ਜਾਣਾ ਸੱਚਮੁੱਚ ਹੋਇਆ ਸੀ, ਜਿਵੇਂ ਕਿ...   \n",
      "1  ਕੂਏਂ ਦਾ ਮੱਛੀ ਸਿਰਫ਼ ਰੂਪਕ ਸੀ, ਪਰ ਇਸਦਾ ਮਤਲਬ ਕਾਫ਼ੀ...   \n",
      "2  ਅੱਗ ਲੱਗ ਜਾਣਾ ਸਿਰਫ਼ ਰੂਪਕ ਸੀ, ਪਰ ਇਸਦਾ ਮਤਲਬ ਕਾਫ਼ੀ...   \n",
      "\n",
      "                    Final_Sentence  \n",
      "0   ਫਿਰ ਉਸਨੇ ਘਰ ਜਾਣ ਦਾ ਫੈਸਲਾ ਕੀਤਾ।  \n",
      "1   ਫਿਰ ਸਾਰੇ ਆਪਣੇ ਕੰਮ ਵਿੱਚ ਲੱਗ ਗਏ।  \n",
      "2  ਉਸ ਤੋਂ ਬਾਅਦ ਮਾਹੌਲ ਸ਼ਾਂਤ ਹੋ ਗਿਆ।  \n",
      "\n",
      "Data types:\n",
      "ID                  int64\n",
      "Metaphor_Used      object\n",
      "Prev_Sentence      object\n",
      "Target_Sentence    object\n",
      "Final_Sentence     object\n",
      "dtype: object\n",
      "\n",
      "Label distribution:\n",
      "Label\n",
      "0    25\n",
      "1    25\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data ready for classification!\n"
     ]
    }
   ],
   "source": [
    "#02 - Load and inspect data\n",
    "\n",
    "# Load the Punjabi metaphor dataset\n",
    "df = pd.read_excel(EXCEL_PATH)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df.head(3))\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "\n",
    "# For this dataset, we'll treat \"Metaphor_Used\" as the key metaphor expression\n",
    "# We need to assign binary labels (0=literal, 1=metaphorical)\n",
    "# Since we don't have ground truth labels, let's create a binary target based on sentence analysis\n",
    "\n",
    "# Create a simple feature: if the target sentence explicitly says \"रूपक\" (metaphor in Hindi)\n",
    "# or similar indicators, mark as 1 (metaphorical), else 0 (literal)\n",
    "\n",
    "df['Label'] = 0  # Default to literal\n",
    "# Mark as metaphorical if sentence mentions it's a metaphor\n",
    "df.loc[df['Target_Sentence'].str.contains('रूपक|ਰੂਪਕ|metaphor', case=False, na=False), 'Label'] = 1\n",
    "\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['Label'].value_counts())\n",
    "print(f\"\\nData ready for classification!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0b65c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for Qwen/Qwen2.5-3B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13834580943472e953a5e971703bffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd82cdd6cab44f889c221582c48723b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08fb6c026828422ea41f619fa003d787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39dc8b5dfe24cbe8d9b2a59768ef183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cedc96275d141a9b53b48d7845d7103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ceb4a18bdf43e8a87ae6a6f1132226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b181b061f1b40c685ff5d1d3c2ac045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a49c09a366e40f382765f3950ec5677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3636e81c3a447b49b1d3967510bf12c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee1fe3bcd0f435ab1ec6d83e077300a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5326fece290d413f8fe1248cdc0ab9a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model and tokenizer loaded!\n"
     ]
    }
   ],
   "source": [
    "# 03 - Load model and tokenizer\n",
    "\n",
    "import gc\n",
    "from huggingface_hub import login as hf_login\n",
    "from getpass import getpass\n",
    "\n",
    "# Optional HF login for gated models\n",
    "hf_token = os.getenv(\"HF_TOKEN\", \"\").strip()\n",
    "if not hf_token:\n",
    "    try:\n",
    "        hf_token = getpass(\"Enter HuggingFace token (or press Enter to skip): \").strip()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if hf_token:\n",
    "    try:\n",
    "        hf_login(token=hf_token)\n",
    "        print(\"✓ Logged in to Hugging Face\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: HF login failed: {e}\")\n",
    "\n",
    "_token_arg = {\"token\": hf_token} if hf_token else {}\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"Loading tokenizer for {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    "    **_token_arg\n",
    ")\n",
    "\n",
    "# Load model\n",
    "print(f\"Loading model...\")\n",
    "if device.type == \"cuda\":\n",
    "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        **_token_arg\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float32,\n",
    "        device_map={\"\": \"cpu\"},\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        **_token_arg\n",
    "    )\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Ensure pad token\n",
    "if tokenizer.pad_token_id is None:\n",
    "    if tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    else:\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"✓ Model and tokenizer loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "540a8de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prompt (Qwen ChatML Format):\n",
      "<|im_start|>system\n",
      "You are a helpful and accurate classification model. Your task is to determine the usage of a Punjabi metaphor/idiom. You must only output '1' if the usage is metaphorical (figurative) or '0' if it is literal. Do not provide any explanation, text, or reasoning.<|im_end|>\n",
      "<|im_start|>user\n",
      "Is the Punjabi metaphor/idiom used metaphorically (figuratively) in the sentence below?\n",
      "\n",
      "Met...\n"
     ]
    }
   ],
   "source": [
    "# 04 - Prompt templates\n",
    "from typing import List, Dict\n",
    "\n",
    "# The instruction is now split into System and User roles for better model compliance.\n",
    "\n",
    "INSTR_ZS_SYSTEM = (\n",
    "    \"You are a helpful and accurate classification model. \"\n",
    "    \"Your task is to determine the usage of a Punjabi metaphor/idiom. \"\n",
    "    \"You must only output '1' if the usage is metaphorical (figurative) or '0' if it is literal. \"\n",
    "    \"Do not provide any explanation, text, or reasoning.\"\n",
    ")\n",
    "\n",
    "INSTR_ZS_USER_TEMPLATE = (\n",
    "    \"Is the Punjabi metaphor/idiom used metaphorically (figuratively) in the sentence below?\"\n",
    "    \"\\n\\nMetaphor: {metaphor}\\n\"\n",
    "    \"Previous Sentence: {prev_sent}\\n\"\n",
    "    \"Target Sentence: {target_sent}\\n\"\n",
    "    \"Next Sentence: {next_sent}\"\n",
    ")\n",
    "\n",
    "def make_prompt(metaphor: str, prev_sent: str, target_sent: str, next_sent: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates a prompt using the Qwen model's ChatML template.\n",
    "    \n",
    "    The tokenizer is assumed to be loaded and assigned to the global `tokenizer` variable.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Format the User content with the specific example\n",
    "    user_content = INSTR_ZS_USER_TEMPLATE.format(\n",
    "        metaphor=metaphor,\n",
    "        prev_sent=prev_sent,\n",
    "        target_sent=target_sent,\n",
    "        next_sent=next_sent\n",
    "    )\n",
    "\n",
    "    # 2. Structure the prompt using the standard Hugging Face ChatML format (used by Qwen)\n",
    "    messages: List[Dict[str, str]] = [\n",
    "        {\"role\": \"system\", \"content\": INSTR_ZS_SYSTEM},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "    # tokenizer.apply_chat_template handles the specific token wrappers for Qwen.\n",
    "    # add_generation_prompt=True adds the final '<|im_start|>assistant\\n' header, \n",
    "    # which primes the model to output the answer (the '0' or '1').\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return prompt_text\n",
    "\n",
    "# Test prompt\n",
    "test_row = df.iloc[0]\n",
    "test_prompt = make_prompt(\n",
    "    test_row['Metaphor_Used'],\n",
    "    test_row['Prev_Sentence'],\n",
    "    test_row['Target_Sentence'],\n",
    "    test_row['Final_Sentence'])\n",
    "    \n",
    "print(\"Sample prompt (Qwen ChatML Format):\")\n",
    "print(test_prompt[:400] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a067c5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID for '0': 15\n",
      "Token ID for '1': 16\n",
      "Classification function ready!\n"
     ]
    }
   ],
   "source": [
    "# 05 - Classification function\n",
    "\n",
    "# Cache token IDs for \"0\" and \"1\"\n",
    "_id0 = None\n",
    "_id1 = None\n",
    "\n",
    "def _candidate_token_id_for_digit(d: str) -> Optional[int]:\n",
    "    ids = tokenizer.encode(d, add_special_tokens=False)\n",
    "    if len(ids) == 1: return ids[0]\n",
    "    ids = tokenizer.encode(\" \" + d, add_special_tokens=False)\n",
    "    if len(ids) == 1: return ids[0]\n",
    "    ids = tokenizer.encode(d + \"\\n\", add_special_tokens=False)\n",
    "    if len(ids) == 1: return ids[0]\n",
    "    return None\n",
    "\n",
    "def _init_digit_ids():\n",
    "    global _id0, _id1\n",
    "    if _id0 is None: _id0 = _candidate_token_id_for_digit(\"0\")\n",
    "    if _id1 is None: _id1 = _candidate_token_id_for_digit(\"1\")\n",
    "\n",
    "_init_digit_ids()\n",
    "print(f\"Token ID for '0': {_id0}\")\n",
    "print(f\"Token ID for '1': {_id1}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def classify_prompt(prompt: str) -> int:\n",
    "    \"\"\"Classify a single prompt by comparing logits of '0' and '1' tokens\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    logits = model(**inputs).logits\n",
    "    next_logits = logits[0, -1, :]\n",
    "    \n",
    "    if _id0 is not None and _id1 is not None:\n",
    "        logit0 = next_logits[_id0].item()\n",
    "        logit1 = next_logits[_id1].item()\n",
    "        return 1 if logit1 >= logit0 else 0\n",
    "    \n",
    "    # Fallback: generate 1 token and check content\n",
    "    gen = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=DO_SAMPLE,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    text = tokenizer.decode(gen[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True)\n",
    "    return 1 if \"1\" in text else 0\n",
    "\n",
    "print(\"Classification function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b49e069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting classification on 50 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9874c02baf4ac8b69fa02d72dd9a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifying:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Classification complete!\n",
      "Predictions: {0: 31, 1: 19}\n"
     ]
    }
   ],
   "source": [
    "# 06 - Batch classification with progress\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def classify_batch(df_batch: pd.DataFrame) -> List[int]:\n",
    "    \"\"\"Classify a batch of samples\"\"\"\n",
    "    predictions = []\n",
    "    for _, row in tqdm(df_batch.iterrows(), total=len(df_batch), desc=\"Classifying\"):\n",
    "        prompt = make_prompt(\n",
    "            row['Metaphor_Used'],\n",
    "            row['Prev_Sentence'],\n",
    "            row['Target_Sentence'],\n",
    "            row['Final_Sentence']\n",
    "        )\n",
    "        pred = classify_prompt(prompt)\n",
    "        predictions.append(pred)\n",
    "    return predictions\n",
    "\n",
    "# Run classification on the full dataset\n",
    "print(f\"Starting classification on {len(df)} samples...\")\n",
    "y_pred = classify_batch(df)\n",
    "y_true = df['Label'].tolist()\n",
    "\n",
    "print(f\"\\n✓ Classification complete!\")\n",
    "print(f\"Predictions: {pd.Series(y_pred).value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aebe479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Classification Results ===\n",
      "Macro F1 Score: 0.6364\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Literal     0.6452    0.8000    0.7143        25\n",
      "Metaphorical     0.7368    0.5600    0.6364        25\n",
      "\n",
      "    accuracy                         0.6800        50\n",
      "   macro avg     0.6910    0.6800    0.6753        50\n",
      "weighted avg     0.6910    0.6800    0.6753        50\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[20  5]\n",
      " [11 14]]\n",
      "\n",
      "✓ Results saved to outputs_punjabi_metaphor/punjabi_metaphor_predictions_Qwen2_5_3B_Instruct.csv\n",
      "\n",
      "Accuracy: 0.6800\n"
     ]
    }
   ],
   "source": [
    "# 07 - Evaluation\n",
    "\n",
    "f1 = f1_score(y_true, y_pred, average=\"binary\")\n",
    "print(f\"\\n=== Classification Results ===\")\n",
    "print(f\"Macro F1 Score: {f1:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Literal', 'Metaphorical'], digits=4))\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Save results\n",
    "results_df = df.copy()\n",
    "results_df['Prediction'] = y_pred\n",
    "results_df['Correct'] = (results_df['Label'] == results_df['Prediction']).astype(int)\n",
    "\n",
    "# --- Naming the output file differently (using the model name) ---\n",
    "model_tag = MODEL_NAME.split('/')[-1].replace('-', '_').replace('.', '_') # e.g., \"Qwen_2_5_3B_Instruct\"\n",
    "results_path = OUT_DIR / f\"punjabi_metaphor_predictions_{model_tag}.csv\" # <-- Updated File Name\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"\\n✓ Results saved to {results_path}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nAccuracy: {results_df['Correct'].mean():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
