{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d074b74c",
   "metadata": {
    "id": "d074b74c"
   },
   "source": [
    "## Setup: device, imports, seeds, paths, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02e5e7f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02e5e7f6",
    "outputId": "9ce49bc4-437d-4789-a819-8550b92a57ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6029407/mhossai6/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: CUDA | Model: meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# 01 - Setup: device, imports, seeds, paths, model\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# ---- Single toggle: \"cpu\" or \"gpu\"\n",
    "RUN_DEVICE = \"gpu\"   # change to \"cpu\" to force CPU\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "if RUN_DEVICE.lower() == \"gpu\" and torch.cuda.is_available():\n",
    "    SELECT_DEVICE = \"cuda\"\n",
    "else:\n",
    "    SELECT_DEVICE = \"cpu\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "device = torch.device(SELECT_DEVICE)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "if device.type == \"cpu\":\n",
    "    torch.set_num_threads(max(1, os.cpu_count() // 2))\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "\n",
    "DATA_DIR = Path(\"SemEval_2022_Task2-idiomaticity/SubTaskA\")\n",
    "TRAIN_ONE_SHOT = DATA_DIR / \"Data\" / \"train_one_shot.csv\"\n",
    "TRAIN_ZERO_SHOT = DATA_DIR / \"Data\" / \"train_zero_shot.csv\"\n",
    "DEV = DATA_DIR / \"Data\" / \"dev.csv\"\n",
    "DEV_GOLD = DATA_DIR / \"Data\" / \"dev_gold.csv\"\n",
    "EVAL = DATA_DIR / \"Data\" / \"eval.csv\"\n",
    "\n",
    "OUT_DIR = Path(\"outputs_pt_llm\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Choose one instruct model that fits RAM; both OK on CPU/GPU:\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# generation caps (we classify via logits; generation is a tiny fallback)\n",
    "MAX_NEW_TOKENS = 1\n",
    "DO_SAMPLE = False\n",
    "\n",
    "print(f\"Using device: {device.type.upper()} | Model: {MODEL_NAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b292cbc",
   "metadata": {
    "id": "3b292cbc"
   },
   "source": [
    "## Data loading & utilities (PT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcca7eda",
   "metadata": {
    "id": "bcca7eda"
   },
   "outputs": [],
   "source": [
    "# 02 - Data loading & utilities (PT)\n",
    "\n",
    "def load_any_csv(path: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(path, sep=None, engine=\"python\", dtype=str)\n",
    "\n",
    "def ensure_label_int(df: pd.DataFrame, col=\"Label\") -> pd.DataFrame:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(int)\n",
    "    return df\n",
    "\n",
    "def load_train_dev(language=\"PT\", oneshot=True) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train_df = load_any_csv(TRAIN_ONE_SHOT if oneshot else TRAIN_ZERO_SHOT)\n",
    "    dev_df = load_any_csv(DEV)\n",
    "    gold_df = load_any_csv(DEV_GOLD)\n",
    "\n",
    "    train_df.columns = [c.strip() for c in train_df.columns]\n",
    "    dev_df.columns = [c.strip() for c in dev_df.columns]\n",
    "    gold_df.columns = [c.strip() for c in gold_df.columns]\n",
    "\n",
    "    train_df = train_df[train_df[\"Language\"] == language].copy()\n",
    "    dev_df = dev_df[dev_df[\"Language\"] == language].copy()\n",
    "\n",
    "    gold = gold_df[gold_df[\"Language\"] == language][[\"ID\",\"Label\"]].copy()\n",
    "    gold[\"ID\"] = gold[\"ID\"].astype(str)\n",
    "    dev_df[\"ID\"] = dev_df[\"ID\"].astype(str)\n",
    "    dev_lab = dev_df.merge(gold, on=\"ID\", how=\"left\")\n",
    "    dev_lab = ensure_label_int(dev_lab, \"Label\")\n",
    "    train_df = ensure_label_int(train_df, \"Label\")\n",
    "    return train_df, dev_lab\n",
    "\n",
    "def load_eval(language=\"PT\") -> pd.DataFrame:\n",
    "    df = load_any_csv(EVAL)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df[df[\"Language\"] == language].copy()\n",
    "\n",
    "def mark_first_case_insensitive(text: str, needle: str, ltag=\"<mwe>\", rtag=\"</mwe>\") -> str:\n",
    "    if not isinstance(text, str) or not isinstance(needle, str):\n",
    "        return text\n",
    "    lt = text.lower(); ln = needle.lower()\n",
    "    i = lt.find(ln)\n",
    "    if i == -1:\n",
    "        return text\n",
    "    return text[:i] + ltag + text[i:i+len(needle)] + rtag + text[i+len(needle):]\n",
    "\n",
    "def pack_context(prev: str, target: str, nxt: str, mwe: str) -> str:\n",
    "    prev = \"\" if pd.isna(prev) else prev\n",
    "    nxt = \"\" if pd.isna(nxt) else nxt\n",
    "    tgt = mark_first_case_insensitive(target, mwe)\n",
    "    # Keep labels in English for stability; content is PT\n",
    "    return f\"Previous: {prev}\\nTarget: {tgt}\\nNext: {nxt}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d4f14d",
   "metadata": {
    "id": "22d4f14d"
   },
   "source": [
    "## One-shot exemplar builders (PT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81c128fc",
   "metadata": {
    "id": "81c128fc"
   },
   "outputs": [],
   "source": [
    "# 03 - One-shot exemplar builders (PT)\n",
    "\n",
    "def build_oneshot_index(train_one_shot_pt: pd.DataFrame) -> Dict[str, Dict[int, Dict[str, str]]]:\n",
    "    idx = {}\n",
    "    for _, r in train_one_shot_pt.iterrows():\n",
    "        mwe = r[\"MWE\"]\n",
    "        y = int(r[\"Label\"])\n",
    "        ctx = pack_context(r.get(\"Previous\",\"\"), r.get(\"Target\",\"\"), r.get(\"Next\",\"\"), mwe)\n",
    "        idx.setdefault(mwe, {})\n",
    "        if y not in idx[mwe]:  # first seen per label\n",
    "            idx[mwe][y] = {\"context\": ctx, \"label\": y}\n",
    "    return idx\n",
    "\n",
    "def pick_global_oneshot_fallback(train_one_shot_pt: pd.DataFrame) -> Dict[int, Dict[str, str]]:\n",
    "    pool = {0: None, 1: None}\n",
    "    for _, r in train_one_shot_pt.iterrows():\n",
    "        y = int(r[\"Label\"])\n",
    "        if pool[y] is None:\n",
    "            mwe = r[\"MWE\"]\n",
    "            ctx = pack_context(r.get(\"Previous\",\"\"), r.get(\"Target\",\"\"), r.get(\"Next\",\"\"), mwe)\n",
    "            pool[y] = {\"context\": ctx, \"label\": y, \"mwe\": mwe}\n",
    "        if pool[0] is not None and pool[1] is not None:\n",
    "            break\n",
    "    return pool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e24940",
   "metadata": {
    "id": "24e24940"
   },
   "source": [
    "## Load LLM (CPU/GPU) and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c68a046e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183,
     "referenced_widgets": [
      "84de982992894a7ba4c6dec2943888d9",
      "31fde20972ad4e78ad8cd0e89289c58d",
      "f9d303c051e34c8495855b86289ce7f3",
      "d4626a0d29b44e0fa66d641c64b1a0c4",
      "b2844ce261524a8f830f764f998c31c1",
      "c2884f3b09ef41799c4695aabb5f94d9",
      "d8c96b07667f41ef97b5d95d7b6a3d78",
      "1c5167a36d334e249e08ee63d121e32a",
      "a7a80a753ca54ad18e82fddaeb25837d",
      "055cbbf7f78b4925a00e754b7d88dd22",
      "0ffb3645429243e4948a9f0c08ebb49c",
      "0c19b7126be64a5da30df9c66253ebd8",
      "101833cfd18d4780887a7530d443a9d0",
      "80790a8e3f544f6bbc67e05dcb4c93d2",
      "01034b69a12c44c581db210261793736",
      "49cd7b195d9943ee9272736df23bc02b",
      "872c8febf46f464c8ddf89d283a58df7",
      "1496f9e90e814472b1377dcedc995de2",
      "238709e9c9ad4d89958c6fabb4ec1f9e",
      "580ea7835cd94f6ca58acf1c1b38b7e9",
      "c594196261f248b59a1b24518c53d891",
      "9f7b51e89689465ca1db88ae3c6d3726"
     ]
    },
    "id": "c68a046e",
    "outputId": "5af94cba-4981-4874-ddc4-ae51bb1e25ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Hugging Face token (press Enter to skip):  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA allocated: 0.00 GiB | reserved: 0.00 GiB\n",
      "VRAM cleanup done.\n",
      "CUDA allocated: 0.00 GiB | reserved: 0.00 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA allocated: 5.98 GiB | reserved: 6.00 GiB\n"
     ]
    }
   ],
   "source": [
    "# 04 - Load LLM (CPU/GPU) and tokenizer with HF login + VRAM-safe setup\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "from huggingface_hub import login as hf_login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# ----- VRAM helpers (safe on CPU too)\n",
    "def free_vram():\n",
    "    for k, v in list(globals().items()):\n",
    "        try:\n",
    "            import torch.nn as nn\n",
    "            if isinstance(v, nn.Module):\n",
    "                del globals()[k]\n",
    "        except Exception:\n",
    "            pass\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            del globals()[k]\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    print(\"VRAM cleanup done.\")\n",
    "\n",
    "def cuda_mem():\n",
    "    if torch.cuda.is_available():\n",
    "        a = torch.cuda.memory_allocated() / (1024**3)\n",
    "        r = torch.cuda.memory_reserved() / (1024**3)\n",
    "        print(f\"CUDA allocated: {a:.2f} GiB | reserved: {r:.2f} GiB\")\n",
    "    else:\n",
    "        print(\"CUDA not available.\")\n",
    "\n",
    "# Prefer segmented allocator on CUDA to reduce fragmentation\n",
    "if device.type == \"cuda\":\n",
    "    os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "\n",
    "# ----- Optional Hugging Face login (supports gated repos)\n",
    "hf_token = os.getenv(\"HF_TOKEN\", \"\").strip()\n",
    "if not hf_token:\n",
    "    try:\n",
    "        hf_token = getpass(\"Enter your Hugging Face token (press Enter to skip): \").strip()\n",
    "    except Exception:\n",
    "        hf_token = input(\"Enter your Hugging Face token (press Enter to skip): \").strip()\n",
    "if hf_token:\n",
    "    try:\n",
    "        hf_login(token=hf_token)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Hugging Face login failed: {e}\")\n",
    "\n",
    "_token_arg = {\"token\": hf_token} if hf_token else {}\n",
    "\n",
    "# ----- Clean up any previous models before loading a new one\n",
    "if device.type == \"cuda\":\n",
    "    cuda_mem()\n",
    "free_vram()\n",
    "if device.type == \"cuda\":\n",
    "    cuda_mem()\n",
    "\n",
    "# ----- Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    "    **_token_arg\n",
    ")\n",
    "\n",
    "# ----- Choose dtype & placement to avoid OOM on Colab T4\n",
    "# On GPU: bf16 if supported, else fp16, with device_map=\"auto\" to spread layers if needed.\n",
    "# On CPU: fp32.\n",
    "if device.type == \"cuda\":\n",
    "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\",            # lets HF place on GPU (and CPU if needed) to fit memory\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        **_token_arg\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float32,\n",
    "        device_map={\"\": \"cpu\"},\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        **_token_arg\n",
    "    )\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# ----- Ensure pad token and attention_mask behavior are well-defined\n",
    "def _ensure_pad():\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        if tokenizer.eos_token_id is not None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        else:\n",
    "            tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "_ensure_pad()\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    cuda_mem()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8c5f38",
   "metadata": {
    "id": "3c8c5f38"
   },
   "source": [
    "## Minimal prompts (compact, predictable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "526917e8",
   "metadata": {
    "id": "526917e8"
   },
   "outputs": [],
   "source": [
    "# 05 - Minimal prompts (compact, predictable)\n",
    "\n",
    "INSTR_ZS = (\n",
    "    \"Decide if the MWE is used idiomatically (figurative) in the Target sentence.\\n\"\n",
    "    \"Output only one digit: 1 (idiomatic) or 0 (literal).\"\n",
    ")\n",
    "\n",
    "def make_zero_shot_prompt(mwe: str, ctx_block: str) -> str:\n",
    "    return (\n",
    "        f\"{INSTR_ZS}\\n\"\n",
    "        f\"MWE: {mwe}\\n\"\n",
    "        f\"{ctx_block}\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "\n",
    "INSTR_1S = (\n",
    "    \"You will see two labeled examples for the SAME MWE, then a new instance.\\n\"\n",
    "    \"Output only one digit: 1 (idiomatic) or 0 (literal).\"\n",
    ")\n",
    "\n",
    "def make_one_shot_prompt(mwe: str, pos_example: str, neg_example: str, ctx_block: str) -> str:\n",
    "    return (\n",
    "        f\"{INSTR_1S}\\n\"\n",
    "        f\"Example+(Label=1)\\nMWE: {mwe}\\n{pos_example}\\n\"\n",
    "        f\"Example-(Label=0)\\nMWE: {mwe}\\n{neg_example}\\n\"\n",
    "        f\"Now classify the new instance.\\n\"\n",
    "        f\"MWE: {mwe}\\n{ctx_block}\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6bb555",
   "metadata": {
    "id": "eb6bb555"
   },
   "source": [
    "## Batched 0/1 classification via next-token logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db4838a7",
   "metadata": {
    "id": "db4838a7"
   },
   "outputs": [],
   "source": [
    "# 06 - Batched 0/1 classification via next-token logits\n",
    "\n",
    "BATCH_GEN = 8  # adjust for memory/speed\n",
    "\n",
    "def _apply_chat_or_plain_batch(texts: List[str]) -> dict:\n",
    "    # Build input_ids and attention_mask with correct padding on CPU/GPU\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        messages_batch = [[\n",
    "            {\"role\": \"system\", \"content\": \"You are a concise assistant. Respond with 0 or 1.\"},\n",
    "            {\"role\": \"user\", \"content\": t}\n",
    "        ] for t in texts]\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages_batch,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "    else:\n",
    "        input_ids = tokenizer(\n",
    "            texts, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        ).input_ids\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "    return {\n",
    "        \"input_ids\": input_ids.to(device, non_blocking=(device.type==\"cuda\")),\n",
    "        \"attention_mask\": attention_mask.to(device, non_blocking=(device.type==\"cuda\"))\n",
    "    }\n",
    "\n",
    "# Cache token ids for digits \"0\" and \"1\"\n",
    "_id0 = None\n",
    "_id1 = None\n",
    "\n",
    "def _candidate_token_id_for_digit(d: str) -> Optional[int]:\n",
    "    ids = tokenizer.encode(d, add_special_tokens=False)\n",
    "    if len(ids) == 1: return ids[0]\n",
    "    ids = tokenizer.encode(\" \" + d, add_special_tokens=False)\n",
    "    if len(ids) == 1: return ids[0]\n",
    "    ids = tokenizer.encode(d + \"\\n\", add_special_tokens=False)\n",
    "    if len(ids) == 1: return ids[0]\n",
    "    return None\n",
    "\n",
    "def _init_digit_ids():\n",
    "    global _id0, _id1\n",
    "    if _id0 is None: _id0 = _candidate_token_id_for_digit(\"0\")\n",
    "    if _id1 is None: _id1 = _candidate_token_id_for_digit(\"1\")\n",
    "\n",
    "_init_digit_ids()\n",
    "\n",
    "@torch.no_grad()\n",
    "def classify_prompts_logits(prompts: List[str]) -> List[int]:\n",
    "    enc = _apply_chat_or_plain_batch(prompts)\n",
    "    logits = model(**enc).logits          # [B, T, V]\n",
    "    next_logits = logits[:, -1, :]        # [B, V]\n",
    "\n",
    "    if (_id0 is not None) and (_id1 is not None):\n",
    "        logit0 = next_logits[:, _id0]\n",
    "        logit1 = next_logits[:, _id1]\n",
    "        return (logit1 >= logit0).long().cpu().tolist()\n",
    "\n",
    "    # Fallback: force-generate 1 token and parse\n",
    "    gen = model.generate(\n",
    "        **enc,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=DO_SAMPLE,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    outs = []\n",
    "    for i in range(gen.size(0)):\n",
    "        cut = enc[\"input_ids\"][i].shape[-1]\n",
    "        new_ids = gen[i][cut:]\n",
    "        text = tokenizer.decode(new_ids, skip_special_tokens=True)\n",
    "        if \"0\" in text and \"1\" in text:\n",
    "            outs.append(1 if text.index(\"1\") < text.index(\"0\") else 0)\n",
    "        elif \"1\" in text:\n",
    "            outs.append(1)\n",
    "        elif \"0\" in text:\n",
    "            outs.append(0)\n",
    "        else:\n",
    "            outs.append(1)\n",
    "    return outs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a48a68",
   "metadata": {
    "id": "89a48a68"
   },
   "source": [
    "## Predictors (single-row helpers called by batch loops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b993579f",
   "metadata": {
    "id": "b993579f"
   },
   "outputs": [],
   "source": [
    "# 07 - Predictors (helpers building prompts per row)\n",
    "\n",
    "def _zero_shot_prompt_for_row(r) -> str:\n",
    "    mwe = r[\"MWE\"]\n",
    "    ctx = pack_context(r.get(\"Previous\",\"\"), r.get(\"Target\",\"\"), r.get(\"Next\",\"\"), mwe)\n",
    "    return make_zero_shot_prompt(mwe, ctx)\n",
    "\n",
    "def _one_shot_prompt_for_row(r, oneshot_index, global_pool) -> str:\n",
    "    mwe = r[\"MWE\"]\n",
    "    ctx = pack_context(r.get(\"Previous\",\"\"), r.get(\"Target\",\"\"), r.get(\"Next\",\"\"), mwe)\n",
    "    pos_ctx = oneshot_index.get(mwe, {}).get(1, {}).get(\"context\")\n",
    "    neg_ctx = oneshot_index.get(mwe, {}).get(0, {}).get(\"context\")\n",
    "    if pos_ctx is None: pos_ctx = global_pool[1][\"context\"]\n",
    "    if neg_ctx is None: neg_ctx = global_pool[0][\"context\"]\n",
    "    return make_one_shot_prompt(mwe, pos_ctx, neg_ctx, ctx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf592dd",
   "metadata": {
    "id": "5bf592dd"
   },
   "source": [
    "## Zero-shot (PT): dev eval + eval submission (progress + RESUME, batched, OOM-safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a78f5092",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "a78f5092",
    "outputId": "29a4885e-299e-4a5d-ecca-947d919db477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot PT (dev) | Resuming with 32 cached / 273 total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zero-shot PT (dev): 100%|██████████| 31/31 [00:18<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot PT (dev) | Newly computed: 241 | Cached at start: 32 | Total: 273 | Elapsed: 18.1s | 0.075s/example (new only)\n",
      "[LLM Zero-shot PT] Dev macro-F1: 0.5170\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5816    0.5325    0.5559       154\n",
      "           1     0.4545    0.5042    0.4781       119\n",
      "\n",
      "    accuracy                         0.5201       273\n",
      "   macro avg     0.5181    0.5183    0.5170       273\n",
      "weighted avg     0.5262    0.5201    0.5220       273\n",
      "\n",
      "[[82 72]\n",
      " [59 60]]\n",
      "Zero-shot PT (eval) | Resuming with 0 cached / 279 total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zero-shot PT (eval): 100%|██████████| 35/35 [00:14<00:00,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot PT (eval) | Newly computed: 279 | Cached at start: 0 | Total: 279 | Elapsed: 14.4s | 0.052s/example (new only)\n",
      "Wrote outputs_pt_llm/eval_submission_pt_llm_zeroshot.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 08 - Zero-shot (PT): dev eval + eval submission (progress + RESUME, batched, OOM-safe)\n",
    "\n",
    "from time import perf_counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# optional: use math SDPA kernels to reduce peak memory (slower but safer)\n",
    "try:\n",
    "    if device.type == \"cuda\":\n",
    "        torch.backends.cuda.enable_flash_sdp(False)\n",
    "        torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "        torch.backends.cuda.enable_math_sdp(True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def _load_cache(cache_path: Path) -> dict:\n",
    "    if cache_path.exists():\n",
    "        df = pd.read_csv(cache_path, dtype={\"ID\": str, \"Label\": int})\n",
    "        return dict(zip(df[\"ID\"].astype(str), df[\"Label\"].astype(int)))\n",
    "    return {}\n",
    "\n",
    "def _append_one(cache_path: Path, rec: tuple):\n",
    "    _id, _lab = rec\n",
    "    header_needed = not cache_path.exists()\n",
    "    with open(cache_path, \"a\") as f:\n",
    "        if header_needed: f.write(\"ID,Label\\n\")\n",
    "        f.write(f\"{_id},{int(_lab)}\\n\")\n",
    "\n",
    "def _classify_micro(prompts: list, micro_bs: int) -> list:\n",
    "    \"\"\"Run classification in small micro-batches, retrying on OOM by halving the batch size.\"\"\"\n",
    "    out = []\n",
    "    i = 0\n",
    "    bs = max(1, micro_bs)\n",
    "    while i < len(prompts):\n",
    "        step = min(bs, len(prompts) - i)\n",
    "        chunk = prompts[i:i+step]\n",
    "        try:\n",
    "            with torch.inference_mode():\n",
    "                out.extend(classify_prompts_logits(chunk))\n",
    "            i += step\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower() and step > 1:\n",
    "                if device.type == \"cuda\":\n",
    "                    torch.cuda.empty_cache(); torch.cuda.ipc_collect()\n",
    "                bs = max(1, step // 2)\n",
    "                continue\n",
    "            raise\n",
    "    return out\n",
    "\n",
    "def progressive_predict_zeroshot_batched(df: pd.DataFrame,\n",
    "                                         cache_path: Path,\n",
    "                                         desc: str,\n",
    "                                         batch_gen: int = None,\n",
    "                                         micro_bs: int = None) -> list:\n",
    "    df = df.copy(); df[\"ID\"] = df[\"ID\"].astype(str)\n",
    "    preds_map = _load_cache(cache_path)\n",
    "    done = set(preds_map.keys())\n",
    "    todo_idx = [i for i, _id in enumerate(df[\"ID\"]) if _id not in done]\n",
    "\n",
    "    # defaults tuned for T4; CPU can go larger\n",
    "    if batch_gen is None:\n",
    "        batch_gen = 8 if device.type == \"cuda\" else 16\n",
    "    if micro_bs is None:\n",
    "        micro_bs = 2 if device.type == \"cuda\" else 8\n",
    "\n",
    "    print(f\"{desc} | Resuming with {len(done)} cached / {len(df)} total\")\n",
    "    t0 = perf_counter()\n",
    "    for start in tqdm(range(0, len(todo_idx), batch_gen), desc=desc, leave=True):\n",
    "        rows = todo_idx[start:start+batch_gen]\n",
    "        prompts = []; ids = []\n",
    "        for j in rows:\n",
    "            r = df.iloc[j]\n",
    "            prompts.append(_zero_shot_prompt_for_row(r))\n",
    "            ids.append(r[\"ID\"])\n",
    "        if not prompts:\n",
    "            continue\n",
    "\n",
    "        labels = _classify_micro(prompts, micro_bs)\n",
    "        for _id, lab in zip(ids, labels):\n",
    "            preds_map[_id] = int(lab)\n",
    "            _append_one(cache_path, (_id, int(lab)))\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    elapsed = perf_counter() - t0\n",
    "    print(f\"{desc} | Newly computed: {len(todo_idx)} | Cached at start: {len(done)} | Total: {len(df)} | \"\n",
    "          f\"Elapsed: {elapsed:.1f}s | {(elapsed/max(1,len(todo_idx))):.3f}s/example (new only)\")\n",
    "    return [preds_map[str(i)] for i in df[\"ID\"]]\n",
    "\n",
    "# ---- DEV (zero-shot PT) ----\n",
    "train_0s_pt, dev_0s_pt = load_train_dev(language=\"PT\", oneshot=False)\n",
    "cache_dev_0s = OUT_DIR / \"cache_llm_zeroshot_dev_pt.csv\"\n",
    "yhat_dev_0s = progressive_predict_zeroshot_batched(dev_0s_pt, cache_dev_0s, desc=\"Zero-shot PT (dev)\")\n",
    "ytrue_dev_0s = dev_0s_pt[\"Label\"].tolist()\n",
    "f1_0s = f1_score(ytrue_dev_0s, yhat_dev_0s, average=\"macro\")\n",
    "print(f\"[LLM Zero-shot PT] Dev macro-F1: {f1_0s:.4f}\")\n",
    "print(classification_report(ytrue_dev_0s, yhat_dev_0s, digits=4))\n",
    "print(confusion_matrix(ytrue_dev_0s, yhat_dev_0s))\n",
    "\n",
    "# ---- EVAL (zero-shot PT) ----\n",
    "eval_pt = load_eval(language=\"PT\")\n",
    "cache_eval_0s = OUT_DIR / \"cache_llm_zeroshot_eval_pt.csv\"\n",
    "yhat_eval_0s = progressive_predict_zeroshot_batched(eval_pt, cache_eval_0s, desc=\"Zero-shot PT (eval)\")\n",
    "\n",
    "sub_0s = pd.DataFrame({\n",
    "    \"ID\": eval_pt[\"ID\"].astype(str),\n",
    "    \"Language\": eval_pt[\"Language\"],\n",
    "    \"Setting\": [\"zero_shot\"] * len(eval_pt),\n",
    "    \"Label\": yhat_eval_0s\n",
    "})\n",
    "sub_0s_path = OUT_DIR / \"eval_submission_pt_llm_zeroshot.csv\"\n",
    "sub_0s.to_csv(sub_0s_path, index=False)\n",
    "print(f\"Wrote {sub_0s_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217350f3",
   "metadata": {
    "id": "217350f3"
   },
   "source": [
    "## One-shot (PT): build exemplars, dev eval + eval submission (progress + RESUME, batched, OOM-safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6950641c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6950641c",
    "outputId": "dd228ace-4c2d-4917-8649-403f09688495"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-shot PT (dev) | Resuming with 0 cached / 273 total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One-shot PT (dev): 100%|██████████| 35/35 [00:34<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-shot PT (dev) | Newly computed: 273 | Cached at start: 0 | Total: 273 | Elapsed: 34.6s | 0.127s/example (new only)\n",
      "[LLM One-shot PT] Dev macro-F1: 0.4412\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5617    0.8571    0.6787       154\n",
      "           1     0.4211    0.1345    0.2038       119\n",
      "\n",
      "    accuracy                         0.5421       273\n",
      "   macro avg     0.4914    0.4958    0.4412       273\n",
      "weighted avg     0.5004    0.5421    0.4717       273\n",
      "\n",
      "[[132  22]\n",
      " [103  16]]\n",
      "One-shot PT (eval) | Resuming with 0 cached / 279 total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One-shot PT (eval): 100%|██████████| 35/35 [00:37<00:00,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-shot PT (eval) | Newly computed: 279 | Cached at start: 0 | Total: 279 | Elapsed: 37.8s | 0.135s/example (new only)\n",
      "Wrote outputs_pt_llm/eval_submission_pt_llm_oneshot.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 09 - One-shot (PT): build exemplars, dev eval + eval submission (progress + RESUME, batched, OOM-safe)\n",
    "\n",
    "from time import perf_counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def progressive_predict_oneshot_batched(df: pd.DataFrame,\n",
    "                                        oneshot_index: dict,\n",
    "                                        global_pool: dict,\n",
    "                                        cache_path: Path,\n",
    "                                        desc: str,\n",
    "                                        batch_gen: int = None,\n",
    "                                        micro_bs: int = None) -> list:\n",
    "    df = df.copy(); df[\"ID\"] = df[\"ID\"].astype(str)\n",
    "    preds_map = _load_cache(cache_path)\n",
    "    done = set(preds_map.keys())\n",
    "    todo_idx = [i for i, _id in enumerate(df[\"ID\"]) if _id not in done]\n",
    "\n",
    "    if batch_gen is None:\n",
    "        batch_gen = 8 if device.type == \"cuda\" else 16\n",
    "    if micro_bs is None:\n",
    "        micro_bs = 2 if device.type == \"cuda\" else 8\n",
    "\n",
    "    print(f\"{desc} | Resuming with {len(done)} cached / {len(df)} total\")\n",
    "    t0 = perf_counter()\n",
    "    for start in tqdm(range(0, len(todo_idx), batch_gen), desc=desc, leave=True):\n",
    "        rows = todo_idx[start:start+batch_gen]\n",
    "        prompts = []; ids = []\n",
    "        for j in rows:\n",
    "            r = df.iloc[j]\n",
    "            prompts.append(_one_shot_prompt_for_row(r, oneshot_index, global_pool))\n",
    "            ids.append(r[\"ID\"])\n",
    "        if not prompts:\n",
    "            continue\n",
    "\n",
    "        labels = _classify_micro(prompts, micro_bs)\n",
    "        for _id, lab in zip(ids, labels):\n",
    "            preds_map[_id] = int(lab)\n",
    "            _append_one(cache_path, (_id, int(lab)))\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    elapsed = perf_counter() - t0\n",
    "    print(f\"{desc} | Newly computed: {len(todo_idx)} | Cached at start: {len(done)} | Total: {len(df)} | \"\n",
    "          f\"Elapsed: {elapsed:.1f}s | {(elapsed/max(1,len(todo_idx))):.3f}s/example (new only)\")\n",
    "    return [preds_map[str(i)] for i in df[\"ID\"]]\n",
    "\n",
    "train_1s_pt, dev_1s_pt = load_train_dev(language=\"PT\", oneshot=True)\n",
    "oneshot_index = build_oneshot_index(train_1s_pt)\n",
    "global_pool = pick_global_oneshot_fallback(train_1s_pt)\n",
    "\n",
    "cache_dev_1s = OUT_DIR / \"cache_llm_oneshot_dev_pt.csv\"\n",
    "yhat_dev_1s = progressive_predict_oneshot_batched(dev_1s_pt, oneshot_index, global_pool, cache_dev_1s, desc=\"One-shot PT (dev)\")\n",
    "ytrue_dev_1s = dev_1s_pt[\"Label\"].tolist()\n",
    "f1_1s = f1_score(ytrue_dev_1s, yhat_dev_1s, average=\"macro\")\n",
    "print(f\"[LLM One-shot PT] Dev macro-F1: {f1_1s:.4f}\")\n",
    "print(classification_report(ytrue_dev_1s, yhat_dev_1s, digits=4))\n",
    "print(confusion_matrix(ytrue_dev_1s, yhat_dev_1s))\n",
    "\n",
    "eval_pt = load_eval(language=\"PT\")\n",
    "cache_eval_1s = OUT_DIR / \"cache_llm_oneshot_eval_pt.csv\"\n",
    "yhat_eval_1s = progressive_predict_oneshot_batched(eval_pt, oneshot_index, global_pool, cache_eval_1s, desc=\"One-shot PT (eval)\")\n",
    "\n",
    "sub_1s = pd.DataFrame({\n",
    "    \"ID\": eval_pt[\"ID\"].astype(str),\n",
    "    \"Language\": eval_pt[\"Language\"],\n",
    "    \"Setting\": [\"zero_shot\"] * len(eval_pt),\n",
    "    \"Label\": yhat_eval_1s\n",
    "})\n",
    "sub_1s_path = OUT_DIR / \"eval_submission_pt_llm_oneshot.csv\"\n",
    "sub_1s.to_csv(sub_1s_path, index=False)\n",
    "print(f\"Wrote {sub_1s_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64lHAyl6-R88",
   "metadata": {
    "id": "64lHAyl6-R88"
   },
   "source": [
    "## Save run metadata + show cache files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "AQyEVJ65-Q8t",
   "metadata": {
    "id": "AQyEVJ65-Q8t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume cache files (delete any to start fresh for that split):\n",
      "- outputs_pt_llm/cache_llm_zeroshot_dev_pt.csv\n",
      "- outputs_pt_llm/cache_llm_zeroshot_eval_pt.csv\n",
      "- outputs_pt_llm/cache_llm_oneshot_dev_pt.csv\n",
      "- outputs_pt_llm/cache_llm_oneshot_eval_pt.csv\n"
     ]
    }
   ],
   "source": [
    "# 10 - Save run metadata + show cache files\n",
    "\n",
    "with open(OUT_DIR / \"run_pt_llm.txt\", \"w\") as f:\n",
    "    f.write(f\"MODEL_NAME={MODEL_NAME}\\n\")\n",
    "    f.write(f\"DEVICE={device.type}\\n\")\n",
    "\n",
    "print(\"Resume cache files (delete any to start fresh for that split):\")\n",
    "for p in [\n",
    "    OUT_DIR / \"cache_llm_zeroshot_dev_pt.csv\",\n",
    "    OUT_DIR / \"cache_llm_zeroshot_eval_pt.csv\",\n",
    "    OUT_DIR / \"cache_llm_oneshot_dev_pt.csv\",\n",
    "    OUT_DIR / \"cache_llm_oneshot_eval_pt.csv\",\n",
    "]:\n",
    "    print(f\"- {p}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01034b69a12c44c581db210261793736": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c594196261f248b59a1b24518c53d891",
      "placeholder": "​",
      "style": "IPY_MODEL_9f7b51e89689465ca1db88ae3c6d3726",
      "value": " 189/189 [00:00&lt;00:00, 21.3kB/s]"
     }
    },
    "055cbbf7f78b4925a00e754b7d88dd22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c19b7126be64a5da30df9c66253ebd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_101833cfd18d4780887a7530d443a9d0",
       "IPY_MODEL_80790a8e3f544f6bbc67e05dcb4c93d2",
       "IPY_MODEL_01034b69a12c44c581db210261793736"
      ],
      "layout": "IPY_MODEL_49cd7b195d9943ee9272736df23bc02b"
     }
    },
    "0ffb3645429243e4948a9f0c08ebb49c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "101833cfd18d4780887a7530d443a9d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_872c8febf46f464c8ddf89d283a58df7",
      "placeholder": "​",
      "style": "IPY_MODEL_1496f9e90e814472b1377dcedc995de2",
      "value": "generation_config.json: 100%"
     }
    },
    "1496f9e90e814472b1377dcedc995de2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1c5167a36d334e249e08ee63d121e32a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "238709e9c9ad4d89958c6fabb4ec1f9e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31fde20972ad4e78ad8cd0e89289c58d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2884f3b09ef41799c4695aabb5f94d9",
      "placeholder": "​",
      "style": "IPY_MODEL_d8c96b07667f41ef97b5d95d7b6a3d78",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "49cd7b195d9943ee9272736df23bc02b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "580ea7835cd94f6ca58acf1c1b38b7e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "80790a8e3f544f6bbc67e05dcb4c93d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_238709e9c9ad4d89958c6fabb4ec1f9e",
      "max": 189,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_580ea7835cd94f6ca58acf1c1b38b7e9",
      "value": 189
     }
    },
    "84de982992894a7ba4c6dec2943888d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_31fde20972ad4e78ad8cd0e89289c58d",
       "IPY_MODEL_f9d303c051e34c8495855b86289ce7f3",
       "IPY_MODEL_d4626a0d29b44e0fa66d641c64b1a0c4"
      ],
      "layout": "IPY_MODEL_b2844ce261524a8f830f764f998c31c1"
     }
    },
    "872c8febf46f464c8ddf89d283a58df7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f7b51e89689465ca1db88ae3c6d3726": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a7a80a753ca54ad18e82fddaeb25837d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b2844ce261524a8f830f764f998c31c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2884f3b09ef41799c4695aabb5f94d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c594196261f248b59a1b24518c53d891": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4626a0d29b44e0fa66d641c64b1a0c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_055cbbf7f78b4925a00e754b7d88dd22",
      "placeholder": "​",
      "style": "IPY_MODEL_0ffb3645429243e4948a9f0c08ebb49c",
      "value": " 2/2 [00:07&lt;00:00,  3.26s/it]"
     }
    },
    "d8c96b07667f41ef97b5d95d7b6a3d78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9d303c051e34c8495855b86289ce7f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c5167a36d334e249e08ee63d121e32a",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a7a80a753ca54ad18e82fddaeb25837d",
      "value": 2
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
