{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ed75eda",
   "metadata": {},
   "source": [
    "## Shared setup: device, imports, paths, utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83bf33af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA H100 80GB HBM3 MIG 2g.20gb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6029407/mhossai6/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE=16, NUM_EPOCHS=3, LR=2e-05\n"
     ]
    }
   ],
   "source": [
    "# 01 - Shared setup: device, imports, paths, utilities\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "# ---- Device selection ----\n",
    "RUN_DEVICE = \"gpu\"  # \"gpu\" or \"cpu\"\n",
    "\n",
    "if RUN_DEVICE.lower() == \"gpu\" and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.set_num_threads(max(1, os.cpu_count() // 2))\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# ---- Seeds ----\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ---- Metrics ----\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "# ---- HF transformers ----\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "# ---- Data paths (SemEval SubTaskA for EN training) ----\n",
    "DATA_DIR = Path(\"SemEval_2022_Task2-idiomaticity/SubTaskA\")\n",
    "TRAIN_ONE_SHOT = DATA_DIR / \"Data\" / \"train_one_shot.csv\"\n",
    "TRAIN_ZERO_SHOT = DATA_DIR / \"Data\" / \"train_zero_shot.csv\"\n",
    "DEV = DATA_DIR / \"Data\" / \"dev.csv\"\n",
    "DEV_GOLD = DATA_DIR / \"Data\" / \"dev_gold.csv\"\n",
    "EVAL = DATA_DIR / \"Data\" / \"eval.csv\"  # not used here, but kept for completeness\n",
    "\n",
    "# ---- LRL data: single CSV with all low-resource languages ----\n",
    "LRL_CSV = Path(\"lrl_idioms.csv\")  # columns: ID,Language,MWE,Previous,Target,Next,Label\n",
    "\n",
    "# ---- Outputs ----\n",
    "OUT_DIR = Path(\"outputs_lrl_xlmr_en\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Model & training config ----\n",
    "MODEL_NAME = \"xlm-roberta-base\"\n",
    "NUM_LABELS = 2\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "LR = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "BATCH_SIZE_GPU = 16\n",
    "BATCH_SIZE_CPU = 8\n",
    "BATCH_SIZE = BATCH_SIZE_GPU if device.type == \"cuda\" else BATCH_SIZE_CPU\n",
    "\n",
    "print(f\"BATCH_SIZE={BATCH_SIZE}, NUM_EPOCHS={NUM_EPOCHS}, LR={LR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6528dc1d",
   "metadata": {},
   "source": [
    "## Data loading & context utilities (SemEval EN + LRL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78d0e78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LRL dataset (all languages):\n",
      "     ID Language                          MWE  \\\n",
      "0  bn_1       BN                   বৃষ্টির জল   \n",
      "1  bn_2       BN  সংসার সুখের হয় রমণীর গুণে ।   \n",
      "2  bn_3       BN                 যাচ্ছা তাই ।   \n",
      "3  bn_4       BN         উলুবনে মুক্ত ছড়ানো ।   \n",
      "4  bn_5       BN                     নিজের ঘর   \n",
      "\n",
      "                                            Previous  \\\n",
      "0                 আকাশে অনেকক্ষণ ধরে কালো মেঘ জমছিল।   \n",
      "1  ছোট বাসা, অল্প রোজগার—সব মিলিয়ে খুব অভাবের জীব...   \n",
      "2  পাড়ার ক্লাবের অনুষ্ঠানের দায়িত্ব যাঁর হাতে দেও...   \n",
      "3  প্রফেসর গভীর মনোযোগ দিয়ে কঠিন তত্ত্বগুলো বুঝিয়...   \n",
      "4   সারাদিন বাইরে থাকায় সে ভীষণ ক্লান্ত হয়ে গিয়েছিল।   \n",
      "\n",
      "                                              Target  \\\n",
      "0           হঠাৎ টুপটাপ করে বৃষ্টির জল পড়া শুরু হলো।   \n",
      "1  তবু হাসিমুখে সব সামলে রাখত তারা; লোকজন বলত, সং...   \n",
      "2  ফলে মঞ্চ, সাউন্ড, আলো—সব মিলিয়ে শেষ দিন একেবার...   \n",
      "3  কিন্তু ছাত্ররা মোবাইলে ব্যস্ত, কেউ শুনছেই না—এ...   \n",
      "4    বাড়ি ফিরে সে সোজা নিজের ঘরে গিয়ে দরজা বন্ধ করল।   \n",
      "\n",
      "                                                Next  Label  \\\n",
      "0  কিছুক্ষণের মধ্যেই রাস্তা ভিজে একেবারে চকচকে হয়...      0   \n",
      "1  পাড়ার সবাই ওদের দাম্পত্য দেখে মুগ্ধ হয়ে আশীর্ব...      1   \n",
      "2  অতিথিরা কেমন যেন মুখ চেপে হেসে অনুষ্ঠান শেষ হও...      1   \n",
      "3  শেষে তিনি বিরক্ত হয়ে ক্লাস বন্ধ করে দিয়ে বললেন...      1   \n",
      "4    ব্যাগটা রেখে একটু পরে বিছানায় শুয়ে বিশ্রাম নিল।      0   \n",
      "\n",
      "                                                text  \n",
      "0  Previous: আকাশে অনেকক্ষণ ধরে কালো মেঘ জমছিল।\\n...  \n",
      "1  Previous: ছোট বাসা, অল্প রোজগার—সব মিলিয়ে খুব ...  \n",
      "2  Previous: পাড়ার ক্লাবের অনুষ্ঠানের দায়িত্ব যাঁ...  \n",
      "3  Previous: প্রফেসর গভীর মনোযোগ দিয়ে কঠিন তত্ত্ব...  \n",
      "4  Previous: সারাদিন বাইরে থাকায় সে ভীষণ ক্লান্ত ...  \n",
      "Total LRL examples: 268\n",
      "\n",
      "LRL languages found: ['BN', 'ML', 'PA']\n"
     ]
    }
   ],
   "source": [
    "# 02 - Data loading & context utilities (SemEval EN + LRL)\n",
    "\n",
    "def load_any_csv(path: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(path, sep=None, engine=\"python\", dtype=str)\n",
    "\n",
    "def ensure_label_int(df: pd.DataFrame, col=\"Label\") -> pd.DataFrame:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(int)\n",
    "    return df\n",
    "\n",
    "def mark_first_case_insensitive(text: str, needle: str, ltag=\"<mwe>\", rtag=\"</mwe>\") -> str:\n",
    "    if not isinstance(text, str) or not isinstance(needle, str):\n",
    "        return text\n",
    "    lt = text.lower()\n",
    "    ln = needle.lower()\n",
    "    i = lt.find(ln)\n",
    "    if i == -1:\n",
    "        return text\n",
    "    return text[:i] + ltag + text[i:i+len(needle)] + rtag + text[i+len(needle):]\n",
    "\n",
    "def pack_context(prev: str, target: str, nxt: str, mwe: str) -> str:\n",
    "    prev = \"\" if pd.isna(prev) else prev\n",
    "    nxt = \"\" if pd.isna(nxt) else nxt\n",
    "    target = \"\" if pd.isna(target) else target\n",
    "    tgt_marked = mark_first_case_insensitive(target, mwe)\n",
    "    return f\"Previous: {prev}\\nTarget: {tgt_marked}\\nNext: {nxt}\"\n",
    "\n",
    "def prepare_supervised_frame(\n",
    "    train_path: Path,\n",
    "    dev_path: Path,\n",
    "    dev_gold_path: Path,\n",
    "    language: str,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Prepare train/dev frames for a given language from SemEval:\n",
    "    - Filter by Language column\n",
    "    - Merge dev with gold labels\n",
    "    - Build 'text' column using pack_context\n",
    "    \"\"\"\n",
    "    train_df = load_any_csv(train_path)\n",
    "    dev_df = load_any_csv(dev_path)\n",
    "    gold_df = load_any_csv(dev_gold_path)\n",
    "\n",
    "    # Strip column names\n",
    "    train_df.columns = [c.strip() for c in train_df.columns]\n",
    "    dev_df.columns = [c.strip() for c in dev_df.columns]\n",
    "    gold_df.columns = [c.strip() for c in gold_df.columns]\n",
    "\n",
    "    # Filter by language\n",
    "    train_df = train_df[train_df[\"Language\"] == language].copy()\n",
    "    dev_df = dev_df[dev_df[\"Language\"] == language].copy()\n",
    "\n",
    "    # Merge dev with gold labels\n",
    "    gold = gold_df[gold_df[\"Language\"] == language][[\"ID\", \"Label\"]].copy()\n",
    "    gold[\"ID\"] = gold[\"ID\"].astype(str)\n",
    "    dev_df[\"ID\"] = dev_df[\"ID\"].astype(str)\n",
    "    dev_lab = dev_df.merge(gold, on=\"ID\", how=\"left\")\n",
    "    dev_lab = ensure_label_int(dev_lab, \"Label\")\n",
    "    train_df = ensure_label_int(train_df, \"Label\")\n",
    "\n",
    "    # Build text column\n",
    "    def _build_text(row):\n",
    "        return pack_context(\n",
    "            row.get(\"Previous\", \"\"),\n",
    "            row.get(\"Target\", \"\"),\n",
    "            row.get(\"Next\", \"\"),\n",
    "            row.get(\"MWE\", \"\"),\n",
    "        )\n",
    "\n",
    "    train_df[\"text\"] = train_df.apply(_build_text, axis=1)\n",
    "    dev_lab[\"text\"] = dev_lab.apply(_build_text, axis=1)\n",
    "\n",
    "    return train_df, dev_lab\n",
    "\n",
    "def load_lrl_frame(lrl_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load LRL dataset lrl_idioms.csv and build 'text' column.\n",
    "    Columns expected: ID, Language, MWE, Previous, Target, Next, Label\n",
    "    \"\"\"\n",
    "    df = load_any_csv(lrl_path)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    df = ensure_label_int(df, \"Label\")\n",
    "\n",
    "    def _build_text(row):\n",
    "        return pack_context(\n",
    "            row.get(\"Previous\", \"\"),\n",
    "            row.get(\"Target\", \"\"),\n",
    "            row.get(\"Next\", \"\"),\n",
    "            row.get(\"MWE\", \"\"),\n",
    "        )\n",
    "\n",
    "    df[\"text\"] = df.apply(_build_text, axis=1)\n",
    "    return df\n",
    "\n",
    "# ---- Load LRL data once, get list of languages ----\n",
    "lrl_df = load_lrl_frame(LRL_CSV)\n",
    "print(\"Loaded LRL dataset (all languages):\")\n",
    "print(lrl_df.head())\n",
    "print(f\"Total LRL examples: {len(lrl_df)}\")\n",
    "\n",
    "lrl_languages = sorted(lrl_df[\"Language\"].dropna().unique().tolist())\n",
    "print(\"\\nLRL languages found:\", lrl_languages)\n",
    "\n",
    "if len(lrl_languages) == 0:\n",
    "    raise ValueError(\"No languages found in lrl_idioms.csv (Language column empty?).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372a56ce",
   "metadata": {},
   "source": [
    "## Dataset, model, dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f9ee7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03 - Dataset, model, dataloaders\n",
    "\n",
    "class IdiomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer, max_length: int = 256, with_labels: bool = True):\n",
    "        self.texts = df[\"text\"].tolist()\n",
    "        self.with_labels = with_labels\n",
    "        self.labels = df[\"Label\"].tolist() if with_labels and \"Label\" in df.columns else None\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        text = self.texts[idx]\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "        if self.with_labels and self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "def build_model_and_tokenizer(move_to_device: bool = True):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # Add MWE markers if not present\n",
    "    special_tokens = {\"additional_special_tokens\": [\"<mwe>\", \"</mwe>\"]}\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "    )\n",
    "\n",
    "    # Resize embeddings for new special tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if move_to_device:\n",
    "        model.to(device)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def make_loader(df: pd.DataFrame, tokenizer, batch_size: int, shuffle: bool,\n",
    "                max_length: int = 256, with_labels: bool = True):\n",
    "    ds = IdiomDataset(df, tokenizer, max_length=max_length, with_labels=with_labels)\n",
    "    return torch.utils.data.DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=2 if device.type == \"cuda\" else 0,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5781f1",
   "metadata": {},
   "source": [
    "## Training & evaluation helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8db7a3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04 - Training & evaluation functions\n",
    "\n",
    "def run_epoch(\n",
    "    model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    train: bool = True,\n",
    "    class_weights: torch.Tensor = None,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Run one epoch. Returns (avg_loss, macro_f1).\n",
    "    If train=False, no optimizer.step / scheduler.step is done.\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    if class_weights is not None:\n",
    "        class_weights = class_weights.to(device)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            if class_weights is not None:\n",
    "                log_probs = torch.log_softmax(logits, dim=-1)\n",
    "                loss = torch.nn.functional.nll_loss(\n",
    "                    log_probs,\n",
    "                    labels,\n",
    "                    weight=class_weights,\n",
    "                )\n",
    "            else:\n",
    "                loss = outputs.loss\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        all_y_true.extend(labels.detach().cpu().tolist())\n",
    "        all_y_pred.extend(preds.detach().cpu().tolist())\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(all_y_true))\n",
    "    macro_f1 = f1_score(all_y_true, all_y_pred, average=\"macro\")\n",
    "    return avg_loss, macro_f1\n",
    "\n",
    "def predict(model, dataloader) -> Tuple[List[int], List[int]]:\n",
    "    model.eval()\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            all_y_true.extend(labels.detach().cpu().tolist())\n",
    "            all_y_pred.extend(preds.detach().cpu().tolist())\n",
    "\n",
    "    return all_y_true, all_y_pred\n",
    "\n",
    "def predict_no_labels(model, dataloader) -> List[int]:\n",
    "    model.eval()\n",
    "    all_y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            all_y_pred.extend(preds.detach().cpu().tolist())\n",
    "\n",
    "    return all_y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b29b2d9",
   "metadata": {},
   "source": [
    "## EN→LRL ZERO-SHOT: train on EN train_zero_shot, test on LRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cd1a5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EN→LRL XLM-R (ZERO-SHOT SETTING: train_zero_shot.csv for EN)\n",
      "================================================================================\n",
      "EN (zero-shot) train size: 3327, EN dev size: 466\n",
      "EN (zero-shot) train label counts: {0: 1762, 1: 1565}\n",
      "EN (zero-shot) class weights: [0.9440976163450624, 1.0629392971246006]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 19:30:12.878823: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-03 19:30:12.891142: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764819012.901686 2481545 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764819012.904401 2481545 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764819012.914509 2481545 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764819012.914522 2481545 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764819012.914524 2481545 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764819012.914525 2481545 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-03 19:30:12.918492: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/3 (EN zero-shot training) ===\n",
      "[EN zero-shot train] loss=0.6002 macro-F1=0.6508\n",
      "[EN zero-shot dev]   loss=0.6785 macro-F1=0.7242\n",
      "New best EN dev macro-F1=0.7242 -> saved to outputs_lrl_xlmr_en/xlmr_en_zero_shot_best.pt\n",
      "\n",
      "=== Epoch 2/3 (EN zero-shot training) ===\n",
      "[EN zero-shot train] loss=0.1728 macro-F1=0.9523\n",
      "[EN zero-shot dev]   loss=1.0651 macro-F1=0.7782\n",
      "New best EN dev macro-F1=0.7782 -> saved to outputs_lrl_xlmr_en/xlmr_en_zero_shot_best.pt\n",
      "\n",
      "=== Epoch 3/3 (EN zero-shot training) ===\n",
      "[EN zero-shot train] loss=0.0680 macro-F1=0.9840\n",
      "[EN zero-shot dev]   loss=1.1448 macro-F1=0.7854\n",
      "New best EN dev macro-F1=0.7854 -> saved to outputs_lrl_xlmr_en/xlmr_en_zero_shot_best.pt\n",
      "\n",
      "Best EN dev macro-F1 (zero-shot): 0.7854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded EN zero-shot checkpoint from: outputs_lrl_xlmr_en/xlmr_en_zero_shot_best.pt\n"
     ]
    }
   ],
   "source": [
    "# 05 - EN→LRL ZERO-SHOT: train on EN train_zero_shot, test on LRL\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EN→LRL XLM-R (ZERO-SHOT SETTING: train_zero_shot.csv for EN)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ---- Prepare EN train/dev (zero-shot setting) ----\n",
    "train_en_0s, dev_en_0s = prepare_supervised_frame(\n",
    "    train_path=TRAIN_ZERO_SHOT,\n",
    "    dev_path=DEV,\n",
    "    dev_gold_path=DEV_GOLD,\n",
    "    language=\"EN\",\n",
    ")\n",
    "\n",
    "print(f\"EN (zero-shot) train size: {len(train_en_0s)}, EN dev size: {len(dev_en_0s)}\")\n",
    "\n",
    "# Class weights for EN zero-shot training\n",
    "label_counts_0s = train_en_0s[\"Label\"].value_counts().to_dict()\n",
    "print(\"EN (zero-shot) train label counts:\", label_counts_0s)\n",
    "total_0s = sum(label_counts_0s.values())\n",
    "class_weights_0s = [total_0s / (2.0 * label_counts_0s.get(i, 1)) for i in range(NUM_LABELS)]\n",
    "class_weights_0s_tensor = torch.tensor(class_weights_0s, dtype=torch.float)\n",
    "print(\"EN (zero-shot) class weights:\", class_weights_0s)\n",
    "\n",
    "# ---- Build model & tokenizer ----\n",
    "model_0s, tok_0s = build_model_and_tokenizer(move_to_device=True)\n",
    "\n",
    "# ---- EN dataloaders ----\n",
    "train_loader_en_0s = make_loader(\n",
    "    train_en_0s,\n",
    "    tok_0s,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    max_length=256,\n",
    "    with_labels=True,\n",
    ")\n",
    "dev_loader_en_0s = make_loader(\n",
    "    dev_en_0s,\n",
    "    tok_0s,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    max_length=256,\n",
    "    with_labels=True,\n",
    ")\n",
    "\n",
    "# ---- Optimizer & scheduler ----\n",
    "num_training_steps_0s = NUM_EPOCHS * len(train_loader_en_0s)\n",
    "optimizer_0s = torch.optim.AdamW(\n",
    "    model_0s.parameters(),\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "scheduler_0s = get_linear_schedule_with_warmup(\n",
    "    optimizer_0s,\n",
    "    num_warmup_steps=int(WARMUP_RATIO * num_training_steps_0s),\n",
    "    num_training_steps=num_training_steps_0s,\n",
    ")\n",
    "\n",
    "best_en_dev_f1_0s = -1.0\n",
    "best_path_en_0s = OUT_DIR / \"xlmr_en_zero_shot_best.pt\"\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{NUM_EPOCHS} (EN zero-shot training) ===\")\n",
    "    train_loss_0s, train_f1_0s = run_epoch(\n",
    "        model_0s,\n",
    "        train_loader_en_0s,\n",
    "        optimizer_0s,\n",
    "        scheduler_0s,\n",
    "        train=True,\n",
    "        class_weights=class_weights_0s_tensor,\n",
    "    )\n",
    "    print(f\"[EN zero-shot train] loss={train_loss_0s:.4f} macro-F1={train_f1_0s:.4f}\")\n",
    "\n",
    "    dev_loss_0s, dev_f1_0s = run_epoch(\n",
    "        model_0s,\n",
    "        dev_loader_en_0s,\n",
    "        optimizer=None,\n",
    "        scheduler=None,\n",
    "        train=False,\n",
    "        class_weights=None,\n",
    "    )\n",
    "    print(f\"[EN zero-shot dev]   loss={dev_loss_0s:.4f} macro-F1={dev_f1_0s:.4f}\")\n",
    "\n",
    "    if dev_f1_0s > best_en_dev_f1_0s:\n",
    "        best_en_dev_f1_0s = dev_f1_0s\n",
    "        torch.save(model_0s.state_dict(), best_path_en_0s)\n",
    "        print(f\"New best EN dev macro-F1={best_en_dev_f1_0s:.4f} -> saved to {best_path_en_0s}\")\n",
    "\n",
    "print(f\"\\nBest EN dev macro-F1 (zero-shot): {best_en_dev_f1_0s:.4f}\")\n",
    "\n",
    "# ---- Reload best EN zero-shot checkpoint for transfer to LRL ----\n",
    "model_0s_xfer, tok_0s_xfer = build_model_and_tokenizer(move_to_device=True)\n",
    "state_0s = torch.load(best_path_en_0s, map_location=device)\n",
    "model_0s_xfer.load_state_dict(state_0s)\n",
    "model_0s_xfer.to(device)\n",
    "model_0s_xfer.eval()\n",
    "print(f\"Loaded EN zero-shot checkpoint from: {best_path_en_0s}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b166d7e3",
   "metadata": {},
   "source": [
    "## Evaluate EN zero-shot model on all LRL languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4968efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EN-trained XLM-R (zero-shot) -> LRL language: BN\n",
      "================================================================================\n",
      "[EN-trained zero-shot -> BN] Macro-F1=0.3661\n",
      "[EN-trained zero-shot -> BN] Macro-recall=0.3563\n",
      "[EN-trained zero-shot -> BN] Accuracy=0.4762\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0877    0.1220    0.1020        41\n",
      "           1     0.6757    0.5906    0.6303       127\n",
      "\n",
      "    accuracy                         0.4762       168\n",
      "   macro avg     0.3817    0.3563    0.3661       168\n",
      "weighted avg     0.5322    0.4762    0.5013       168\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 5 36]\n",
      " [52 75]]\n",
      "\n",
      "Saved zero-shot predictions for BN to: outputs_lrl_xlmr_en/BN_xlmr_en_zero_shot_predictions.csv\n",
      "\n",
      "================================================================================\n",
      "EN-trained XLM-R (zero-shot) -> LRL language: ML\n",
      "================================================================================\n",
      "[EN-trained zero-shot -> ML] Macro-F1=0.2695\n",
      "[EN-trained zero-shot -> ML] Macro-recall=0.2762\n",
      "[EN-trained zero-shot -> ML] Accuracy=0.2800\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.1379    0.2667    0.1818        15\n",
      "           1     0.4762    0.2857    0.3571        35\n",
      "\n",
      "    accuracy                         0.2800        50\n",
      "   macro avg     0.3071    0.2762    0.2695        50\n",
      "weighted avg     0.3747    0.2800    0.3045        50\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 4 11]\n",
      " [25 10]]\n",
      "\n",
      "Saved zero-shot predictions for ML to: outputs_lrl_xlmr_en/ML_xlmr_en_zero_shot_predictions.csv\n",
      "\n",
      "================================================================================\n",
      "EN-trained XLM-R (zero-shot) -> LRL language: PA\n",
      "================================================================================\n",
      "[EN-trained zero-shot -> PA] Macro-F1=0.4156\n",
      "[EN-trained zero-shot -> PA] Macro-recall=0.5536\n",
      "[EN-trained zero-shot -> PA] Accuracy=0.5000\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4681    1.0000    0.6377        22\n",
      "           1     1.0000    0.1071    0.1935        28\n",
      "\n",
      "    accuracy                         0.5000        50\n",
      "   macro avg     0.7340    0.5536    0.4156        50\n",
      "weighted avg     0.7660    0.5000    0.3890        50\n",
      "\n",
      "Confusion matrix:\n",
      "[[22  0]\n",
      " [25  3]]\n",
      "\n",
      "Saved zero-shot predictions for PA to: outputs_lrl_xlmr_en/PA_xlmr_en_zero_shot_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# 06 - Evaluate EN-trained zero-shot model on all LRL languages\n",
    "\n",
    "zero_results = []\n",
    "\n",
    "for lang in lrl_languages:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"EN-trained XLM-R (zero-shot) -> LRL language: {lang}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    df_lang = lrl_df[lrl_df[\"Language\"] == lang].copy()\n",
    "    if df_lang.empty:\n",
    "        print(f\"No rows for language {lang}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Build dataloader for this language (with labels)\n",
    "    dev_loader_lang = make_loader(\n",
    "        df_lang,\n",
    "        tok_0s_xfer,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        max_length=256,\n",
    "        with_labels=True,\n",
    "    )\n",
    "\n",
    "    ytrue, ypred = predict(model_0s_xfer, dev_loader_lang)\n",
    "\n",
    "    f1_macro = f1_score(ytrue, ypred, average=\"macro\")\n",
    "    recall_macro = recall_score(ytrue, ypred, average=\"macro\")\n",
    "    acc = accuracy_score(ytrue, ypred)\n",
    "\n",
    "    print(f\"[EN-trained zero-shot -> {lang}] Macro-F1={f1_macro:.4f}\")\n",
    "    print(f\"[EN-trained zero-shot -> {lang}] Macro-recall={recall_macro:.4f}\")\n",
    "    print(f\"[EN-trained zero-shot -> {lang}] Accuracy={acc:.4f}\\n\")\n",
    "\n",
    "    print(\"Classification report:\")\n",
    "    print(classification_report(ytrue, ypred, digits=4))\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix(ytrue, ypred))\n",
    "\n",
    "    # Save per-language predictions\n",
    "    out_df = df_lang.copy()\n",
    "    out_df[\"PredLabel_EN_ZeroShot\"] = ypred\n",
    "    pred_path = OUT_DIR / f\"{lang}_xlmr_en_zero_shot_predictions.csv\"\n",
    "    out_df.to_csv(pred_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"\\nSaved zero-shot predictions for {lang} to: {pred_path}\")\n",
    "\n",
    "    zero_results.append({\n",
    "        \"Language\": lang,\n",
    "        \"NumExamples\": len(df_lang),\n",
    "        \"MacroF1_ZeroShot\": f1_macro,\n",
    "        \"MacroRecall_ZeroShot\": recall_macro,\n",
    "        \"Accuracy_ZeroShot\": acc,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e422c00",
   "metadata": {},
   "source": [
    "## EN→LRL ONE-SHOT: train on EN train_one_shot, test on LRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "023d5fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EN→LRL XLM-R (ONE-SHOT SETTING: train_one_shot.csv for EN)\n",
      "================================================================================\n",
      "EN (one-shot) train size: 87, EN dev size: 466\n",
      "EN (one-shot) train label counts: {1: 55, 0: 32}\n",
      "EN (one-shot) class weights: [1.359375, 0.7909090909090909]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/3 (EN one-shot training) ===\n",
      "[EN one-shot train] loss=0.7162 macro-F1=0.3562\n",
      "[EN one-shot dev]   loss=0.6819 macro-F1=0.4420\n",
      "New best EN dev macro-F1=0.4420 -> saved to outputs_lrl_xlmr_en/xlmr_en_one_shot_best.pt\n",
      "\n",
      "=== Epoch 2/3 (EN one-shot training) ===\n",
      "[EN one-shot train] loss=0.7117 macro-F1=0.5239\n",
      "[EN one-shot dev]   loss=0.6911 macro-F1=0.5857\n",
      "New best EN dev macro-F1=0.5857 -> saved to outputs_lrl_xlmr_en/xlmr_en_one_shot_best.pt\n",
      "\n",
      "=== Epoch 3/3 (EN one-shot training) ===\n",
      "[EN one-shot train] loss=0.7056 macro-F1=0.4386\n",
      "[EN one-shot dev]   loss=0.6903 macro-F1=0.5965\n",
      "New best EN dev macro-F1=0.5965 -> saved to outputs_lrl_xlmr_en/xlmr_en_one_shot_best.pt\n",
      "\n",
      "Best EN dev macro-F1 (one-shot): 0.5965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded EN one-shot checkpoint from: outputs_lrl_xlmr_en/xlmr_en_one_shot_best.pt\n"
     ]
    }
   ],
   "source": [
    "# 07 - EN→LRL ONE-SHOT: train on EN train_one_shot, test on LRL\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EN→LRL XLM-R (ONE-SHOT SETTING: train_one_shot.csv for EN)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ---- Prepare EN train/dev (one-shot setting) ----\n",
    "train_en_1s, dev_en_1s = prepare_supervised_frame(\n",
    "    train_path=TRAIN_ONE_SHOT,\n",
    "    dev_path=DEV,\n",
    "    dev_gold_path=DEV_GOLD,\n",
    "    language=\"EN\",\n",
    ")\n",
    "\n",
    "print(f\"EN (one-shot) train size: {len(train_en_1s)}, EN dev size: {len(dev_en_1s)}\")\n",
    "\n",
    "# Class weights for EN one-shot training\n",
    "label_counts_1s = train_en_1s[\"Label\"].value_counts().to_dict()\n",
    "print(\"EN (one-shot) train label counts:\", label_counts_1s)\n",
    "total_1s = sum(label_counts_1s.values())\n",
    "class_weights_1s = [total_1s / (2.0 * label_counts_1s.get(i, 1)) for i in range(NUM_LABELS)]\n",
    "class_weights_1s_tensor = torch.tensor(class_weights_1s, dtype=torch.float)\n",
    "print(\"EN (one-shot) class weights:\", class_weights_1s)\n",
    "\n",
    "# ---- Build model & tokenizer ----\n",
    "model_1s, tok_1s = build_model_and_tokenizer(move_to_device=True)\n",
    "\n",
    "# ---- EN dataloaders ----\n",
    "train_loader_en_1s = make_loader(\n",
    "    train_en_1s,\n",
    "    tok_1s,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    max_length=256,\n",
    "    with_labels=True,\n",
    ")\n",
    "dev_loader_en_1s = make_loader(\n",
    "    dev_en_1s,\n",
    "    tok_1s,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    max_length=256,\n",
    "    with_labels=True,\n",
    ")\n",
    "\n",
    "# ---- Optimizer & scheduler ----\n",
    "num_training_steps_1s = NUM_EPOCHS * len(train_loader_en_1s)\n",
    "optimizer_1s = torch.optim.AdamW(\n",
    "    model_1s.parameters(),\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "scheduler_1s = get_linear_schedule_with_warmup(\n",
    "    optimizer_1s,\n",
    "    num_warmup_steps=int(WARMUP_RATIO * num_training_steps_1s),\n",
    "    num_training_steps=num_training_steps_1s,\n",
    ")\n",
    "\n",
    "best_en_dev_f1_1s = -1.0\n",
    "best_path_en_1s = OUT_DIR / \"xlmr_en_one_shot_best.pt\"\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{NUM_EPOCHS} (EN one-shot training) ===\")\n",
    "    train_loss_1s, train_f1_1s = run_epoch(\n",
    "        model_1s,\n",
    "        train_loader_en_1s,\n",
    "        optimizer_1s,\n",
    "        scheduler_1s,\n",
    "        train=True,\n",
    "        class_weights=class_weights_1s_tensor,\n",
    "    )\n",
    "    print(f\"[EN one-shot train] loss={train_loss_1s:.4f} macro-F1={train_f1_1s:.4f}\")\n",
    "\n",
    "    dev_loss_1s, dev_f1_1s = run_epoch(\n",
    "        model_1s,\n",
    "        dev_loader_en_1s,\n",
    "        optimizer=None,\n",
    "        scheduler=None,\n",
    "        train=False,\n",
    "        class_weights=None,\n",
    "    )\n",
    "    print(f\"[EN one-shot dev]   loss={dev_loss_1s:.4f} macro-F1={dev_f1_1s:.4f}\")\n",
    "\n",
    "    if dev_f1_1s > best_en_dev_f1_1s:\n",
    "        best_en_dev_f1_1s = dev_f1_1s\n",
    "        torch.save(model_1s.state_dict(), best_path_en_1s)\n",
    "        print(f\"New best EN dev macro-F1={best_en_dev_f1_1s:.4f} -> saved to {best_path_en_1s}\")\n",
    "\n",
    "print(f\"\\nBest EN dev macro-F1 (one-shot): {best_en_dev_f1_1s:.4f}\")\n",
    "\n",
    "# ---- Reload best EN one-shot checkpoint for transfer to LRL ----\n",
    "model_1s_xfer, tok_1s_xfer = build_model_and_tokenizer(move_to_device=True)\n",
    "state_1s = torch.load(best_path_en_1s, map_location=device)\n",
    "model_1s_xfer.load_state_dict(state_1s)\n",
    "model_1s_xfer.to(device)\n",
    "model_1s_xfer.eval()\n",
    "print(f\"Loaded EN one-shot checkpoint from: {best_path_en_1s}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5085ba",
   "metadata": {},
   "source": [
    "## Evaluate EN one-shot model on all LRL languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c996d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EN-trained XLM-R (one-shot) -> LRL language: BN\n",
      "================================================================================\n",
      "[EN-trained one-shot -> BN] Macro-F1=0.1962\n",
      "[EN-trained one-shot -> BN] Macro-recall=0.5000\n",
      "[EN-trained one-shot -> BN] Accuracy=0.2440\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2440    1.0000    0.3923        41\n",
      "           1     0.0000    0.0000    0.0000       127\n",
      "\n",
      "    accuracy                         0.2440       168\n",
      "   macro avg     0.1220    0.5000    0.1962       168\n",
      "weighted avg     0.0596    0.2440    0.0958       168\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 41   0]\n",
      " [127   0]]\n",
      "\n",
      "Saved one-shot predictions for BN to: outputs_lrl_xlmr_en/BN_xlmr_en_one_shot_predictions.csv\n",
      "\n",
      "================================================================================\n",
      "EN-trained XLM-R (one-shot) -> LRL language: ML\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6029407/mhossai6/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/project/6029407/mhossai6/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/project/6029407/mhossai6/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EN-trained one-shot -> ML] Macro-F1=0.2857\n",
      "[EN-trained one-shot -> ML] Macro-recall=0.2857\n",
      "[EN-trained one-shot -> ML] Accuracy=0.4000\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000        15\n",
      "           1     0.5714    0.5714    0.5714        35\n",
      "\n",
      "    accuracy                         0.4000        50\n",
      "   macro avg     0.2857    0.2857    0.2857        50\n",
      "weighted avg     0.4000    0.4000    0.4000        50\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 0 15]\n",
      " [15 20]]\n",
      "\n",
      "Saved one-shot predictions for ML to: outputs_lrl_xlmr_en/ML_xlmr_en_one_shot_predictions.csv\n",
      "\n",
      "================================================================================\n",
      "EN-trained XLM-R (one-shot) -> LRL language: PA\n",
      "================================================================================\n",
      "[EN-trained one-shot -> PA] Macro-F1=0.3107\n",
      "[EN-trained one-shot -> PA] Macro-recall=0.4497\n",
      "[EN-trained one-shot -> PA] Accuracy=0.4000\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4130    0.8636    0.5588        22\n",
      "           1     0.2500    0.0357    0.0625        28\n",
      "\n",
      "    accuracy                         0.4000        50\n",
      "   macro avg     0.3315    0.4497    0.3107        50\n",
      "weighted avg     0.3217    0.4000    0.2809        50\n",
      "\n",
      "Confusion matrix:\n",
      "[[19  3]\n",
      " [27  1]]\n",
      "\n",
      "Saved one-shot predictions for PA to: outputs_lrl_xlmr_en/PA_xlmr_en_one_shot_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# 08 - Evaluate EN-trained one-shot model on all LRL languages\n",
    "\n",
    "one_results = []\n",
    "\n",
    "for lang in lrl_languages:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"EN-trained XLM-R (one-shot) -> LRL language: {lang}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    df_lang = lrl_df[lrl_df[\"Language\"] == lang].copy()\n",
    "    if df_lang.empty:\n",
    "        print(f\"No rows for language {lang}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Build dataloader for this language (with labels)\n",
    "    dev_loader_lang = make_loader(\n",
    "        df_lang,\n",
    "        tok_1s_xfer,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        max_length=256,\n",
    "        with_labels=True,\n",
    "    )\n",
    "\n",
    "    ytrue, ypred = predict(model_1s_xfer, dev_loader_lang)\n",
    "\n",
    "    f1_macro = f1_score(ytrue, ypred, average=\"macro\")\n",
    "    recall_macro = recall_score(ytrue, ypred, average=\"macro\")\n",
    "    acc = accuracy_score(ytrue, ypred)\n",
    "\n",
    "    print(f\"[EN-trained one-shot -> {lang}] Macro-F1={f1_macro:.4f}\")\n",
    "    print(f\"[EN-trained one-shot -> {lang}] Macro-recall={recall_macro:.4f}\")\n",
    "    print(f\"[EN-trained one-shot -> {lang}] Accuracy={acc:.4f}\\n\")\n",
    "\n",
    "    print(\"Classification report:\")\n",
    "    print(classification_report(ytrue, ypred, digits=4))\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix(ytrue, ypred))\n",
    "\n",
    "    # Save per-language predictions\n",
    "    out_df = df_lang.copy()\n",
    "    out_df[\"PredLabel_EN_OneShot\"] = ypred\n",
    "    pred_path = OUT_DIR / f\"{lang}_xlmr_en_one_shot_predictions.csv\"\n",
    "    out_df.to_csv(pred_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"\\nSaved one-shot predictions for {lang} to: {pred_path}\")\n",
    "\n",
    "    one_results.append({\n",
    "        \"Language\": lang,\n",
    "        \"NumExamples\": len(df_lang),\n",
    "        \"MacroF1_OneShot\": f1_macro,\n",
    "        \"MacroRecall_OneShot\": recall_macro,\n",
    "        \"Accuracy_OneShot\": acc,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f04fa65",
   "metadata": {},
   "source": [
    "## Summaries & metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22c24cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== EN-trained XLM-R → LRL Summary =====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>NumExamples</th>\n",
       "      <th>MacroF1_ZeroShot</th>\n",
       "      <th>MacroRecall_ZeroShot</th>\n",
       "      <th>Accuracy_ZeroShot</th>\n",
       "      <th>MacroF1_OneShot</th>\n",
       "      <th>MacroRecall_OneShot</th>\n",
       "      <th>Accuracy_OneShot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BN</td>\n",
       "      <td>168</td>\n",
       "      <td>0.366146</td>\n",
       "      <td>0.356251</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.196172</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.244048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ML</td>\n",
       "      <td>50</td>\n",
       "      <td>0.269481</td>\n",
       "      <td>0.276190</td>\n",
       "      <td>0.28000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PA</td>\n",
       "      <td>50</td>\n",
       "      <td>0.415615</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.310662</td>\n",
       "      <td>0.449675</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Language  NumExamples  MacroF1_ZeroShot  MacroRecall_ZeroShot  \\\n",
       "0       BN          168          0.366146              0.356251   \n",
       "1       ML           50          0.269481              0.276190   \n",
       "2       PA           50          0.415615              0.553571   \n",
       "\n",
       "   Accuracy_ZeroShot  MacroF1_OneShot  MacroRecall_OneShot  Accuracy_OneShot  \n",
       "0            0.47619         0.196172             0.500000          0.244048  \n",
       "1            0.28000         0.285714             0.285714          0.400000  \n",
       "2            0.50000         0.310662             0.449675          0.400000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved summary CSV to: outputs_lrl_xlmr_en/xlmr_en_to_lrl_summary_all_languages.csv\n",
      "\n",
      "Done. EN-trained XLM-R evaluated on all LRL languages in lrl_idioms.csv.\n"
     ]
    }
   ],
   "source": [
    "# 09 - Summaries & metadata\n",
    "\n",
    "# Join zero-shot and one-shot results on Language\n",
    "summary_df = None\n",
    "if zero_results and one_results:\n",
    "    zero_df = pd.DataFrame(zero_results)\n",
    "    one_df = pd.DataFrame(one_results)\n",
    "    summary_df = pd.merge(zero_df, one_df, on=[\"Language\", \"NumExamples\"], how=\"outer\")\n",
    "elif zero_results:\n",
    "    summary_df = pd.DataFrame(zero_results)\n",
    "elif one_results:\n",
    "    summary_df = pd.DataFrame(one_results)\n",
    "\n",
    "if summary_df is not None:\n",
    "    print(\"\\n===== EN-trained XLM-R → LRL Summary =====\")\n",
    "    display(summary_df)\n",
    "\n",
    "    summary_path = OUT_DIR / \"xlmr_en_to_lrl_summary_all_languages.csv\"\n",
    "    summary_df.to_csv(summary_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"\\nSaved summary CSV to: {summary_path}\")\n",
    "else:\n",
    "    print(\"No results collected (check data / training).\")\n",
    "\n",
    "# Save simple run metadata\n",
    "with open(OUT_DIR / \"run_lrl_xlmr_en.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"MODEL_NAME={MODEL_NAME}\\n\")\n",
    "    f.write(f\"DEVICE={device.type}\\n\")\n",
    "    f.write(f\"BATCH_SIZE={BATCH_SIZE}\\n\")\n",
    "    f.write(f\"NUM_EPOCHS={NUM_EPOCHS}\\n\")\n",
    "    f.write(f\"LR={LR}\\n\")\n",
    "    f.write(f\"DATA_SEMEVAL={DATA_DIR}\\n\")\n",
    "    f.write(f\"DATA_LRL={LRL_CSV}\\n\")\n",
    "    f.write(f\"BEST_EN_DEV_F1_ZERO={best_en_dev_f1_0s:.4f}\\n\")\n",
    "    f.write(f\"BEST_EN_DEV_F1_ONE={best_en_dev_f1_1s:.4f}\\n\")\n",
    "    if summary_df is not None:\n",
    "        for _, row in summary_df.iterrows():\n",
    "            lang = row[\"Language\"]\n",
    "            f.write(\n",
    "                f\"LANG={lang},N={int(row['NumExamples'])},\"\n",
    "                f\"MacroF1_Zero={row.get('MacroF1_ZeroShot', float('nan'))},\"\n",
    "                f\"MacroF1_One={row.get('MacroF1_OneShot', float('nan'))}\\n\"\n",
    "            )\n",
    "\n",
    "print(\"\\nDone. EN-trained XLM-R evaluated on all LRL languages in lrl_idioms.csv.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
