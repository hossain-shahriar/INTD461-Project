{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6278c996",
   "metadata": {},
   "source": [
    "## Setup: device, imports, seeds, paths, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a89d0049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6029407/mhossai6/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: CUDA | Model: meta-llama/Llama-3.2-3B-Instruct\n",
      "CSV path: /project/6029407/mhossai6/INTD461-Project/lrl_idioms.csv\n",
      "Selected language: BN\n"
     ]
    }
   ],
   "source": [
    "# 01 - Setup: device, imports, seeds, paths, model\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "import gc\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# ---- Single toggle: \"cpu\" or \"gpu\"\n",
    "RUN_DEVICE = \"gpu\"   # change to \"cpu\" to force CPU\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "if RUN_DEVICE.lower() == \"gpu\" and torch.cuda.is_available():\n",
    "    SELECT_DEVICE = \"cuda\"\n",
    "else:\n",
    "    SELECT_DEVICE = \"cpu\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "device = torch.device(SELECT_DEVICE)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "if device.type == \"cpu\":\n",
    "    torch.set_num_threads(max(1, os.cpu_count() // 2))\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "\n",
    "# ---- Single CSV & language setup\n",
    "# lrl_idioms.csv lives in the SAME FOLDER as this notebook\n",
    "MULTI_CSV = Path(\"lrl_idioms.csv\")\n",
    "\n",
    "# choose which language to run: e.g., \"BN\", \"ML\", \"PA\"\n",
    "LANGUAGE = \"BN\"\n",
    "\n",
    "OUT_DIR = Path(f\"outputs_{LANGUAGE.lower()}_llm\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Choose instruct model that fits RAM\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# generation caps (we classify via logits; generation is a tiny fallback)\n",
    "MAX_NEW_TOKENS = 1\n",
    "DO_SAMPLE = False\n",
    "\n",
    "print(f\"Using device: {device.type.upper()} | Model: {MODEL_NAME}\")\n",
    "print(f\"CSV path: {MULTI_CSV.resolve()}\")\n",
    "print(f\"Selected language: {LANGUAGE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec371f0",
   "metadata": {},
   "source": [
    "## Data loading & utilities (single CSV + language filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb1a2d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'lrl_idioms.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrevious: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprev\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTarget: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtgt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNext: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnxt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Load full LRL dataset\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m full_df \u001b[38;5;241m=\u001b[39m \u001b[43mload_lrl_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMULTI_CSV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded lrl_idioms.csv (all languages):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(full_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m, in \u001b[0;36mload_lrl_csv\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lrl_csv\u001b[39m(path: Path) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    Assumes CSV with columns at least:\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    ID, Language, MWE, Previous, Target, Next, Label\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    We'll keep Label only for metrics, not for prompts.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpython\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [c\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[0;32m/project/6029407/mhossai6/nlp/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/6029407/mhossai6/nlp/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/project/6029407/mhossai6/nlp/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/6029407/mhossai6/nlp/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/project/6029407/mhossai6/nlp/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lrl_idioms.csv'"
     ]
    }
   ],
   "source": [
    "# 02 - Data loading & utilities (single CSV + language filter)\n",
    "\n",
    "def load_lrl_csv(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assumes CSV with columns at least:\n",
    "    ID, Language, MWE, Previous, Target, Next, Label\n",
    "\n",
    "    We'll keep Label only for metrics, not for prompts.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, sep=None, engine=\"python\", dtype=str)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    if \"Label\" in df.columns:\n",
    "        df[\"Label\"] = df[\"Label\"].astype(int)\n",
    "    return df\n",
    "\n",
    "def mark_first_case_insensitive(text: str, needle: str,\n",
    "                                ltag=\"<mwe>\", rtag=\"</mwe>\") -> str:\n",
    "    if not isinstance(text, str) or not isinstance(needle, str):\n",
    "        return text\n",
    "    lt = text.lower()\n",
    "    ln = needle.lower()\n",
    "    i = lt.find(ln)\n",
    "    if i == -1:\n",
    "        return text\n",
    "    return text[:i] + ltag + text[i:i+len(needle)] + rtag + text[i+len(needle):]\n",
    "\n",
    "def pack_context(prev: str, target: str, nxt: str, mwe: str) -> str:\n",
    "    \"\"\"\n",
    "    Build the context block for the LLM. Note: NO label is included.\n",
    "    \"\"\"\n",
    "    prev = \"\" if pd.isna(prev) else prev\n",
    "    nxt = \"\" if pd.isna(nxt) else nxt\n",
    "    tgt = mark_first_case_insensitive(target, mwe)\n",
    "    # Keep instructions in English; content can be BN/ML/PA/etc.\n",
    "    return f\"Previous: {prev}\\nTarget: {tgt}\\nNext: {nxt}\"\n",
    "\n",
    "# Load full LRL dataset\n",
    "full_df = load_lrl_csv(MULTI_CSV)\n",
    "print(\"Loaded lrl_idioms.csv (all languages):\")\n",
    "print(full_df.head())\n",
    "print(f\"Total examples (all languages): {len(full_df)}\")\n",
    "\n",
    "# Filter by language\n",
    "lang_df = full_df[full_df[\"Language\"] == LANGUAGE].copy()\n",
    "print(f\"\\nFiltered to language = {LANGUAGE}:\")\n",
    "print(lang_df.head())\n",
    "print(f\"Total examples for {LANGUAGE}: {len(lang_df)}\")\n",
    "\n",
    "if len(lang_df) == 0:\n",
    "    raise ValueError(f\"No rows found for LANGUAGE={LANGUAGE}. Check your CSV.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9d0038",
   "metadata": {},
   "source": [
    "## Load LLM (CPU/GPU) and tokenizer with HF login + VRAM-safe setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bf9cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03 - Load LLM (CPU/GPU) and tokenizer with HF login + VRAM-safe setup\n",
    "\n",
    "from getpass import getpass\n",
    "from huggingface_hub import login as hf_login\n",
    "\n",
    "# ----- VRAM helpers (safe on CPU too)\n",
    "def free_vram():\n",
    "    for k, v in list(globals()).copy().items():\n",
    "        try:\n",
    "            import torch.nn as nn\n",
    "            if isinstance(v, nn.Module):\n",
    "                del globals()[k]\n",
    "        except Exception:\n",
    "            pass\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            del globals()[k]\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    print(\"VRAM cleanup done.\")\n",
    "\n",
    "def cuda_mem():\n",
    "    if torch.cuda.is_available():\n",
    "        a = torch.cuda.memory_allocated() / (1024**3)\n",
    "        r = torch.cuda.memory_reserved() / (1024**3)\n",
    "        print(f\"CUDA allocated: {a:.2f} GiB | reserved: {r:.2f} GiB\")\n",
    "    else:\n",
    "        print(\"CUDA not available.\")\n",
    "\n",
    "# Prefer segmented allocator on CUDA to reduce fragmentation\n",
    "if device.type == \"cuda\":\n",
    "    os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "\n",
    "# ----- Optional Hugging Face login (supports gated repos)\n",
    "hf_token = os.getenv(\"HF_TOKEN\", \"\").strip()\n",
    "if not hf_token:\n",
    "    try:\n",
    "        hf_token = getpass(\"Enter your Hugging Face token (press Enter to skip): \").strip()\n",
    "    except Exception:\n",
    "        # Some environments don't like getpass\n",
    "        hf_token = input(\"Enter your Hugging Face token (press Enter to skip): \").strip()\n",
    "if hf_token:\n",
    "    try:\n",
    "        hf_login(token=hf_token)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Hugging Face login failed: {e}\")\n",
    "\n",
    "_token_arg = {\"token\": hf_token} if hf_token else {}\n",
    "\n",
    "# ----- Clean up any previous models before loading a new one\n",
    "if device.type == \"cuda\":\n",
    "    cuda_mem()\n",
    "free_vram()\n",
    "if device.type == \"cuda\":\n",
    "    cuda_mem()\n",
    "\n",
    "# ----- Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    "    **_token_arg\n",
    ")\n",
    "\n",
    "# ----- Choose dtype & placement to avoid OOM\n",
    "if device.type == \"cuda\":\n",
    "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\",            # lets HF place on GPU (and CPU if needed)\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        **_token_arg\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float32,\n",
    "        device_map={\"\": \"cpu\"},\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        **_token_arg\n",
    "    )\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# ----- Ensure pad token and attention_mask behavior are well-defined\n",
    "def _ensure_pad():\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        if tokenizer.eos_token_id is not None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        else:\n",
    "            tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "_ensure_pad()\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    cuda_mem()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd57501e",
   "metadata": {},
   "source": [
    "## Minimal zero-shot prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c095c5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04 - Minimal prompts (zero-shot only)\n",
    "\n",
    "INSTR_ZS = (\n",
    "    \"Decide if the MWE is used idiomatically (figurative) in the Target sentence.\\n\"\n",
    "    \"Output only one digit: 1 (idiomatic) or 0 (literal).\"\n",
    ")\n",
    "\n",
    "def make_zero_shot_prompt(mwe: str, ctx_block: str) -> str:\n",
    "    return (\n",
    "        f\"{INSTR_ZS}\\n\"\n",
    "        f\"MWE: {mwe}\\n\"\n",
    "        f\"{ctx_block}\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "\n",
    "def _zero_shot_prompt_for_row(r) -> str:\n",
    "    \"\"\"\n",
    "    Build the prompt for a single row.\n",
    "    IMPORTANT: We do NOT include the Label column here.\n",
    "    \"\"\"\n",
    "    mwe = r[\"MWE\"]\n",
    "    ctx = pack_context(r.get(\"Previous\", \"\"), r.get(\"Target\", \"\"), r.get(\"Next\", \"\"), mwe)\n",
    "    return make_zero_shot_prompt(mwe, ctx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc93d9cf",
   "metadata": {},
   "source": [
    "## Batched 0/1 classification via next-token logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7826de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05 - Batched 0/1 classification via next-token logits\n",
    "\n",
    "BATCH_GEN = 8  # adjust for memory/speed\n",
    "\n",
    "def _apply_chat_or_plain_batch(texts: List[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Convert a batch of strings to model inputs (input_ids, attention_mask).\n",
    "    \"\"\"\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        messages_batch = [[\n",
    "            {\"role\": \"system\", \"content\": \"You are a concise assistant. Respond with 0 or 1.\"},\n",
    "            {\"role\": \"user\", \"content\": t}\n",
    "        ] for t in texts]\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages_batch,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "    else:\n",
    "        input_ids = tokenizer(\n",
    "            texts, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        ).input_ids\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "    return {\n",
    "        \"input_ids\": input_ids.to(device, non_blocking=(device.type == \"cuda\")),\n",
    "        \"attention_mask\": attention_mask.to(device, non_blocking=(device.type == \"cuda\"))\n",
    "    }\n",
    "\n",
    "# Cache token ids for digits \"0\" and \"1\"\n",
    "_id0 = None\n",
    "_id1 = None\n",
    "\n",
    "def _candidate_token_id_for_digit(d: str) -> Optional[int]:\n",
    "    ids = tokenizer.encode(d, add_special_tokens=False)\n",
    "    if len(ids) == 1:\n",
    "        return ids[0]\n",
    "    ids = tokenizer.encode(\" \" + d, add_special_tokens=False)\n",
    "    if len(ids) == 1:\n",
    "        return ids[0]\n",
    "    ids = tokenizer.encode(d + \"\\n\", add_special_tokens=False)\n",
    "    if len(ids) == 1:\n",
    "        return ids[0]\n",
    "    return None\n",
    "\n",
    "def _init_digit_ids():\n",
    "    global _id0, _id1\n",
    "    if _id0 is None:\n",
    "        _id0 = _candidate_token_id_for_digit(\"0\")\n",
    "    if _id1 is None:\n",
    "        _id1 = _candidate_token_id_for_digit(\"1\")\n",
    "\n",
    "_init_digit_ids()\n",
    "\n",
    "@torch.no_grad()\n",
    "def classify_prompts_logits(prompts: List[str]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Given a batch of prompts, classify with 0/1 using next-token logits.\n",
    "    \"\"\"\n",
    "    enc = _apply_chat_or_plain_batch(prompts)# 05 - Batched 0/1 classification via next-token logits\n",
    "\n",
    "BATCH_GEN = 8  # adjust for memory/speed\n",
    "\n",
    "def _apply_chat_or_plain_batch(texts: List[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Convert a batch of strings to model inputs (input_ids, attention_mask).\n",
    "    \"\"\"\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        messages_batch = [[\n",
    "            {\"role\": \"system\", \"content\": \"You are a concise assistant. Respond with 0 or 1.\"},\n",
    "            {\"role\": \"user\", \"content\": t}\n",
    "        ] for t in texts]\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages_batch,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "    else:\n",
    "        input_ids = tokenizer(\n",
    "            texts, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        ).input_ids\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "    return {\n",
    "        \"input_ids\": input_ids.to(device, non_blocking=(device.type == \"cuda\")),\n",
    "        \"attention_mask\": attention_mask.to(device, non_blocking=(device.type == \"cuda\"))\n",
    "    }\n",
    "\n",
    "# Cache token ids for digits \"0\" and \"1\"\n",
    "_id0 = None\n",
    "_id1 = None\n",
    "\n",
    "def _candidate_token_id_for_digit(d: str) -> Optional[int]:\n",
    "    ids = tokenizer.encode(d, add_special_tokens=False)\n",
    "    if len(ids) == 1:\n",
    "        return ids[0]\n",
    "    ids = tokenizer.encode(\" \" + d, add_special_tokens=False)\n",
    "    if len(ids) == 1:\n",
    "        return ids[0]\n",
    "    ids = tokenizer.encode(d + \"\\n\", add_special_tokens=False)\n",
    "    if len(ids) == 1:\n",
    "        return ids[0]\n",
    "    return None\n",
    "\n",
    "def _init_digit_ids():\n",
    "    global _id0, _id1\n",
    "    if _id0 is None:\n",
    "        _id0 = _candidate_token_id_for_digit(\"0\")\n",
    "    if _id1 is None:\n",
    "        _id1 = _candidate_token_id_for_digit(\"1\")\n",
    "\n",
    "_init_digit_ids()\n",
    "\n",
    "@torch.no_grad()\n",
    "def classify_prompts_logits(prompts: List[str]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Given a batch of prompts, classify with 0/1 using next-token logits.\n",
    "    \"\"\"\n",
    "    enc = _apply_chat_or_plain_batch(prompts)\n",
    "    logits = model(**enc).logits          # [B, T, V]\n",
    "    next_logits = logits[:, -1, :]        # [B, V]\n",
    "\n",
    "    if (_id0 is not None) and (_id1 is not None):\n",
    "        logit0 = next_logits[:, _id0]\n",
    "        logit1 = next_logits[:, _id1]\n",
    "        return (logit1 >= logit0).long().cpu().tolist()\n",
    "\n",
    "    # Fallback: force-generate 1 token and parse\n",
    "    gen = model.generate(\n",
    "        **enc,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=DO_SAMPLE,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    outs = []\n",
    "    for i in range(gen.size(0)):\n",
    "        cut = enc[\"input_ids\"][i].shape[-1]\n",
    "        new_ids = gen[i][cut:]\n",
    "        text = tokenizer.decode(new_ids, skip_special_tokens=True)\n",
    "        if \"0\" in text and \"1\" in text:\n",
    "            outs.append(1 if text.index(\"1\") < text.index(\"0\") else 0)\n",
    "        elif \"1\" in text:\n",
    "            outs.append(1)\n",
    "        elif \"0\" in text:\n",
    "            outs.append(0)\n",
    "        else:\n",
    "            outs.append(1)  # default to 1 if ambiguous\n",
    "    return outs\n",
    "\n",
    "    logits = model(**enc).logits          # [B, T, V]\n",
    "    next_logits = logits[:, -1, :]        # [B, V]\n",
    "\n",
    "    if (_id0 is not None) and (_id1 is not None):\n",
    "        logit0 = next_logits[:, _id0]\n",
    "        logit1 = next_logits[:, _id1]\n",
    "        return (logit1 >= logit0).long().cpu().tolist()\n",
    "\n",
    "    # Fallback: force-generate 1 token and parse\n",
    "    gen = model.generate(\n",
    "        **enc,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=DO_SAMPLE,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    outs = []\n",
    "    for i in range(gen.size(0)):\n",
    "        cut = enc[\"input_ids\"][i].shape[-1]\n",
    "        new_ids = gen[i][cut:]\n",
    "        text = tokenizer.decode(new_ids, skip_special_tokens=True)\n",
    "        if \"0\" in text and \"1\" in text:\n",
    "            outs.append(1 if text.index(\"1\") < text.index(\"0\") else 0)\n",
    "        elif \"1\" in text:\n",
    "            outs.append(1)\n",
    "        elif \"0\" in text:\n",
    "            outs.append(0)\n",
    "        else:\n",
    "            outs.append(1)  # default to 1 if ambiguous\n",
    "    return outs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf04100d",
   "metadata": {},
   "source": [
    "## Progressive prediction (zero-shot, with caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c0d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06 - Progressive prediction (zero-shot, with caching)\n",
    "\n",
    "from time import perf_counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# optional: use math SDPA kernels to reduce peak memory (slower but safer)\n",
    "try:\n",
    "    if device.type == \"cuda\":\n",
    "        torch.backends.cuda.enable_flash_sdp(False)\n",
    "        torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "        torch.backends.cuda.enable_math_sdp(True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def _load_cache(cache_path: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Load cached predictions if they exist.\n",
    "    \"\"\"\n",
    "    if cache_path.exists():\n",
    "        df = pd.read_csv(cache_path, dtype={\"ID\": str, \"Label\": int})\n",
    "        return dict(zip(df[\"ID\"].astype(str), df[\"Label\"].astype(int)))\n",
    "    return {}\n",
    "\n",
    "def _append_one(cache_path: Path, rec: tuple):\n",
    "    _id, _lab = rec\n",
    "    header_needed = not cache_path.exists()\n",
    "    with open(cache_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        if header_needed:\n",
    "            f.write(\"ID,Label\\n\")\n",
    "        f.write(f\"{_id},{int(_lab)}\\n\")\n",
    "\n",
    "def _classify_micro(prompts: list, micro_bs: int) -> list:\n",
    "    \"\"\"\n",
    "    Run classification in small micro-batches, retrying on OOM by halving the batch size.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    i = 0\n",
    "    bs = max(1, micro_bs)\n",
    "    while i < len(prompts):\n",
    "        step = min(bs, len(prompts) - i)\n",
    "        chunk = prompts[i:i+step]\n",
    "        try:\n",
    "            with torch.inference_mode():\n",
    "                out.extend(classify_prompts_logits(chunk))\n",
    "            i += step\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower() and step > 1:\n",
    "                if device.type == \"cuda\":\n",
    "                    torch.cuda.empty_cache(); torch.cuda.ipc_collect()\n",
    "                bs = max(1, step // 2)\n",
    "                continue\n",
    "            raise\n",
    "    return out\n",
    "\n",
    "def progressive_predict_zeroshot_batched(df: pd.DataFrame,\n",
    "                                         cache_path: Path,\n",
    "                                         desc: str,\n",
    "                                         batch_gen: int = None,\n",
    "                                         micro_bs: int = None) -> list:\n",
    "    \"\"\"\n",
    "    Zero-shot predictions on the given dataframe, with resumable cache.\n",
    "    Assumes df has columns: ID, MWE, Previous, Target, Next.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"ID\"] = df[\"ID\"].astype(str)\n",
    "\n",
    "    preds_map = _load_cache(cache_path)\n",
    "    done = set(preds_map.keys())\n",
    "    todo_idx = [i for i, _id in enumerate(df[\"ID\"]) if _id not in done]\n",
    "\n",
    "    # defaults tuned for T4; CPU can go larger\n",
    "    if batch_gen is None:\n",
    "        batch_gen = 8 if device.type == \"cuda\" else 16\n",
    "    if micro_bs is None:\n",
    "        micro_bs = 2 if device.type == \"cuda\" else 8\n",
    "\n",
    "    print(f\"{desc} | Resuming with {len(done)} cached / {len(df)} total\")\n",
    "    t0 = perf_counter()\n",
    "    for start in tqdm(range(0, len(todo_idx), batch_gen), desc=desc, leave=True):\n",
    "        rows = todo_idx[start:start+batch_gen]\n",
    "        prompts = []\n",
    "        ids = []\n",
    "        for j in rows:\n",
    "            r = df.iloc[j]\n",
    "            prompts.append(_zero_shot_prompt_for_row(r))\n",
    "            ids.append(r[\"ID\"])\n",
    "        if not prompts:\n",
    "            continue\n",
    "\n",
    "        labels = _classify_micro(prompts, micro_bs)\n",
    "        for _id, lab in zip(ids, labels):\n",
    "            preds_map[_id] = int(lab)\n",
    "            _append_one(cache_path, (_id, int(lab)))\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    elapsed = perf_counter() - t0\n",
    "    print(\n",
    "        f\"{desc} | Newly computed: {len(todo_idx)} | Cached at start: {len(done)} | \"\n",
    "        f\"Total: {len(df)} | Elapsed: {elapsed:.1f}s | \"\n",
    "        f\"{(elapsed / max(1, len(todo_idx))):.3f}s/example (new only)\"\n",
    "    )\n",
    "    return [preds_map[str(i)] for i in df[\"ID\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9eaf38",
   "metadata": {},
   "source": [
    "## Run zero-shot classification for chosen language, compute metrics, save outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ca492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 07 - Run zero-shot classification for chosen language, compute metrics, save outputs\n",
    "\n",
    "# Cache for selected language\n",
    "cache_lang_0s = OUT_DIR / f\"cache_llm_zeroshot_{LANGUAGE.lower()}.csv\"\n",
    "\n",
    "yhat_lang_0s = progressive_predict_zeroshot_batched(\n",
    "    lang_df,\n",
    "    cache_lang_0s,\n",
    "    desc=f\"Zero-shot {LANGUAGE} (full dataset)\"\n",
    ")\n",
    "\n",
    "# Ground-truth labels (used ONLY here, AFTER prediction; never sent to model)\n",
    "if \"Label\" not in lang_df.columns:\n",
    "    raise ValueError(\"No 'Label' column found in lrl_idioms.csv; cannot compute metrics.\")\n",
    "\n",
    "ytrue_lang = lang_df[\"Label\"].astype(int).tolist()\n",
    "\n",
    "f1_macro = f1_score(ytrue_lang, yhat_lang_0s, average=\"macro\")\n",
    "acc = accuracy_score(ytrue_lang, yhat_lang_0s)\n",
    "\n",
    "print(f\"[LLM Zero-shot {LANGUAGE}] Macro-F1: {f1_macro:.4f}\")\n",
    "print(f\"[LLM Zero-shot {LANGUAGE}] Accuracy: {acc:.4f}\")\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(ytrue_lang, yhat_lang_0s, digits=4))\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(ytrue_lang, yhat_lang_0s))\n",
    "\n",
    "# Save predictions together with original data\n",
    "lang_out = lang_df.copy()\n",
    "lang_out[\"PredLabel\"] = yhat_lang_0s\n",
    "pred_path = OUT_DIR / f\"{LANGUAGE.lower()}_predictions_llm_zeroshot.csv\"\n",
    "lang_out.to_csv(pred_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\nSaved predictions to: {pred_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b70dcb",
   "metadata": {},
   "source": [
    "## Save run metadata & show cache file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14c470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 08 - Save run metadata & show cache file\n",
    "\n",
    "with open(OUT_DIR / f\"run_{LANGUAGE.lower()}_llm.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"MODEL_NAME={MODEL_NAME}\\n\")\n",
    "    f.write(f\"DEVICE={device.type}\\n\")\n",
    "    f.write(f\"DATA={MULTI_CSV}\\n\")\n",
    "    f.write(f\"LANGUAGE={LANGUAGE}\\n\")\n",
    "\n",
    "print(\"Resume cache file (delete it to start fresh for this language):\")\n",
    "print(f\"- {OUT_DIR / f'cache_llm_zeroshot_{LANGUAGE.lower()}.csv'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
