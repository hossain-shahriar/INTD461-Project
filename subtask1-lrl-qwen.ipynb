{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa7d3a99",
   "metadata": {},
   "source": [
    "## Setup: CPU/GPU, imports, seeds, paths, model choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc7d0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA H100 80GB HBM3 MIG 2g.20gb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6029407/mhossai6/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV path: /project/6029407/mhossai6/INTD461-Project/lrl_idioms.csv\n",
      "Model: Qwen/Qwen2.5-7B-Instruct\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 01 - Setup: single switch for CPU/GPU, imports, seeds, paths, model choice\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "RUN_DEVICE = \"gpu\"  # \"gpu\" to use CUDA if available, otherwise CPU\n",
    "\n",
    "if RUN_DEVICE.lower() == \"gpu\" and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.set_num_threads(max(1, os.cpu_count() // 2))\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ---- Single CSV with all languages; same folder as this notebook\n",
    "MULTI_CSV = Path(\"lrl_idioms.csv\")\n",
    "\n",
    "# Output directory (shared, with per-language files)\n",
    "OUT_DIR = Path(\"outputs_lrl_qwen\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Qwen model\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "# Batch size for batched logits classification\n",
    "BATCH_GEN_ZERO = 8 if device.type == \"cuda\" else 4\n",
    "\n",
    "print(f\"CSV path: {MULTI_CSV.resolve()}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Device: {device.type}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f43d6c1",
   "metadata": {},
   "source": [
    "## Data loading & text utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acf87c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded lrl_idioms.csv (all languages):\n",
      "     ID Language                          MWE  \\\n",
      "0  bn_1       BN                   বৃষ্টির জল   \n",
      "1  bn_2       BN  সংসার সুখের হয় রমণীর গুণে ।   \n",
      "2  bn_3       BN                 যাচ্ছা তাই ।   \n",
      "3  bn_4       BN         উলুবনে মুক্ত ছড়ানো ।   \n",
      "4  bn_5       BN                     নিজের ঘর   \n",
      "\n",
      "                                            Previous  \\\n",
      "0                 আকাশে অনেকক্ষণ ধরে কালো মেঘ জমছিল।   \n",
      "1  ছোট বাসা, অল্প রোজগার—সব মিলিয়ে খুব অভাবের জীব...   \n",
      "2  পাড়ার ক্লাবের অনুষ্ঠানের দায়িত্ব যাঁর হাতে দেও...   \n",
      "3  প্রফেসর গভীর মনোযোগ দিয়ে কঠিন তত্ত্বগুলো বুঝিয়...   \n",
      "4   সারাদিন বাইরে থাকায় সে ভীষণ ক্লান্ত হয়ে গিয়েছিল।   \n",
      "\n",
      "                                              Target  \\\n",
      "0           হঠাৎ টুপটাপ করে বৃষ্টির জল পড়া শুরু হলো।   \n",
      "1  তবু হাসিমুখে সব সামলে রাখত তারা; লোকজন বলত, সং...   \n",
      "2  ফলে মঞ্চ, সাউন্ড, আলো—সব মিলিয়ে শেষ দিন একেবার...   \n",
      "3  কিন্তু ছাত্ররা মোবাইলে ব্যস্ত, কেউ শুনছেই না—এ...   \n",
      "4    বাড়ি ফিরে সে সোজা নিজের ঘরে গিয়ে দরজা বন্ধ করল।   \n",
      "\n",
      "                                                Next  Label  \n",
      "0  কিছুক্ষণের মধ্যেই রাস্তা ভিজে একেবারে চকচকে হয়...      0  \n",
      "1  পাড়ার সবাই ওদের দাম্পত্য দেখে মুগ্ধ হয়ে আশীর্ব...      1  \n",
      "2  অতিথিরা কেমন যেন মুখ চেপে হেসে অনুষ্ঠান শেষ হও...      1  \n",
      "3  শেষে তিনি বিরক্ত হয়ে ক্লাস বন্ধ করে দিয়ে বললেন...      1  \n",
      "4    ব্যাগটা রেখে একটু পরে বিছানায় শুয়ে বিশ্রাম নিল।      0  \n",
      "Total examples (all languages): 268\n",
      "\n",
      "Languages found in dataset: ['BN', 'ML', 'PA']\n"
     ]
    }
   ],
   "source": [
    "# 02 - Data loading & text utilities\n",
    "\n",
    "def load_lrl_csv(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assumes CSV with columns at least:\n",
    "    ID, Language, MWE, Previous, Target, Next, Label\n",
    "\n",
    "    We'll keep Label only for metrics, not for prompts.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, sep=None, engine=\"python\", dtype=str)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    if \"Label\" in df.columns:\n",
    "        df[\"Label\"] = df[\"Label\"].astype(int)\n",
    "    return df\n",
    "\n",
    "def mark_first_case_insensitive(text: str, needle: str,\n",
    "                                ltag=\"<mwe>\", rtag=\"</mwe>\") -> str:\n",
    "    if not isinstance(text, str) or not isinstance(needle, str):\n",
    "        return text\n",
    "    lt = text.lower()\n",
    "    ln = needle.lower()\n",
    "    i = lt.find(ln)\n",
    "    if i == -1:\n",
    "        return text\n",
    "    return text[:i] + ltag + text[i:i+len(needle)] + rtag + text[i+len(needle):]\n",
    "\n",
    "def pack_context(prev: str, target: str, nxt: str, mwe: str) -> str:\n",
    "    \"\"\"\n",
    "    Build the context block for the LLM. Note: NO label is included.\n",
    "    \"\"\"\n",
    "    prev = \"\" if pd.isna(prev) else prev\n",
    "    nxt = \"\" if pd.isna(nxt) else nxt\n",
    "    tgt = mark_first_case_insensitive(target, mwe)\n",
    "    # Keep instructions in English; content can be BN/ML/PA/etc.\n",
    "    return f\"Previous: {prev}\\nTarget: {tgt}\\nNext: {nxt}\"\n",
    "\n",
    "# Load full LRL dataset\n",
    "full_df = load_lrl_csv(MULTI_CSV)\n",
    "print(\"Loaded lrl_idioms.csv (all languages):\")\n",
    "print(full_df.head())\n",
    "print(f\"Total examples (all languages): {len(full_df)}\")\n",
    "\n",
    "# Get unique languages present\n",
    "languages = sorted(full_df[\"Language\"].dropna().unique().tolist())\n",
    "print(\"\\nLanguages found in dataset:\", languages)\n",
    "\n",
    "if len(languages) == 0:\n",
    "    raise ValueError(\"No languages found in lrl_idioms.csv (Language column empty?).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31652f",
   "metadata": {},
   "source": [
    "## Load Qwen (CPU/GPU) and tokenizer with VRAM-safe setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed901e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA allocated: 0.00 GiB | reserved: 0.00 GiB\n",
      "VRAM cleanup done.\n",
      "CUDA allocated: 0.00 GiB | reserved: 0.00 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "2025-12-03 19:21:10.737211: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-03 19:21:10.750980: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764818470.761425 2479577 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764818470.764162 2479577 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764818470.773431 2479577 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764818470.773443 2479577 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764818470.773445 2479577 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764818470.773446 2479577 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-03 19:21:10.776816: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.25s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA allocated: 10.57 GiB | reserved: 10.59 GiB\n"
     ]
    }
   ],
   "source": [
    "# 03 - Load Qwen LLM (CPU or GPU) and tokenizer with VRAM-safe setup\n",
    "\n",
    "from getpass import getpass\n",
    "from huggingface_hub import login as hf_login\n",
    "import gc\n",
    "\n",
    "# ----- VRAM helpers (safe on CPU too)\n",
    "def free_vram():\n",
    "    \"\"\"\n",
    "    Basic cleanup: run garbage collection and clear CUDA cache.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    print(\"VRAM cleanup done.\")\n",
    "\n",
    "def cuda_mem():\n",
    "    if torch.cuda.is_available():\n",
    "        a = torch.cuda.memory_allocated() / (1024**3)\n",
    "        r = torch.cuda.memory_reserved() / (1024**3)\n",
    "        print(f\"CUDA allocated: {a:.2f} GiB | reserved: {r:.2f} GiB\")\n",
    "    else:\n",
    "        print(\"CUDA not available.\")\n",
    "\n",
    "# Prefer segmented allocator on CUDA to reduce fragmentation\n",
    "if device.type == \"cuda\":\n",
    "    os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "\n",
    "# ----- Optional Hugging Face login (supports gated repos)\n",
    "hf_token = os.getenv(\"HF_TOKEN\", \"\").strip()\n",
    "if not hf_token:\n",
    "    try:\n",
    "        hf_token = getpass(\"Enter your Hugging Face token (or press Enter to skip): \").strip()\n",
    "    except Exception:\n",
    "        hf_token = input(\"Enter your Hugging Face token (or press Enter to skip): \").strip()\n",
    "if hf_token:\n",
    "    try:\n",
    "        hf_login(token=hf_token)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Hugging Face login failed: {e}\")\n",
    "\n",
    "_token_arg = {\"token\": hf_token} if hf_token else {}\n",
    "\n",
    "# ----- Clean up any previous models before loading a new one\n",
    "if device.type == \"cuda\":\n",
    "    cuda_mem()\n",
    "free_vram()\n",
    "if device.type == \"cuda\":\n",
    "    cuda_mem()\n",
    "\n",
    "# ----- Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    "    **_token_arg\n",
    ")\n",
    "\n",
    "# ----- Choose dtype & placement to avoid OOM\n",
    "if device.type == \"cuda\":\n",
    "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\",      # let HF shard / offload as needed\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        **_token_arg\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float32,\n",
    "        device_map={\"\": \"cpu\"},  # all on CPU\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        **_token_arg\n",
    "    )\n",
    "\n",
    "# ----- Ensure pad token and attention_mask behavior are well-defined\n",
    "if tokenizer.pad_token_id is None:\n",
    "    if tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    else:\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model.eval()\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    cuda_mem()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a6ec5a",
   "metadata": {},
   "source": [
    "## Zero-shot prompt builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a409128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04 - Minimal zero-shot prompt builder\n",
    "\n",
    "INSTR_ZS = (\n",
    "    \"Decide if the MWE is used idiomatically (figurative) in the Target sentence.\\n\"\n",
    "    \"Output only one digit: 1 (idiomatic) or 0 (literal).\"\n",
    ")\n",
    "\n",
    "def make_zero_shot_prompt(mwe: str, ctx_block: str) -> str:\n",
    "    return (\n",
    "        f\"{INSTR_ZS}\\n\"\n",
    "        f\"MWE: {mwe}\\n\"\n",
    "        f\"{ctx_block}\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e76987",
   "metadata": {},
   "source": [
    "## Batched next-token logits classification (0/1 only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d7bb0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05 - Batched next-token logits classification (0/1 only), CPU/GPU\n",
    "\n",
    "def _apply_chat_or_plain_batch(texts: List[str]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Turn raw prompts into input_ids + attention_mask.\n",
    "    \"\"\"\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        messages_batch = [[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful, concise assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": t}\n",
    "        ] for t in texts]\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages_batch,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "    else:\n",
    "        input_ids = tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        ).input_ids\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "_id0: Optional[int] = None\n",
    "_id1: Optional[int] = None\n",
    "\n",
    "def _candidate_token_id_for_digit(d: str) -> Optional[int]:\n",
    "    ids = tokenizer.encode(d, add_special_tokens=False)\n",
    "    if len(ids) == 1:\n",
    "        return ids[0]\n",
    "    ids = tokenizer.encode(\" \" + d, add_special_tokens=False)\n",
    "    if len(ids) == 1:\n",
    "        return ids[0]\n",
    "    ids = tokenizer.encode(d + \"\\n\", add_special_tokens=False)\n",
    "    if len(ids) == 1:\n",
    "        return ids[0]\n",
    "    return None\n",
    "\n",
    "def _init_digit_ids():\n",
    "    global _id0, _id1\n",
    "    if _id0 is None:\n",
    "        _id0 = _candidate_token_id_for_digit(\"0\")\n",
    "    if _id1 is None:\n",
    "        _id1 = _candidate_token_id_for_digit(\"1\")\n",
    "\n",
    "_init_digit_ids()\n",
    "\n",
    "def classify_prompts_logits(prompts: List[str]) -> List[int]:\n",
    "    enc = _apply_chat_or_plain_batch(prompts)\n",
    "\n",
    "    # Move inputs to the same device as the model\n",
    "    model_device = next(model.parameters()).device\n",
    "    enc = {k: v.to(model_device) for k, v in enc.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits  # [B, T, V]\n",
    "        next_logits = logits[:, -1, :]\n",
    "        if _id0 is not None and _id1 is not None:\n",
    "            logit0 = next_logits[:, _id0]\n",
    "            logit1 = next_logits[:, _id1]\n",
    "            return (logit1 >= logit0).long().detach().cpu().tolist()\n",
    "\n",
    "    # Fallback: generate one token and parse\n",
    "    outs: List[int] = []\n",
    "    with torch.no_grad():\n",
    "        gen = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=1,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    for i in range(gen.size(0)):\n",
    "        cut = enc[\"input_ids\"][i].shape[-1]\n",
    "        new_ids = gen[i][cut:]\n",
    "        text = tokenizer.decode(new_ids, skip_special_tokens=True)\n",
    "        if \"0\" in text and \"1\" in text:\n",
    "            outs.append(1 if text.index(\"1\") < text.index(\"0\") else 0)\n",
    "        elif \"1\" in text:\n",
    "            outs.append(1)\n",
    "        elif \"0\" in text:\n",
    "            outs.append(0)\n",
    "        else:\n",
    "            outs.append(1)\n",
    "    return outs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b8ac82",
   "metadata": {},
   "source": [
    "## Caching helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddc0423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06 - Caching helpers\n",
    "\n",
    "from time import perf_counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def _load_cache(cache_path: Path) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Load a simple ID -> Label cache from CSV, if it exists.\n",
    "    \"\"\"\n",
    "    if cache_path.exists():\n",
    "        df = pd.read_csv(cache_path, dtype={\"ID\": str, \"Label\": int})\n",
    "        return dict(zip(df[\"ID\"].astype(str), df[\"Label\"].astype(int)))\n",
    "    return {}\n",
    "\n",
    "def _append_one(cache_path: Path, rec: Tuple[str, int]):\n",
    "    \"\"\"\n",
    "    Append a single (ID, Label) record to the cache CSV (creating header if needed).\n",
    "    \"\"\"\n",
    "    _id, _lab = rec\n",
    "    header_needed = not cache_path.exists()\n",
    "    with open(cache_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        if header_needed:\n",
    "            f.write(\"ID,Label\\n\")\n",
    "        f.write(f\"{_id},{int(_lab)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78219705",
   "metadata": {},
   "source": [
    "## Zero-shot prediction for a single language (batched, resumable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec99e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 07 - Zero-shot prediction for a single language (batched, resumable)\n",
    "\n",
    "def progressive_predict_zero_shot_batched_for_language(\n",
    "    df_lang: pd.DataFrame,\n",
    "    language: str,\n",
    "    cache_path: Path,\n",
    "    desc: str = \"Zero-shot Qwen\",\n",
    ") -> List[int]:\n",
    "    df = df_lang.copy()\n",
    "    df[\"ID\"] = df[\"ID\"].astype(str)\n",
    "\n",
    "    preds_map = _load_cache(cache_path)\n",
    "    done = set(preds_map.keys())\n",
    "    todo_idx = [i for i, _id in enumerate(df[\"ID\"]) if _id not in done]\n",
    "\n",
    "    print(f\"{desc} [{language}] | Resuming with {len(done)} cached / {len(df)} total\")\n",
    "    t0 = perf_counter()\n",
    "\n",
    "    for start in tqdm(range(0, len(todo_idx), BATCH_GEN_ZERO), desc=f\"{desc} [{language}]\", leave=True):\n",
    "        batch_rows = todo_idx[start:start + BATCH_GEN_ZERO]\n",
    "        prompts: List[str] = []\n",
    "        ids: List[str] = []\n",
    "\n",
    "        for j in batch_rows:\n",
    "            r = df.iloc[j]\n",
    "            _id = r[\"ID\"]\n",
    "            mwe = r[\"MWE\"]\n",
    "            ctx = pack_context(\n",
    "                r.get(\"Previous\", \"\"),\n",
    "                r.get(\"Target\", \"\"),\n",
    "                r.get(\"Next\", \"\"),\n",
    "                mwe,\n",
    "            )\n",
    "            prompts.append(make_zero_shot_prompt(mwe, ctx))\n",
    "            ids.append(_id)\n",
    "\n",
    "        if not prompts:\n",
    "            continue\n",
    "\n",
    "        labels = classify_prompts_logits(prompts)\n",
    "        for _id, lab in zip(ids, labels):\n",
    "            preds_map[_id] = int(lab)\n",
    "            _append_one(cache_path, (_id, int(lab)))\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    elapsed = perf_counter() - t0\n",
    "    print(\n",
    "        f\"{desc} [{language}] | Newly computed: {len(todo_idx)} | Cached at start: {len(done)} | \"\n",
    "        f\"Total: {len(df)} | Elapsed: {elapsed:.1f}s | \"\n",
    "        f\"{(elapsed / max(1, len(todo_idx))):.3f}s/example (new only)\"\n",
    "    )\n",
    "\n",
    "    yhat = [preds_map[str(i)] for i in df[\"ID\"]]\n",
    "    return yhat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35ce2eb",
   "metadata": {},
   "source": [
    "## Run zero-shot evaluation for all languages and collect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4f34ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing language: BN\n",
      "============================================================\n",
      "Zero-shot Qwen [BN] | Resuming with 0 cached / 168 total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zero-shot Qwen [BN]: 100%|██████████| 21/21 [00:18<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot Qwen [BN] | Newly computed: 168 | Cached at start: 0 | Total: 168 | Elapsed: 18.5s | 0.110s/example (new only)\n",
      "\n",
      "[Qwen Zero-shot BN] Macro-F1: 0.3971\n",
      "[Qwen Zero-shot BN] Macro-recall: 0.5776\n",
      "[Qwen Zero-shot BN] Accuracy: 0.3988\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2794    0.9268    0.4294        41\n",
      "           1     0.9062    0.2283    0.3648       127\n",
      "\n",
      "    accuracy                         0.3988       168\n",
      "   macro avg     0.5928    0.5776    0.3971       168\n",
      "weighted avg     0.7533    0.3988    0.3805       168\n",
      "\n",
      "Confusion matrix:\n",
      "[[38  3]\n",
      " [98 29]]\n",
      "\n",
      "Saved predictions for BN to: outputs_lrl_qwen/BN_qwen_zeroshot_predictions.csv\n",
      "\n",
      "============================================================\n",
      "Processing language: ML\n",
      "============================================================\n",
      "Zero-shot Qwen [ML] | Resuming with 0 cached / 50 total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zero-shot Qwen [ML]: 100%|██████████| 7/7 [00:03<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot Qwen [ML] | Newly computed: 50 | Cached at start: 0 | Total: 50 | Elapsed: 3.5s | 0.069s/example (new only)\n",
      "\n",
      "[Qwen Zero-shot ML] Macro-F1: 0.3912\n",
      "[Qwen Zero-shot ML] Macro-recall: 0.5333\n",
      "[Qwen Zero-shot ML] Accuracy: 0.4000\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3171    0.8667    0.4643        15\n",
      "           1     0.7778    0.2000    0.3182        35\n",
      "\n",
      "    accuracy                         0.4000        50\n",
      "   macro avg     0.5474    0.5333    0.3912        50\n",
      "weighted avg     0.6396    0.4000    0.3620        50\n",
      "\n",
      "Confusion matrix:\n",
      "[[13  2]\n",
      " [28  7]]\n",
      "\n",
      "Saved predictions for ML to: outputs_lrl_qwen/ML_qwen_zeroshot_predictions.csv\n",
      "\n",
      "============================================================\n",
      "Processing language: PA\n",
      "============================================================\n",
      "Zero-shot Qwen [PA] | Resuming with 0 cached / 50 total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zero-shot Qwen [PA]: 100%|██████████| 7/7 [00:03<00:00,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot Qwen [PA] | Newly computed: 50 | Cached at start: 0 | Total: 50 | Elapsed: 3.2s | 0.064s/example (new only)\n",
      "\n",
      "[Qwen Zero-shot PA] Macro-F1: 0.3779\n",
      "[Qwen Zero-shot PA] Macro-recall: 0.4578\n",
      "[Qwen Zero-shot PA] Accuracy: 0.4200\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4146    0.7727    0.5397        22\n",
      "           1     0.4444    0.1429    0.2162        28\n",
      "\n",
      "    accuracy                         0.4200        50\n",
      "   macro avg     0.4295    0.4578    0.3779        50\n",
      "weighted avg     0.4313    0.4200    0.3585        50\n",
      "\n",
      "Confusion matrix:\n",
      "[[17  5]\n",
      " [24  4]]\n",
      "\n",
      "Saved predictions for PA to: outputs_lrl_qwen/PA_qwen_zeroshot_predictions.csv\n",
      "\n",
      "Finished all languages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 08 - Run zero-shot evaluation for ALL languages and collect results (Qwen)\n",
    "\n",
    "all_results = []  # will store per-language metric summaries\n",
    "\n",
    "for lang in languages:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Processing language: {lang}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    df_lang = full_df[full_df[\"Language\"] == lang].copy()\n",
    "    if df_lang.empty:\n",
    "        print(f\"No rows for language {lang}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    if \"Label\" not in df_lang.columns:\n",
    "        print(f\"No 'Label' column for language {lang}, skipping (no metrics possible).\")\n",
    "        continue\n",
    "\n",
    "    cache_zero = OUT_DIR / f\"cache_qwen_zeroshot_{lang}.csv\"\n",
    "\n",
    "    yhat = progressive_predict_zero_shot_batched_for_language(\n",
    "        df_lang,\n",
    "        language=lang,\n",
    "        cache_path=cache_zero,\n",
    "        desc=\"Zero-shot Qwen\"\n",
    "    )\n",
    "\n",
    "    ytrue = df_lang[\"Label\"].astype(int).tolist()\n",
    "\n",
    "    f1_macro = f1_score(ytrue, yhat, average=\"macro\")\n",
    "    recall_macro = recall_score(ytrue, yhat, average=\"macro\")\n",
    "    acc = accuracy_score(ytrue, yhat)\n",
    "\n",
    "    print(f\"\\n[Qwen Zero-shot {lang}] Macro-F1: {f1_macro:.4f}\")\n",
    "    print(f\"[Qwen Zero-shot {lang}] Macro-recall: {recall_macro:.4f}\")\n",
    "    print(f\"[Qwen Zero-shot {lang}] Accuracy: {acc:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(ytrue, yhat, digits=4))\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix(ytrue, yhat))\n",
    "\n",
    "    # Save per-language predictions\n",
    "    out_df = df_lang.copy()\n",
    "    out_df[\"PredLabel_ZeroShot\"] = yhat\n",
    "    pred_path = OUT_DIR / f\"{lang}_qwen_zeroshot_predictions.csv\"\n",
    "    out_df.to_csv(pred_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"\\nSaved predictions for {lang} to: {pred_path}\")\n",
    "\n",
    "    all_results.append({\n",
    "        \"Language\": lang,\n",
    "        \"NumExamples\": len(df_lang),\n",
    "        \"MacroF1\": f1_macro,\n",
    "        \"MacroRecall\": recall_macro,\n",
    "        \"Accuracy\": acc,\n",
    "    })\n",
    "\n",
    "print(\"\\nFinished all languages.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f8bb4",
   "metadata": {},
   "source": [
    "## Show summary table and save metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe3598cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Qwen Zero-shot Summary over all languages =====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>NumExamples</th>\n",
       "      <th>MacroF1</th>\n",
       "      <th>MacroRecall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BN</td>\n",
       "      <td>168</td>\n",
       "      <td>0.397079</td>\n",
       "      <td>0.577588</td>\n",
       "      <td>0.39881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ML</td>\n",
       "      <td>50</td>\n",
       "      <td>0.391234</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.40000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PA</td>\n",
       "      <td>50</td>\n",
       "      <td>0.377949</td>\n",
       "      <td>0.457792</td>\n",
       "      <td>0.42000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Language  NumExamples   MacroF1  MacroRecall  Accuracy\n",
       "0       BN          168  0.397079     0.577588   0.39881\n",
       "1       ML           50  0.391234     0.533333   0.40000\n",
       "2       PA           50  0.377949     0.457792   0.42000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved summary CSV to: outputs_lrl_qwen/qwen_zeroshot_summary_all_languages.csv\n",
      "\n",
      "Resume cache files (delete to start fresh per language):\n",
      "- outputs_lrl_qwen/cache_qwen_zeroshot_BN.csv\n",
      "- outputs_lrl_qwen/cache_qwen_zeroshot_ML.csv\n",
      "- outputs_lrl_qwen/cache_qwen_zeroshot_PA.csv\n"
     ]
    }
   ],
   "source": [
    "# 09 - Show summary table and save metadata\n",
    "\n",
    "if all_results:\n",
    "    summary_df = pd.DataFrame(all_results)\n",
    "    print(\"\\n===== Qwen Zero-shot Summary over all languages =====\")\n",
    "    display(summary_df)\n",
    "\n",
    "    summary_path = OUT_DIR / \"qwen_zeroshot_summary_all_languages.csv\"\n",
    "    summary_df.to_csv(summary_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"\\nSaved summary CSV to: {summary_path}\")\n",
    "else:\n",
    "    print(\"No results collected (check dataset / labels).\")\n",
    "\n",
    "# Save simple run metadata\n",
    "with open(OUT_DIR / \"run_lrl_qwen_zeroshot.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"MODEL_NAME={MODEL_NAME}\\n\")\n",
    "    f.write(f\"DEVICE={device.type}\\n\")\n",
    "    f.write(f\"BATCH_GEN_ZERO={BATCH_GEN_ZERO}\\n\")\n",
    "    f.write(f\"DATA={MULTI_CSV}\\n\")\n",
    "    if all_results:\n",
    "        for row in all_results:\n",
    "            f.write(\n",
    "                f\"LANG={row['Language']},N={row['NumExamples']},\"\n",
    "                f\"MacroF1={row['MacroF1']:.4f},\"\n",
    "                f\"MacroRecall={row['MacroRecall']:.4f},\"\n",
    "                f\"Acc={row['Accuracy']:.4f}\\n\"\n",
    "            )\n",
    "\n",
    "print(\"\\nResume cache files (delete to start fresh per language):\")\n",
    "for lang in languages:\n",
    "    cache_path = OUT_DIR / f\"cache_qwen_zeroshot_{lang}.csv\"\n",
    "    if cache_path.exists():\n",
    "        print(f\"- {cache_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "nlp"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
