{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa339937",
   "metadata": {},
   "source": [
    "## Setup: CPU-only, imports, seeds, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c5b7d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01 - Setup: CPU-only, imports, seeds, paths\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.set_num_threads(max(1, os.cpu_count() // 2))\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DATA_DIR = Path(\"SemEval_2022_Task2-idiomaticity/SubTaskA\")\n",
    "TRAIN_ONE_SHOT = DATA_DIR / \"Data\" / \"train_one_shot.csv\"\n",
    "TRAIN_ZERO_SHOT = DATA_DIR / \"Data\" / \"train_zero_shot.csv\"\n",
    "DEV = DATA_DIR / \"Data\" / \"dev.csv\"\n",
    "DEV_GOLD = DATA_DIR / \"Data\" / \"dev_gold.csv\"\n",
    "EVAL = DATA_DIR / \"Data\" / \"eval.csv\"\n",
    "EVAL_SUB_FMT = DATA_DIR / \"Data\" / \"eval_submission_format.csv\"\n",
    "\n",
    "OUT_DIR = Path(\"outputs_en_xlmr\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"xlm-roberta-base\"  # change to xlm-roberta-large if you can tolerate slower CPU training\n",
    "BATCH_SIZE = 8                   # CPU-friendly; lower if memory constrained\n",
    "MAX_LEN = 256                    # long enough for Prev/Target/Next\n",
    "EPOCHS = 2                       # keep small for CPU; adjust if you have patience\n",
    "LR = 2e-5\n",
    "WARMUP_RATIO = 0.06\n",
    "WEIGHT_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf771532",
   "metadata": {},
   "source": [
    "## IO helpers and data preparation for Subtask A (EN only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8950a364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 - IO helpers and data preparation for Subtask A (EN only)\n",
    "\n",
    "def load_any_csv(path: Path) -> pd.DataFrame:\n",
    "    # auto-detect separator\n",
    "    return pd.read_csv(path, sep=None, engine=\"python\", dtype=str)\n",
    "\n",
    "def ensure_label_int(df: pd.DataFrame, col=\"Label\") -> pd.DataFrame:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(int)\n",
    "    return df\n",
    "\n",
    "def mark_first_case_insensitive(text: str, needle: str, ltag: str=\"<mwe>\", rtag: str=\"</mwe>\") -> str:\n",
    "    if not isinstance(text, str) or not isinstance(needle, str):\n",
    "        return text\n",
    "    low_t = text.lower()\n",
    "    low_n = needle.lower()\n",
    "    idx = low_t.find(low_n)\n",
    "    if idx == -1:\n",
    "        return text\n",
    "    return text[:idx] + ltag + text[idx:idx+len(needle)] + rtag + text[idx+len(needle):]\n",
    "\n",
    "def build_input(prev: str, target: str, nxt: str, mwe: str, sep_token: str) -> str:\n",
    "    target_marked = mark_first_case_insensitive(target, mwe, \"<mwe>\", \"</mwe>\")\n",
    "    # XLM-R sep token is '</s>'; include context on both sides\n",
    "    prev = \"\" if pd.isna(prev) else prev\n",
    "    nxt = \"\" if pd.isna(nxt) else nxt\n",
    "    return f\"{prev} {sep_token} {target_marked} {sep_token} {nxt}\".strip()\n",
    "\n",
    "def prepare_supervised_frame(train_path: Path, dev_path: Path, dev_gold_path: Path, language=\"EN\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train_df = load_any_csv(train_path)\n",
    "    dev_df = load_any_csv(dev_path)\n",
    "    gold_df = load_any_csv(dev_gold_path)\n",
    "\n",
    "    # Harmonize columns\n",
    "    train_df.columns = [c.strip() for c in train_df.columns]\n",
    "    dev_df.columns = [c.strip() for c in dev_df.columns]\n",
    "    gold_df.columns = [c.strip() for c in gold_df.columns]\n",
    "\n",
    "    # Filter EN\n",
    "    train_df = train_df[train_df[\"Language\"] == language].copy()\n",
    "    dev_df = dev_df[dev_df[\"Language\"] == language].copy()\n",
    "\n",
    "    # Attach gold labels to dev\n",
    "    dev_gold = gold_df[gold_df[\"Language\"] == language][[\"ID\", \"Label\"]].copy()\n",
    "    dev_gold[\"ID\"] = dev_gold[\"ID\"].astype(str)\n",
    "    dev_df[\"ID\"] = dev_df[\"ID\"].astype(str)\n",
    "    dev_labeled = dev_df.merge(dev_gold, on=\"ID\", how=\"left\")\n",
    "    dev_labeled = ensure_label_int(dev_labeled, \"Label\")\n",
    "\n",
    "    # Ensure train label int\n",
    "    train_df = ensure_label_int(train_df, \"Label\")\n",
    "\n",
    "    return train_df, dev_labeled\n",
    "\n",
    "def prepare_eval_frame(eval_path: Path, language=\"EN\") -> pd.DataFrame:\n",
    "    eval_df = load_any_csv(eval_path)\n",
    "    eval_df.columns = [c.strip() for c in eval_df.columns]\n",
    "    eval_df = eval_df[eval_df[\"Language\"] == language].copy()\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c24c8e",
   "metadata": {},
   "source": [
    "## Dataset & collate for XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc418e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03 - Dataset & collate for XLM-R\n",
    "\n",
    "class IdiomDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer, max_len: int, is_infer: bool=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.is_infer = is_infer\n",
    "\n",
    "        self.sep = tokenizer.sep_token if tokenizer.sep_token is not None else \"</s>\"\n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "\n",
    "        for i, row in self.df.iterrows():\n",
    "            prev = row.get(\"Previous\", \"\")\n",
    "            target = row.get(\"Target\", \"\")\n",
    "            nxt = row.get(\"Next\", \"\")\n",
    "            mwe = row.get(\"MWE\", \"\")\n",
    "            text = build_input(prev, target, nxt, mwe, self.sep)\n",
    "            self.texts.append(text)\n",
    "            if not is_infer:\n",
    "                self.labels.append(int(row[\"Label\"]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            \"text\": self.texts[idx]\n",
    "        }\n",
    "        if not self.is_infer:\n",
    "            item[\"label\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "def collate_fn(batch, tokenizer, max_len: int, is_infer: bool=False):\n",
    "    texts = [b[\"text\"] for b in batch]\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    if not is_infer:\n",
    "        labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n",
    "        return enc, labels\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d93d14e",
   "metadata": {},
   "source": [
    "## Model, training, evaluation (CPU-only) + weighted-loss variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b37a083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04 - Model, training, evaluation (CPU-only) + weighted-loss variant\n",
    "\n",
    "def build_model_and_tokenizer(model_name: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    special_tokens = {\"additional_special_tokens\": [\"<mwe>\", \"</mwe>\"]}\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "def run_epoch(model, loader, tokenizer, optimizer, scheduler, train_mode: bool):\n",
    "    if train_mode:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "        if train_mode:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        enc, labels = batch\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(train_mode):\n",
    "            out = model(**enc, labels=labels)\n",
    "            loss = out.loss\n",
    "            logits = out.logits\n",
    "\n",
    "            if train_mode:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        preds = torch.argmax(logits, dim=-1).detach().cpu().numpy().tolist()\n",
    "        labs = labels.detach().cpu().numpy().tolist()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labs)\n",
    "\n",
    "    avg_loss = total_loss / len(all_labels)\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    return avg_loss, macro_f1, all_labels, all_preds\n",
    "\n",
    "# ---- weighted-loss version for tiny/imbalanced one-shot ----\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def run_epoch_weighted(model, loader, tokenizer, optimizer, scheduler, train_mode: bool, class_weights: Optional[torch.Tensor]=None):\n",
    "    if train_mode:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "        if train_mode:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        enc, labels = batch\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(train_mode):\n",
    "            out = model(**enc)\n",
    "            logits = out.logits\n",
    "            if class_weights is None:\n",
    "                loss = F.cross_entropy(logits, labels)\n",
    "            else:\n",
    "                loss = F.cross_entropy(logits, labels, weight=class_weights)\n",
    "\n",
    "            if train_mode:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        preds = torch.argmax(logits, dim=-1).detach().cpu().numpy().tolist()\n",
    "        labs = labels.detach().cpu().numpy().tolist()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labs)\n",
    "\n",
    "    avg_loss = total_loss / len(all_labels)\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    return avg_loss, macro_f1, all_labels, all_preds\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, loader, tokenizer):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    for batch in loader:\n",
    "        enc = batch\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        logits = model(**enc).logits\n",
    "        preds = torch.argmax(logits, dim=-1).cpu().numpy().tolist()\n",
    "        all_preds.extend(preds)\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258d5207",
   "metadata": {},
   "source": [
    "## Data loaders factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a4239e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05 - Data loaders factory\n",
    "\n",
    "def make_loaders(train_df: pd.DataFrame, dev_df: pd.DataFrame, tokenizer, max_len: int, batch_size: int):\n",
    "    train_ds = IdiomDataset(train_df, tokenizer, max_len, is_infer=False)\n",
    "    dev_ds = IdiomDataset(dev_df, tokenizer, max_len, is_infer=False)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer, max_len, is_infer=False)\n",
    "    )\n",
    "    dev_loader = DataLoader(\n",
    "        dev_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer, max_len, is_infer=False)\n",
    "    )\n",
    "    return train_loader, dev_loader\n",
    "\n",
    "def make_infer_loader(df: pd.DataFrame, tokenizer, max_len: int, batch_size: int):\n",
    "    ds = IdiomDataset(df, tokenizer, max_len, is_infer=True)\n",
    "    loader = DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer, max_len, is_infer=True)\n",
    "    )\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50399d10",
   "metadata": {},
   "source": [
    "## Training runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c21fae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06 - Training runner\n",
    "\n",
    "def train_and_eval(\n",
    "    train_df: pd.DataFrame,\n",
    "    dev_df: pd.DataFrame,\n",
    "    run_name: str,\n",
    "    model_name: str = MODEL_NAME,\n",
    "    max_len: int = MAX_LEN,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    epochs: int = EPOCHS,\n",
    "    lr: float = LR,\n",
    "    weight_decay: float = WEIGHT_DECAY,\n",
    "    warmup_ratio: float = WARMUP_RATIO\n",
    "):\n",
    "    ckpt_dir = OUT_DIR / f\"ckpt_{run_name}\"\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model, tokenizer = build_model_and_tokenizer(model_name)\n",
    "    train_loader, dev_loader = make_loaders(train_df, dev_df, tokenizer, max_len, batch_size)\n",
    "\n",
    "    total_steps = epochs * len(train_loader)\n",
    "    warmup_steps = max(1, int(warmup_ratio * total_steps))\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "    best_f1 = -1.0\n",
    "    best_path = ckpt_dir / \"best.pt\"\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        tr_loss, tr_f1, _, _ = run_epoch(model, train_loader, tokenizer, optimizer, scheduler, train_mode=True)\n",
    "        dv_loss, dv_f1, y_true, y_pred = run_epoch(model, dev_loader, tokenizer, optimizer=None, scheduler=None, train_mode=False)\n",
    "\n",
    "        print(f\"[{run_name}] Epoch {epoch}/{epochs} | Train loss {tr_loss:.4f} F1 {tr_f1:.4f} | Dev loss {dv_loss:.4f} F1 {dv_f1:.4f}\")\n",
    "\n",
    "        if dv_f1 > best_f1:\n",
    "            best_f1 = dv_f1\n",
    "            torch.save({\"model_state\": model.state_dict(), \"tokenizer\": tokenizer.get_vocab(), \"f1\": best_f1}, best_path)\n",
    "\n",
    "    print(f\"[{run_name}] Best dev macro-F1: {best_f1:.4f}\")\n",
    "    return best_path, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71a4e01",
   "metadata": {},
   "source": [
    "## One-shot (EN) with class-weighted loss, longer training on CPU, and eval submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a527e721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[oneshot_en_weighted] Epoch 1/6 | Train loss 0.6872 F1 0.5220 | Dev loss 0.6936 F1 0.3155\n",
      "[oneshot_en_weighted] Epoch 2/6 | Train loss 0.6971 F1 0.3746 | Dev loss 0.6950 F1 0.2809\n",
      "[oneshot_en_weighted] Epoch 3/6 | Train loss 0.6827 F1 0.5402 | Dev loss 0.6899 F1 0.6680\n",
      "[oneshot_en_weighted] Epoch 4/6 | Train loss 0.6842 F1 0.5646 | Dev loss 0.6862 F1 0.4116\n",
      "[oneshot_en_weighted] Epoch 5/6 | Train loss 0.6793 F1 0.5735 | Dev loss 0.6864 F1 0.5660\n",
      "[oneshot_en_weighted] Epoch 6/6 | Train loss 0.6633 F1 0.5496 | Dev loss 0.6851 F1 0.4906\n",
      "[oneshot_en_weighted] Best dev macro-F1: 0.6680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote outputs_en_xlmr/eval_submission_en_oneshot_weighted.csv\n"
     ]
    }
   ],
   "source": [
    "# 07 - One-shot (EN) with class-weighted loss, longer training on CPU, and eval submission\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "train_1s_df, dev_1s_df = prepare_supervised_frame(TRAIN_ONE_SHOT, DEV, DEV_GOLD, language=\"EN\")\n",
    "\n",
    "label_counts = Counter(train_1s_df[\"Label\"].astype(int).tolist())\n",
    "num0, num1 = label_counts.get(0, 1), label_counts.get(1, 1)\n",
    "total = num0 + num1\n",
    "w0 = total / (2.0 * num0)\n",
    "w1 = total / (2.0 * num1)\n",
    "class_weights = torch.tensor([w0, w1], dtype=torch.float32, device=device)\n",
    "\n",
    "model_1s, tok_1s = build_model_and_tokenizer(MODEL_NAME)\n",
    "train_loader_1s, dev_loader_1s = make_loaders(train_1s_df, dev_1s_df, tok_1s, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "EPOCHS_1S = 6\n",
    "LR_1S = 1.5e-5\n",
    "WARMUP_RATIO_1S = 0.1\n",
    "\n",
    "total_steps = EPOCHS_1S * len(train_loader_1s)\n",
    "warmup_steps = max(1, int(WARMUP_RATIO_1S * total_steps))\n",
    "optimizer_1s = torch.optim.AdamW(model_1s.parameters(), lr=LR_1S, weight_decay=WEIGHT_DECAY)\n",
    "scheduler_1s = get_linear_schedule_with_warmup(optimizer_1s, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "best_f1 = -1.0\n",
    "best_dir_1s = OUT_DIR / \"ckpt_oneshot_en_xlmr_weighted\"\n",
    "best_dir_1s.mkdir(parents=True, exist_ok=True)\n",
    "best_file_1s = best_dir_1s / \"best.pt\"\n",
    "\n",
    "for epoch in range(1, EPOCHS_1S + 1):\n",
    "    tr_loss, tr_f1, _, _ = run_epoch_weighted(model_1s, train_loader_1s, tok_1s, optimizer_1s, scheduler_1s, train_mode=True, class_weights=class_weights)\n",
    "    dv_loss, dv_f1, y_true, y_pred = run_epoch_weighted(model_1s, dev_loader_1s, tok_1s, optimizer_1s, scheduler_1s, train_mode=False, class_weights=None)\n",
    "    print(f\"[oneshot_en_weighted] Epoch {epoch}/{EPOCHS_1S} | Train loss {tr_loss:.4f} F1 {tr_f1:.4f} | Dev loss {dv_loss:.4f} F1 {dv_f1:.4f}\")\n",
    "    if dv_f1 > best_f1:\n",
    "        best_f1 = dv_f1\n",
    "        torch.save({\"model_state\": model_1s.state_dict(), \"f1\": best_f1}, best_file_1s)\n",
    "\n",
    "print(f\"[oneshot_en_weighted] Best dev macro-F1: {best_f1:.4f}\")\n",
    "\n",
    "# ---- reload best (CPU) and produce eval predictions ----\n",
    "model_1s_eval = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "tok_1s.add_special_tokens({\"additional_special_tokens\": [\"<mwe>\", \"</mwe>\"]})\n",
    "model_1s_eval.resize_token_embeddings(len(tok_1s))\n",
    "state = torch.load(best_file_1s, map_location=\"cpu\")\n",
    "model_1s_eval.load_state_dict(state[\"model_state\"])\n",
    "model_1s_eval.to(device)\n",
    "model_1s_eval.eval()\n",
    "\n",
    "eval_en_df = prepare_eval_frame(EVAL, language=\"EN\")\n",
    "eval_loader_1s = make_infer_loader(eval_en_df, tok_1s, MAX_LEN, BATCH_SIZE)\n",
    "eval_preds_1s = predict(model_1s_eval, eval_loader_1s, tok_1s)\n",
    "\n",
    "sub_1s = pd.DataFrame({\n",
    "    \"ID\": eval_en_df[\"ID\"].astype(str),\n",
    "    \"Language\": eval_en_df[\"Language\"],\n",
    "    \"Setting\": [\"zero_shot\"] * len(eval_en_df),\n",
    "    \"Label\": eval_preds_1s\n",
    "})\n",
    "sub_path_1s = OUT_DIR / \"eval_submission_en_oneshot_weighted.csv\"\n",
    "sub_1s.to_csv(sub_path_1s, index=False)\n",
    "print(f\"Wrote {sub_path_1s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634d3483",
   "metadata": {},
   "source": [
    "## Zero-shot (EN): load, train, eval, and write eval predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f94c555e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 08 - Zero-shot (EN): load, train, eval, and write eval predictions\u001b[39;00m\n\u001b[1;32m      3\u001b[0m train_0s_df, dev_0s_df \u001b[38;5;241m=\u001b[39m prepare_supervised_frame(TRAIN_ZERO_SHOT, DEV, DEV_GOLD, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m best_zeroshot_path, zeroshot_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_0s_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdev_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_0s_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzeroshot_en_xlmr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Reload best for inference\u001b[39;00m\n\u001b[1;32m     12\u001b[0m model_zeroshot \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_NAME, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m, in \u001b[0;36mtrain_and_eval\u001b[0;34m(train_df, dev_df, run_name, model_name, max_len, batch_size, epochs, lr, weight_decay, warmup_ratio)\u001b[0m\n\u001b[1;32m     27\u001b[0m best_path \u001b[38;5;241m=\u001b[39m ckpt_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 30\u001b[0m     tr_loss, tr_f1, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     dv_loss, dv_f1, y_true, y_pred \u001b[38;5;241m=\u001b[39m run_epoch(model, dev_loader, tokenizer, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, train_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m F1 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Dev loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdv_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m F1 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdv_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 38\u001b[0m, in \u001b[0;36mrun_epoch\u001b[0;34m(model, loader, tokenizer, optimizer, scheduler, train_mode)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_mode:\n\u001b[1;32m     37\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 38\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.venvs/nlp/lib/python3.9/site-packages/torch/nn/utils/clip_grad.py:36\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/nlp/lib/python3.9/site-packages/torch/nn/utils/clip_grad.py:221\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m    216\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    217\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`parameters` is an empty generator, no gradient clipping will occur.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    218\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    219\u001b[0m         )\n\u001b[1;32m    220\u001b[0m grads \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m--> 221\u001b[0m total_norm \u001b[38;5;241m=\u001b[39m \u001b[43m_get_total_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_if_nonfinite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m _clip_grads_with_norm_(parameters, max_norm, total_norm, foreach)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_norm\n",
      "File \u001b[0;32m~/.venvs/nlp/lib/python3.9/site-packages/torch/nn/utils/clip_grad.py:36\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/nlp/lib/python3.9/site-packages/torch/nn/utils/clip_grad.py:89\u001b[0m, in \u001b[0;36m_get_total_norm\u001b[0;34m(tensors, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (device, _), ([device_tensors], _) \u001b[38;5;129;01min\u001b[39;00m grouped_tensors\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m _has_foreach_support(device_tensors, device)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m     87\u001b[0m         foreach \u001b[38;5;129;01mand\u001b[39;00m _device_has_foreach_support(device)\n\u001b[1;32m     88\u001b[0m     ):\n\u001b[0;32m---> 89\u001b[0m         norms\u001b[38;5;241m.\u001b[39mextend(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m foreach:\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     92\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach=True was passed, but can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt use the foreach API on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 08 - Zero-shot (EN): load, train, eval, and write eval predictions\n",
    "\n",
    "train_0s_df, dev_0s_df = prepare_supervised_frame(TRAIN_ZERO_SHOT, DEV, DEV_GOLD, language=\"EN\")\n",
    "\n",
    "best_zeroshot_path, zeroshot_tokenizer = train_and_eval(\n",
    "    train_df=train_0s_df,\n",
    "    dev_df=dev_0s_df,\n",
    "    run_name=\"zeroshot_en_xlmr\"\n",
    ")\n",
    "\n",
    "# Reload best for inference\n",
    "model_zeroshot = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "special_tokens = {\"additional_special_tokens\": [\"<mwe>\", \"</mwe>\"]}\n",
    "zeroshot_tokenizer.add_special_tokens(special_tokens)\n",
    "model_zeroshot.resize_token_embeddings(len(zeroshot_tokenizer))\n",
    "state = torch.load(OUT_DIR / \"ckpt_zeroshot_en_xlmr\" / \"best.pt\", map_location=\"cpu\")\n",
    "model_zeroshot.load_state_dict(state[\"model_state\"])\n",
    "model_zeroshot.to(device)\n",
    "model_zeroshot.eval()\n",
    "\n",
    "eval_en_df = prepare_eval_frame(EVAL, language=\"EN\")\n",
    "eval_loader_0s = make_infer_loader(eval_en_df, zeroshot_tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "eval_preds_0s = predict(model_zeroshot, eval_loader_0s, zeroshot_tokenizer)\n",
    "\n",
    "sub_0s = pd.DataFrame({\n",
    "    \"ID\": eval_en_df[\"ID\"].astype(str),\n",
    "    \"Language\": eval_en_df[\"Language\"],\n",
    "    \"Setting\": [\"zero_shot\"] * len(eval_en_df),\n",
    "    \"Label\": eval_preds_0s\n",
    "})\n",
    "sub_path_0s = OUT_DIR / \"eval_submission_en_zeroshot.csv\"\n",
    "sub_0s.to_csv(sub_path_0s, index=False)\n",
    "print(f\"Wrote {sub_path_0s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb662f38",
   "metadata": {},
   "source": [
    "## Dev-set diagnostics: confusion matrix & report for best runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fde64fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[One-shot EN XLM-R (weighted)] Dev macro-F1: 0.6680\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6885    0.4615    0.5526       182\n",
      "           1     0.7151    0.8662    0.7834       284\n",
      "\n",
      "    accuracy                         0.7082       466\n",
      "   macro avg     0.7018    0.6639    0.6680       466\n",
      "weighted avg     0.7047    0.7082    0.6933       466\n",
      "\n",
      "[[ 84  98]\n",
      " [ 38 246]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Zero-shot EN XLM-R] Dev macro-F1: 0.7451\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6667    0.7363    0.6997       182\n",
      "           1     0.8189    0.7641    0.7905       284\n",
      "\n",
      "    accuracy                         0.7532       466\n",
      "   macro avg     0.7428    0.7502    0.7451       466\n",
      "weighted avg     0.7594    0.7532    0.7551       466\n",
      "\n",
      "[[134  48]\n",
      " [ 67 217]]\n"
     ]
    }
   ],
   "source": [
    "# 09 - Dev-set diagnostics: confusion matrix & report for best runs\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def eval_on_dev(best_ckpt_path: Path, tokenizer, dev_df: pd.DataFrame, tag: str):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "    tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<mwe>\", \"</mwe>\"]})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    state = torch.load(best_ckpt_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(state[\"model_state\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    dev_loader = DataLoader(\n",
    "        IdiomDataset(dev_df, tokenizer, MAX_LEN, is_infer=False),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer, MAX_LEN, is_infer=False)\n",
    "    )\n",
    "\n",
    "    _, f1, y_true, y_pred = run_epoch(model, dev_loader, tokenizer, optimizer=None, scheduler=None, train_mode=False)\n",
    "    print(f\"[{tag}] Dev macro-F1: {f1:.4f}\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# evaluate weighted one-shot\n",
    "eval_on_dev(OUT_DIR / \"ckpt_oneshot_en_xlmr_weighted\" / \"best.pt\", tok_1s, dev_1s_df, \"One-shot EN XLM-R (weighted)\")\n",
    "\n",
    "# evaluate zero-shot (original)\n",
    "eval_on_dev(OUT_DIR / \"ckpt_zeroshot_en_xlmr\" / \"best.pt\", zeroshot_tokenizer, dev_0s_df, \"Zero-shot EN XLM-R\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8d665c",
   "metadata": {},
   "source": [
    "## Save final best checkpoints to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "651e81b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final checkpoints.\n"
     ]
    }
   ],
   "source": [
    "# 10 - Save final best checkpoints to disk\n",
    "\n",
    "final_oneshot_dir = OUT_DIR / \"final_oneshot_en_xlmr\"\n",
    "final_zeroshot_dir = OUT_DIR / \"final_zeroshot_en_xlmr\"\n",
    "final_oneshot_dir.mkdir(parents=True, exist_ok=True)\n",
    "final_zeroshot_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save tokenizer vocab and model weights for reproducibility\n",
    "# One-shot\n",
    "torch.save(torch.load(OUT_DIR / \"ckpt_oneshot_en_xlmr\" / \"best.pt\", map_location=\"cpu\"), final_oneshot_dir / \"model.pt\")\n",
    "\n",
    "# Zero-shot\n",
    "torch.save(torch.load(OUT_DIR / \"ckpt_zeroshot_en_xlmr\" / \"best.pt\", map_location=\"cpu\"), final_zeroshot_dir / \"model.pt\")\n",
    "\n",
    "print(\"Saved final checkpoints.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
