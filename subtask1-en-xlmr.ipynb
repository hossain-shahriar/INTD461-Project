{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa339937",
   "metadata": {},
   "source": [
    "## Setup: single switch for CPU/GPU, imports, seeds, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c5b7d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shahriar/.venvs/nlp/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/shahriar/.venvs/nlp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 01 - Setup: single switch for CPU/GPU, imports, seeds, paths\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "RUN_DEVICE = \"cpu\"  # change to \"gpu\" to use CUDA if available\n",
    "\n",
    "if RUN_DEVICE.lower() == \"gpu\" and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.set_num_threads(max(1, os.cpu_count() // 2))\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DATA_DIR = Path(\"SemEval_2022_Task2-idiomaticity/SubTaskA\")\n",
    "TRAIN_ONE_SHOT = DATA_DIR / \"Data\" / \"train_one_shot.csv\"\n",
    "TRAIN_ZERO_SHOT = DATA_DIR / \"Data\" / \"train_zero_shot.csv\"\n",
    "DEV = DATA_DIR / \"Data\" / \"dev.csv\"\n",
    "DEV_GOLD = DATA_DIR / \"Data\" / \"dev_gold.csv\"\n",
    "EVAL = DATA_DIR / \"Data\" / \"eval.csv\"\n",
    "EVAL_SUB_FMT = DATA_DIR / \"Data\" / \"eval_submission_format.csv\"\n",
    "\n",
    "OUT_DIR = Path(\"outputs_en_xlmr\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"xlm-roberta-base\"\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 3\n",
    "else:\n",
    "    BATCH_SIZE = 8\n",
    "    EPOCHS = 2\n",
    "\n",
    "MAX_LEN = 256\n",
    "LR = 2e-5\n",
    "WARMUP_RATIO = 0.06\n",
    "WEIGHT_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf771532",
   "metadata": {},
   "source": [
    "## IO helpers and data preparation for Subtask A (EN only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8950a364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 - IO helpers and data preparation for Subtask A (EN only)\n",
    "\n",
    "def load_any_csv(path: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(path, sep=None, engine=\"python\", dtype=str)\n",
    "\n",
    "def ensure_label_int(df: pd.DataFrame, col=\"Label\") -> pd.DataFrame:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(int)\n",
    "    return df\n",
    "\n",
    "def mark_first_case_insensitive(text: str, needle: str, ltag: str=\"<mwe>\", rtag: str=\"</mwe>\") -> str:\n",
    "    if not isinstance(text, str) or not isinstance(needle, str):\n",
    "        return text\n",
    "    low_t = text.lower()\n",
    "    low_n = needle.lower()\n",
    "    idx = low_t.find(low_n)\n",
    "    if idx == -1:\n",
    "        return text\n",
    "    return text[:idx] + ltag + text[idx:idx+len(needle)] + rtag + text[idx+len(needle):]\n",
    "\n",
    "def build_input(prev: str, target: str, nxt: str, mwe: str, sep_token: str) -> str:\n",
    "    target_marked = mark_first_case_insensitive(target, mwe, \"<mwe>\", \"</mwe>\")\n",
    "    prev = \"\" if pd.isna(prev) else prev\n",
    "    nxt = \"\" if pd.isna(nxt) else nxt\n",
    "    return f\"{prev} {sep_token} {target_marked} {sep_token} {nxt}\".strip()\n",
    "\n",
    "def prepare_supervised_frame(train_path: Path, dev_path: Path, dev_gold_path: Path, language=\"EN\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train_df = load_any_csv(train_path)\n",
    "    dev_df = load_any_csv(dev_path)\n",
    "    gold_df = load_any_csv(dev_gold_path)\n",
    "\n",
    "    train_df.columns = [c.strip() for c in train_df.columns]\n",
    "    dev_df.columns = [c.strip() for c in dev_df.columns]\n",
    "    gold_df.columns = [c.strip() for c in gold_df.columns]\n",
    "\n",
    "    train_df = train_df[train_df[\"Language\"] == language].copy()\n",
    "    dev_df = dev_df[dev_df[\"Language\"] == language].copy()\n",
    "\n",
    "    dev_gold = gold_df[gold_df[\"Language\"] == language][[\"ID\", \"Label\"]].copy()\n",
    "    dev_gold[\"ID\"] = dev_gold[\"ID\"].astype(str)\n",
    "    dev_df[\"ID\"] = dev_df[\"ID\"].astype(str)\n",
    "    dev_labeled = dev_df.merge(dev_gold, on=\"ID\", how=\"left\")\n",
    "    dev_labeled = ensure_label_int(dev_labeled, \"Label\")\n",
    "\n",
    "    train_df = ensure_label_int(train_df, \"Label\")\n",
    "    return train_df, dev_labeled\n",
    "\n",
    "def prepare_eval_frame(eval_path: Path, language=\"EN\") -> pd.DataFrame:\n",
    "    eval_df = load_any_csv(eval_path)\n",
    "    eval_df.columns = [c.strip() for c in eval_df.columns]\n",
    "    eval_df = eval_df[eval_df[\"Language\"] == language].copy()\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c24c8e",
   "metadata": {},
   "source": [
    "## Dataset & collate for XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc418e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03 - Dataset & collate for XLM-R\n",
    "\n",
    "class IdiomDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer, max_len: int, is_infer: bool=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.is_infer = is_infer\n",
    "\n",
    "        self.sep = tokenizer.sep_token if tokenizer.sep_token is not None else \"</s>\"\n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "\n",
    "        for _, row in self.df.iterrows():\n",
    "            prev = row.get(\"Previous\", \"\")\n",
    "            target = row.get(\"Target\", \"\")\n",
    "            nxt = row.get(\"Next\", \"\")\n",
    "            mwe = row.get(\"MWE\", \"\")\n",
    "            text = build_input(prev, target, nxt, mwe, self.sep)\n",
    "            self.texts.append(text)\n",
    "            if not is_infer:\n",
    "                self.labels.append(int(row[\"Label\"]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"text\": self.texts[idx]}\n",
    "        if not self.is_infer:\n",
    "            item[\"label\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "def collate_fn(batch, tokenizer, max_len: int, is_infer: bool=False):\n",
    "    texts = [b[\"text\"] for b in batch]\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    if not is_infer:\n",
    "        labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n",
    "        return enc, labels\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d93d14e",
   "metadata": {},
   "source": [
    "## Model, training, evaluation (+ weighted-loss variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b37a083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04 - Model, training, evaluation (+ weighted-loss variant)\n",
    "\n",
    "def build_model_and_tokenizer(model_name: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    special_tokens = {\"additional_special_tokens\": [\"<mwe>\", \"</mwe>\"]}\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "def run_epoch(model, loader, tokenizer, optimizer, scheduler, train_mode: bool):\n",
    "    model.train() if train_mode else model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "        if train_mode:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        enc, labels = batch\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(train_mode):\n",
    "            out = model(**enc, labels=labels)\n",
    "            loss = out.loss\n",
    "            logits = out.logits\n",
    "            if train_mode:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        preds = torch.argmax(logits, dim=-1).detach().cpu().numpy().tolist()\n",
    "        labs = labels.detach().cpu().numpy().tolist()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labs)\n",
    "\n",
    "    avg_loss = total_loss / len(all_labels)\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    return avg_loss, macro_f1, all_labels, all_preds\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def run_epoch_weighted(model, loader, tokenizer, optimizer, scheduler, train_mode: bool, class_weights: Optional[torch.Tensor]=None):\n",
    "    model.train() if train_mode else model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "        if train_mode:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        enc, labels = batch\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(train_mode):\n",
    "            out = model(**enc)\n",
    "            logits = out.logits\n",
    "            loss = F.cross_entropy(logits, labels, weight=class_weights)\n",
    "            if train_mode:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        preds = torch.argmax(logits, dim=-1).detach().cpu().numpy().tolist()\n",
    "        labs = labels.detach().cpu().numpy().tolist()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labs)\n",
    "\n",
    "    avg_loss = total_loss / len(all_labels)\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    return avg_loss, macro_f1, all_labels, all_preds\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, loader, tokenizer):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    for batch in loader:\n",
    "        enc = batch\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        logits = model(**enc).logits\n",
    "        preds = torch.argmax(logits, dim=-1).cpu().numpy().tolist()\n",
    "        all_preds.extend(preds)\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258d5207",
   "metadata": {},
   "source": [
    "## Data loaders factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a4239e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05 - Data loaders factory\n",
    "\n",
    "def make_loaders(train_df: pd.DataFrame, dev_df: pd.DataFrame, tokenizer, max_len: int, batch_size: int):\n",
    "    train_ds = IdiomDataset(train_df, tokenizer, max_len, is_infer=False)\n",
    "    dev_ds = IdiomDataset(dev_df, tokenizer, max_len, is_infer=False)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=(device.type == \"cuda\"),\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer, max_len, is_infer=False)\n",
    "    )\n",
    "    dev_loader = DataLoader(\n",
    "        dev_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=(device.type == \"cuda\"),\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer, max_len, is_infer=False)\n",
    "    )\n",
    "    return train_loader, dev_loader\n",
    "\n",
    "def make_infer_loader(df: pd.DataFrame, tokenizer, max_len: int, batch_size: int):\n",
    "    ds = IdiomDataset(df, tokenizer, max_len, is_infer=True)\n",
    "    loader = DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=(device.type == \"cuda\"),\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer, max_len, is_infer=True)\n",
    "    )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50399d10",
   "metadata": {},
   "source": [
    "## Training runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c21fae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06 - Training runner\n",
    "\n",
    "def train_and_eval(\n",
    "    train_df: pd.DataFrame,\n",
    "    dev_df: pd.DataFrame,\n",
    "    run_name: str,\n",
    "    model_name: str = MODEL_NAME,\n",
    "    max_len: int = MAX_LEN,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    epochs: int = EPOCHS,\n",
    "    lr: float = LR,\n",
    "    weight_decay: float = WEIGHT_DECAY,\n",
    "    warmup_ratio: float = WARMUP_RATIO\n",
    "):\n",
    "    ckpt_dir = OUT_DIR / f\"ckpt_{run_name}\"\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model, tokenizer = build_model_and_tokenizer(model_name)\n",
    "    train_loader, dev_loader = make_loaders(train_df, dev_df, tokenizer, max_len, batch_size)\n",
    "\n",
    "    total_steps = max(1, epochs * len(train_loader))\n",
    "    warmup_steps = max(1, int(warmup_ratio * total_steps))\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "    best_f1 = -1.0\n",
    "    best_path = ckpt_dir / \"best.pt\"\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        tr_loss, tr_f1, _, _ = run_epoch(model, train_loader, tokenizer, optimizer, scheduler, train_mode=True)\n",
    "        dv_loss, dv_f1, y_true, y_pred = run_epoch(model, dev_loader, tokenizer, optimizer=None, scheduler=None, train_mode=False)\n",
    "        print(f\"[{run_name}] Epoch {epoch}/{epochs} | Train loss {tr_loss:.4f} F1 {tr_f1:.4f} | Dev loss {dv_loss:.4f} F1 {dv_f1:.4f}\")\n",
    "        if dv_f1 > best_f1:\n",
    "            best_f1 = dv_f1\n",
    "            torch.save({\"model_state\": model.state_dict(), \"tokenizer\": tokenizer.get_vocab(), \"f1\": best_f1}, best_path)\n",
    "\n",
    "    print(f\"[{run_name}] Best dev macro-F1: {best_f1:.4f}\")\n",
    "    return best_path, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71a4e01",
   "metadata": {},
   "source": [
    "## One-shot (EN) with class-weighted loss and eval submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a527e721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[oneshot_en_weighted] Epoch 1/6 | Train loss 0.7160 F1 0.2689 | Dev loss 0.7195 F1 0.2809\n",
      "[oneshot_en_weighted] Epoch 2/6 | Train loss 0.6990 F1 0.4292 | Dev loss 0.7067 F1 0.2809\n",
      "[oneshot_en_weighted] Epoch 3/6 | Train loss 0.6864 F1 0.4991 | Dev loss 0.7023 F1 0.2809\n",
      "[oneshot_en_weighted] Epoch 4/6 | Train loss 0.6793 F1 0.5402 | Dev loss 0.6900 F1 0.5940\n",
      "[oneshot_en_weighted] Epoch 5/6 | Train loss 0.6970 F1 0.5655 | Dev loss 0.6818 F1 0.5922\n",
      "[oneshot_en_weighted] Epoch 6/6 | Train loss 0.6783 F1 0.5260 | Dev loss 0.6685 F1 0.5203\n",
      "[oneshot_en_weighted] Best dev macro-F1: 0.5940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote outputs_en_xlmr/eval_submission_en_oneshot_weighted.csv\n"
     ]
    }
   ],
   "source": [
    "# 07 - One-shot (EN) with class-weighted loss and eval submission\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "train_1s_df, dev_1s_df = prepare_supervised_frame(TRAIN_ONE_SHOT, DEV, DEV_GOLD, language=\"EN\")\n",
    "\n",
    "label_counts = Counter(train_1s_df[\"Label\"].astype(int).tolist())\n",
    "num0, num1 = label_counts.get(0, 1), label_counts.get(1, 1)\n",
    "total = num0 + num1\n",
    "w0 = total / (2.0 * num0)\n",
    "w1 = total / (2.0 * num1)\n",
    "class_weights = torch.tensor([w0, w1], dtype=torch.float32, device=device)\n",
    "\n",
    "model_1s, tok_1s = build_model_and_tokenizer(MODEL_NAME)\n",
    "train_loader_1s, dev_loader_1s = make_loaders(train_1s_df, dev_1s_df, tok_1s, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "EPOCHS_1S = 6 if device.type == \"cpu\" else 4\n",
    "LR_1S = 1.5e-5\n",
    "WARMUP_RATIO_1S = 0.1\n",
    "\n",
    "total_steps = max(1, EPOCHS_1S * len(train_loader_1s))\n",
    "warmup_steps = max(1, int(WARMUP_RATIO_1S * total_steps))\n",
    "optimizer_1s = torch.optim.AdamW(model_1s.parameters(), lr=LR_1S, weight_decay=WEIGHT_DECAY)\n",
    "scheduler_1s = get_linear_schedule_with_warmup(optimizer_1s, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "best_f1 = -1.0\n",
    "best_dir_1s = OUT_DIR / \"ckpt_oneshot_en_xlmr_weighted\"\n",
    "best_dir_1s.mkdir(parents=True, exist_ok=True)\n",
    "best_file_1s = best_dir_1s / \"best.pt\"\n",
    "\n",
    "for epoch in range(1, EPOCHS_1S + 1):\n",
    "    tr_loss, tr_f1, _, _ = run_epoch_weighted(model_1s, train_loader_1s, tok_1s, optimizer_1s, scheduler_1s, train_mode=True, class_weights=class_weights)\n",
    "    dv_loss, dv_f1, y_true, y_pred = run_epoch_weighted(model_1s, dev_loader_1s, tok_1s, optimizer_1s, scheduler_1s, train_mode=False, class_weights=None)\n",
    "    print(f\"[oneshot_en_weighted] Epoch {epoch}/{EPOCHS_1S} | Train loss {tr_loss:.4f} F1 {tr_f1:.4f} | Dev loss {dv_loss:.4f} F1 {dv_f1:.4f}\")\n",
    "    if dv_f1 > best_f1:\n",
    "        best_f1 = dv_f1\n",
    "        torch.save({\"model_state\": model_1s.state_dict(), \"f1\": best_f1}, best_file_1s)\n",
    "\n",
    "print(f\"[oneshot_en_weighted] Best dev macro-F1: {best_f1:.4f}\")\n",
    "\n",
    "model_1s_eval = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "tok_1s.add_special_tokens({\"additional_special_tokens\": [\"<mwe>\", \"</mwe>\"]})\n",
    "model_1s_eval.resize_token_embeddings(len(tok_1s))\n",
    "state = torch.load(best_file_1s, map_location=str(device))\n",
    "model_1s_eval.load_state_dict(state[\"model_state\"])\n",
    "model_1s_eval.to(device)\n",
    "model_1s_eval.eval()\n",
    "\n",
    "eval_en_df = prepare_eval_frame(EVAL, language=\"EN\")\n",
    "eval_loader_1s = make_infer_loader(eval_en_df, tok_1s, MAX_LEN, BATCH_SIZE)\n",
    "eval_preds_1s = predict(model_1s_eval, eval_loader_1s, tok_1s)\n",
    "\n",
    "sub_1s = pd.DataFrame({\n",
    "    \"ID\": eval_en_df[\"ID\"].astype(str),\n",
    "    \"Language\": eval_en_df[\"Language\"],\n",
    "    \"Setting\": [\"zero_shot\"] * len(eval_en_df),\n",
    "    \"Label\": eval_preds_1s\n",
    "})\n",
    "sub_path_1s = OUT_DIR / \"eval_submission_en_oneshot_weighted.csv\"\n",
    "sub_1s.to_csv(sub_path_1s, index=False)\n",
    "print(f\"Wrote {sub_path_1s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634d3483",
   "metadata": {},
   "source": [
    "## Zero-shot (EN): load, train, eval, and write eval predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c555e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 08 - Zero-shot (EN): load, train, eval, and write eval predictions\n",
    "\n",
    "train_0s_df, dev_0s_df = prepare_supervised_frame(TRAIN_ZERO_SHOT, DEV, DEV_GOLD, language=\"EN\")\n",
    "\n",
    "best_zeroshot_path, zeroshot_tokenizer = train_and_eval(\n",
    "    train_df=train_0s_df,\n",
    "    dev_df=dev_0s_df,\n",
    "    run_name=\"zeroshot_en_xlmr\"\n",
    ")\n",
    "\n",
    "model_zeroshot = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "special_tokens = {\"additional_special_tokens\": [\"<mwe>\", \"</mwe>\"]}\n",
    "zeroshot_tokenizer.add_special_tokens(special_tokens)\n",
    "model_zeroshot.resize_token_embeddings(len(zeroshot_tokenizer))\n",
    "state = torch.load(OUT_DIR / \"ckpt_zeroshot_en_xlmr\" / \"best.pt\", map_location=str(device))\n",
    "model_zeroshot.load_state_dict(state[\"model_state\"])\n",
    "model_zeroshot.to(device)\n",
    "model_zeroshot.eval()\n",
    "\n",
    "eval_en_df = prepare_eval_frame(EVAL, language=\"EN\")\n",
    "eval_loader_0s = make_infer_loader(eval_en_df, zeroshot_tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "eval_preds_0s = predict(model_zeroshot, eval_loader_0s, zeroshot_tokenizer)\n",
    "\n",
    "sub_0s = pd.DataFrame({\n",
    "    \"ID\": eval_en_df[\"ID\"].astype(str),\n",
    "    \"Language\": eval_en_df[\"Language\"],\n",
    "    \"Setting\": [\"zero_shot\"] * len(eval_en_df),\n",
    "    \"Label\": eval_preds_0s\n",
    "})\n",
    "sub_path_0s = OUT_DIR / \"eval_submission_en_zeroshot.csv\"\n",
    "sub_0s.to_csv(sub_path_0s, index=False)\n",
    "print(f\"Wrote {sub_path_0s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb662f38",
   "metadata": {},
   "source": [
    "## Dev-set diagnostics: confusion matrix & report for best runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fde64fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[One-shot EN XLM-R (weighted)] Dev macro-F1: 0.6680\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6885    0.4615    0.5526       182\n",
      "           1     0.7151    0.8662    0.7834       284\n",
      "\n",
      "    accuracy                         0.7082       466\n",
      "   macro avg     0.7018    0.6639    0.6680       466\n",
      "weighted avg     0.7047    0.7082    0.6933       466\n",
      "\n",
      "[[ 84  98]\n",
      " [ 38 246]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Zero-shot EN XLM-R] Dev macro-F1: 0.7451\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6667    0.7363    0.6997       182\n",
      "           1     0.8189    0.7641    0.7905       284\n",
      "\n",
      "    accuracy                         0.7532       466\n",
      "   macro avg     0.7428    0.7502    0.7451       466\n",
      "weighted avg     0.7594    0.7532    0.7551       466\n",
      "\n",
      "[[134  48]\n",
      " [ 67 217]]\n"
     ]
    }
   ],
   "source": [
    "# 09 - Dev-set diagnostics: confusion matrix & report for best runs\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def eval_on_dev(best_ckpt_path: Path, tokenizer, dev_df: pd.DataFrame, tag: str):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "    tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<mwe>\", \"</mwe>\"]})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    state = torch.load(best_ckpt_path, map_location=str(device))\n",
    "    model.load_state_dict(state[\"model_state\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    dev_loader = DataLoader(\n",
    "        IdiomDataset(dev_df, tokenizer, MAX_LEN, is_infer=False),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        pin_memory=(device.type == \"cuda\"),\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer, MAX_LEN, is_infer=False)\n",
    "    )\n",
    "\n",
    "    _, f1, y_true, y_pred = run_epoch(model, dev_loader, tokenizer, optimizer=None, scheduler=None, train_mode=False)\n",
    "    print(f\"[{tag}] Dev macro-F1: {f1:.4f}\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "eval_on_dev(OUT_DIR / \"ckpt_oneshot_en_xlmr_weighted\" / \"best.pt\", tok_1s, dev_1s_df, \"One-shot EN XLM-R (weighted)\")\n",
    "eval_on_dev(OUT_DIR / \"ckpt_zeroshot_en_xlmr\" / \"best.pt\", zeroshot_tokenizer, dev_0s_df, \"Zero-shot EN XLM-R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8d665c",
   "metadata": {},
   "source": [
    "## Save final best checkpoints to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651e81b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final checkpoints.\n"
     ]
    }
   ],
   "source": [
    "# 10 - Save final best checkpoints to disk\n",
    "\n",
    "final_oneshot_dir = OUT_DIR / \"final_oneshot_en_xlmr\"\n",
    "final_zeroshot_dir = OUT_DIR / \"final_zeroshot_en_xlmr\"\n",
    "final_oneshot_dir.mkdir(parents=True, exist_ok=True)\n",
    "final_zeroshot_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "torch.save(torch.load(OUT_DIR / \"ckpt_oneshot_en_xlmr_weighted\" / \"best.pt\", map_location=str(device)), final_oneshot_dir / \"model.pt\")\n",
    "torch.save(torch.load(OUT_DIR / \"ckpt_zeroshot_en_xlmr\" / \"best.pt\", map_location=str(device)), final_zeroshot_dir / \"model.pt\")\n",
    "\n",
    "print(\"Saved final checkpoints.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
