{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d15f373",
   "metadata": {},
   "source": [
    "## Setup: CPU-only, imports, seeds, paths, model choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eb853c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shahriar/.venvs/nlp/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/shahriar/.venvs/nlp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 01 - Setup: CPU-only, imports, seeds, paths, model choice\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.set_num_threads(max(1, os.cpu_count() // 2))\n",
    "device = torch.device(\"cpu\")\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "\n",
    "DATA_DIR = Path(\"SemEval_2022_Task2-idiomaticity/SubTaskA\")\n",
    "TRAIN_ONE_SHOT = DATA_DIR / \"Data\" / \"train_one_shot.csv\"\n",
    "TRAIN_ZERO_SHOT = DATA_DIR / \"Data\" / \"train_zero_shot.csv\"\n",
    "DEV = DATA_DIR / \"Data\" / \"dev.csv\"\n",
    "DEV_GOLD = DATA_DIR / \"Data\" / \"dev_gold.csv\"\n",
    "EVAL = DATA_DIR / \"Data\" / \"eval.csv\"\n",
    "EVAL_SUB_FMT = DATA_DIR / \"Data\" / \"eval_submission_format.csv\"\n",
    "\n",
    "OUT_DIR = Path(\"outputs_en_llm\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Choose one instruct model that fits your RAM; both run on CPU:\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "MAX_NEW_TOKENS = 2\n",
    "TEMPERATURE = 0.0\n",
    "TOP_P = 1.0\n",
    "DO_SAMPLE = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e166403",
   "metadata": {},
   "source": [
    "## Data loading & utilities (EN only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74a5fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 - Data loading & utilities (EN only)\n",
    "\n",
    "def load_any_csv(path: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(path, sep=None, engine=\"python\", dtype=str)\n",
    "\n",
    "def ensure_label_int(df: pd.DataFrame, col=\"Label\") -> pd.DataFrame:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(int)\n",
    "    return df\n",
    "\n",
    "def load_train_dev(language=\"EN\", oneshot=True) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    if oneshot:\n",
    "        train_df = load_any_csv(TRAIN_ONE_SHOT)\n",
    "    else:\n",
    "        train_df = load_any_csv(TRAIN_ZERO_SHOT)\n",
    "    dev_df = load_any_csv(DEV)\n",
    "    gold_df = load_any_csv(DEV_GOLD)\n",
    "\n",
    "    train_df.columns = [c.strip() for c in train_df.columns]\n",
    "    dev_df.columns = [c.strip() for c in dev_df.columns]\n",
    "    gold_df.columns = [c.strip() for c in gold_df.columns]\n",
    "\n",
    "    train_df = train_df[train_df[\"Language\"] == language].copy()\n",
    "    dev_df = dev_df[dev_df[\"Language\"] == language].copy()\n",
    "\n",
    "    gold = gold_df[gold_df[\"Language\"] == language][[\"ID\",\"Label\"]].copy()\n",
    "    gold[\"ID\"] = gold[\"ID\"].astype(str)\n",
    "    dev_df[\"ID\"] = dev_df[\"ID\"].astype(str)\n",
    "    dev_lab = dev_df.merge(gold, on=\"ID\", how=\"left\")\n",
    "    dev_lab = ensure_label_int(dev_lab, \"Label\")\n",
    "    train_df = ensure_label_int(train_df, \"Label\")\n",
    "    return train_df, dev_lab\n",
    "\n",
    "def load_eval(language=\"EN\") -> pd.DataFrame:\n",
    "    df = load_any_csv(EVAL)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df[df[\"Language\"] == language].copy()\n",
    "\n",
    "def mark_first_case_insensitive(text: str, needle: str, ltag=\"<mwe>\", rtag=\"</mwe>\") -> str:\n",
    "    if not isinstance(text, str) or not isinstance(needle, str):\n",
    "        return text\n",
    "    lt = text.lower()\n",
    "    ln = needle.lower()\n",
    "    i = lt.find(ln)\n",
    "    if i == -1:\n",
    "        return text\n",
    "    return text[:i] + ltag + text[i:i+len(needle)] + rtag + text[i+len(needle):]\n",
    "\n",
    "def pack_context(prev: str, target: str, nxt: str, mwe: str) -> str:\n",
    "    prev = \"\" if pd.isna(prev) else prev\n",
    "    nxt = \"\" if pd.isna(nxt) else nxt\n",
    "    tgt = mark_first_case_insensitive(target, mwe)\n",
    "    return f\"Previous: {prev}\\nTarget: {tgt}\\nNext: {nxt}\"\n",
    "\n",
    "def label_to_str(y: int) -> str:\n",
    "    return \"1\" if int(y) == 1 else \"0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494e153f",
   "metadata": {},
   "source": [
    "## Build one-shot exemplars per MWE from train_one_shot (EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eff3721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03 - Build one-shot exemplars per MWE from train_one_shot (EN)\n",
    "\n",
    "def build_oneshot_index(train_one_shot_en: pd.DataFrame) -> Dict[str, Dict[int, Dict[str, str]]]:\n",
    "    idx = {}\n",
    "    for _, r in train_one_shot_en.iterrows():\n",
    "        mwe = r[\"MWE\"]\n",
    "        y = int(r[\"Label\"])\n",
    "        ctx = pack_context(r.get(\"Previous\",\"\"), r.get(\"Target\",\"\"), r.get(\"Next\",\"\"), mwe)\n",
    "        idx.setdefault(mwe, {})\n",
    "        # keep the first example per label if multiple exist\n",
    "        if y not in idx[mwe]:\n",
    "            idx[mwe][y] = {\"context\": ctx, \"label\": y}\n",
    "    return idx\n",
    "\n",
    "def pick_global_oneshot_fallback(train_one_shot_en: pd.DataFrame) -> Dict[int, Dict[str, str]]:\n",
    "    pool = {0: None, 1: None}\n",
    "    for _, r in train_one_shot_en.iterrows():\n",
    "        y = int(r[\"Label\"])\n",
    "        if pool[y] is None:\n",
    "            mwe = r[\"MWE\"]\n",
    "            ctx = pack_context(r.get(\"Previous\",\"\"), r.get(\"Target\",\"\"), r.get(\"Next\",\"\"), mwe)\n",
    "            pool[y] = {\"context\": ctx, \"label\": y, \"mwe\": mwe}\n",
    "        if pool[0] is not None and pool[1] is not None:\n",
    "            break\n",
    "    return pool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bbb0c3",
   "metadata": {},
   "source": [
    "## Load LLM (CPU-only) and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ccb201b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'user', 'id': '68d86c57f1bd7f5086be81bb', 'name': 'mhossai6', 'fullname': 'Mohammad Shahriar Hossain', 'email': 'mhossai6@ualberta.ca', 'emailVerified': True, 'canPay': False, 'periodEnd': None, 'isPro': False, 'avatarUrl': '/avatars/fad622d4d934fb9bf3204eaa20999042.svg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'CMPUT497-A1', 'role': 'read', 'createdAt': '2025-11-12T07:19:03.479Z'}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.31s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 04 - Load LLM (CPU-only) and tokenizer\n",
    "\n",
    "import getpass\n",
    "from huggingface_hub import login, whoami\n",
    "\n",
    "hf_token = getpass.getpass(\"Paste your Hugging Face token (read scope is enough): \")\n",
    "login(token=hf_token)          # authenticates this Python process\n",
    "print(whoami())                # sanity check\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map={\"\": \"cpu\"},\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95082776",
   "metadata": {},
   "source": [
    "## Prompt builders (zero-shot and one-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a49bbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05 - Prompt builders (zero-shot and one-shot)\n",
    "\n",
    "INSTR_ZS = (\n",
    "    \"You are a precise classifier for idiomaticity of multi-word expressions (MWEs).\\n\"\n",
    "    \"Task: Given a context (Previous/Target/Next) and the target MWE, decide if the usage in the Target sentence is idiomatic (figurative) or literal.\\n\"\n",
    "    \"Output strictly one digit: 1 for idiomatic, 0 for literal.\\n\"\n",
    "    \"Do not output any other text.\"\n",
    ")\n",
    "\n",
    "def make_zero_shot_prompt(mwe: str, ctx_block: str) -> str:\n",
    "    return (\n",
    "        f\"{INSTR_ZS}\\n\\n\"\n",
    "        f\"MWE: {mwe}\\n\"\n",
    "        f\"{ctx_block}\\n\"\n",
    "        f\"Answer (0 or 1):\"\n",
    "    )\n",
    "\n",
    "INSTR_1S = (\n",
    "    \"You are a precise classifier for idiomaticity of multi-word expressions (MWEs).\\n\"\n",
    "    \"See two labeled examples for the SAME MWE, then classify the new instance.\\n\"\n",
    "    \"Output strictly one digit: 1 for idiomatic, 0 for literal.\"\n",
    ")\n",
    "\n",
    "def make_one_shot_prompt(mwe: str, pos_example: str, neg_example: str, ctx_block: str) -> str:\n",
    "    return (\n",
    "        f\"{INSTR_1S}\\n\\n\"\n",
    "        f\"Example A (Label=1):\\nMWE: {mwe}\\n{pos_example}\\n\\n\"\n",
    "        f\"Example B (Label=0):\\nMWE: {mwe}\\n{neg_example}\\n\\n\"\n",
    "        f\"Now classify the new instance.\\n\"\n",
    "        f\"MWE: {mwe}\\n{ctx_block}\\n\"\n",
    "        f\"Answer (0 or 1):\"\n",
    "    )\n",
    "\n",
    "def apply_chat_template_if_available(text: str) -> Dict[str, torch.Tensor]:\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful, concise assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "        prompt_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "        return {\"input_ids\": prompt_ids.to(device)}\n",
    "    ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    return {\"input_ids\": ids}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a42fbc",
   "metadata": {},
   "source": [
    "## Generation + parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffe05490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06 - Generation + parsing\n",
    "\n",
    "def generate_label_from_prompt(prompt: str) -> int:\n",
    "    enc = apply_chat_template_if_available(prompt)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=DO_SAMPLE,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    gen = out[0][enc[\"input_ids\"].shape[-1]:]\n",
    "    text = tokenizer.decode(gen, skip_special_tokens=True)\n",
    "    m = re.search(r\"\\b([01])\\b\", text)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    if \"idiomatic\" in text.lower():\n",
    "        return 1\n",
    "    if \"literal\" in text.lower():\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8a16b5",
   "metadata": {},
   "source": [
    "## Predictors for zero-shot and one-shot (EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b786013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 07 - Predictors for zero-shot and one-shot (EN)\n",
    "\n",
    "def predict_zero_shot(df: pd.DataFrame) -> List[int]:\n",
    "    preds = []\n",
    "    for _, r in df.iterrows():\n",
    "        mwe = r[\"MWE\"]\n",
    "        ctx = pack_context(r.get(\"Previous\",\"\"), r.get(\"Target\",\"\"), r.get(\"Next\",\"\"), mwe)\n",
    "        prompt = make_zero_shot_prompt(mwe, ctx)\n",
    "        yhat = generate_label_from_prompt(prompt)\n",
    "        preds.append(yhat)\n",
    "    return preds\n",
    "\n",
    "def predict_one_shot(df: pd.DataFrame, oneshot_index: Dict[str, Dict[int, Dict[str, str]]], global_pool: Dict[int, Dict[str, str]]) -> List[int]:\n",
    "    preds = []\n",
    "    for _, r in df.iterrows():\n",
    "        mwe = r[\"MWE\"]\n",
    "        ctx = pack_context(r.get(\"Previous\",\"\"), r.get(\"Target\",\"\"), r.get(\"Next\",\"\"), mwe)\n",
    "\n",
    "        pos_ctx = None\n",
    "        neg_ctx = None\n",
    "        if mwe in oneshot_index:\n",
    "            pos_ctx = oneshot_index[mwe].get(1, {}).get(\"context\")\n",
    "            neg_ctx = oneshot_index[mwe].get(0, {}).get(\"context\")\n",
    "\n",
    "        if pos_ctx is None:\n",
    "            pos_ctx = global_pool[1][\"context\"]\n",
    "        if neg_ctx is None:\n",
    "            neg_ctx = global_pool[0][\"context\"]\n",
    "\n",
    "        prompt = make_one_shot_prompt(mwe, pos_ctx, neg_ctx, ctx)\n",
    "        yhat = generate_label_from_prompt(prompt)\n",
    "        preds.append(yhat)\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b40f65f",
   "metadata": {},
   "source": [
    "## Zero-shot (EN): dev eval + eval submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c050874a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zero-shot EN (dev):   0%|          | 0/466 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Zero-shot EN (dev):   2%|▏         | 11/466 [01:16<52:43,  6.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Zero-shot EN (dev):   3%|▎         | 12/466 [03:25<2:38:21, 20.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Zero-shot EN (dev):   3%|▎         | 12/466 [04:28<2:49:30, 22.40s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m train_0s_en, dev_0s_en \u001b[38;5;241m=\u001b[39m load_train_dev(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEN\u001b[39m\u001b[38;5;124m\"\u001b[39m, oneshot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     60\u001b[0m cache_dev_0s \u001b[38;5;241m=\u001b[39m OUT_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_llm_zeroshot_dev_en.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 61\u001b[0m yhat_dev_0s \u001b[38;5;241m=\u001b[39m \u001b[43mprogressive_predict_zero_shot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev_0s_en\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dev_0s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mZero-shot EN (dev)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m ytrue_dev_0s \u001b[38;5;241m=\u001b[39m dev_0s_en[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     63\u001b[0m f1_0s \u001b[38;5;241m=\u001b[39m f1_score(ytrue_dev_0s, yhat_dev_0s, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 39\u001b[0m, in \u001b[0;36mprogressive_predict_zero_shot\u001b[0;34m(df, cache_path, desc)\u001b[0m\n\u001b[1;32m     37\u001b[0m ctx \u001b[38;5;241m=\u001b[39m pack_context(r\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrevious\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m), r\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m), r\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNext\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m), mwe)\n\u001b[1;32m     38\u001b[0m prompt \u001b[38;5;241m=\u001b[39m make_zero_shot_prompt(mwe, ctx)\n\u001b[0;32m---> 39\u001b[0m yhat \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_label_from_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m preds_map[_id] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(yhat)\n\u001b[1;32m     41\u001b[0m to_flush\u001b[38;5;241m.\u001b[39mappend((_id, \u001b[38;5;28mint\u001b[39m(yhat)))\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36mgenerate_label_from_prompt\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      4\u001b[0m enc \u001b[38;5;241m=\u001b[39m apply_chat_template_if_available(prompt)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 6\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43menc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_NEW_TOKENS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDO_SAMPLE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTEMPERATURE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOP_P\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m gen \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;241m0\u001b[39m][enc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:]\n\u001b[1;32m     15\u001b[0m text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(gen, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.venvs/nlp/lib/python3.9/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/nlp/lib/python3.9/site-packages/transformers/generation/utils.py:2564\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2561\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[0;32m-> 2564\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2565\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2566\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2570\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2571\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2574\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[1;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2576\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2579\u001b[0m ):\n",
      "File \u001b[0;32m~/.venvs/nlp/lib/python3.9/site-packages/transformers/generation/utils.py:2784\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2781\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 2784\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2785\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2786\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.venvs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.venvs/nlp/lib/python3.9/site-packages/transformers/utils/generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    920\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/.venvs/nlp/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:473\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n\u001b[1;32m    472\u001b[0m slice_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m-\u001b[39mlogits_to_keep, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logits_to_keep, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m logits_to_keep\n\u001b[0;32m--> 473\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.venvs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.venvs/nlp/lib/python3.9/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 08 - Zero-shot (EN): dev eval + eval submission (with progress bar + timing + RESUME, immediate cache)\n",
    "\n",
    "from time import perf_counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def _load_cache(cache_path: Path) -> dict:\n",
    "    if cache_path.exists():\n",
    "        df = pd.read_csv(cache_path, dtype={\"ID\": str, \"Label\": int})\n",
    "        return dict(zip(df[\"ID\"].astype(str), df[\"Label\"].astype(int)))\n",
    "    return {}\n",
    "\n",
    "def _append_one(cache_path: Path, rec: tuple):\n",
    "    _id, _lab = rec\n",
    "    header_needed = not cache_path.exists()\n",
    "    with open(cache_path, \"a\") as f:\n",
    "        if header_needed:\n",
    "            f.write(\"ID,Label\\n\")\n",
    "        f.write(f\"{_id},{int(_lab)}\\n\")\n",
    "\n",
    "def progressive_predict_zero_shot(df: pd.DataFrame, cache_path: Path, desc: str = \"Zero-shot EN\") -> list:\n",
    "    df = df.copy()\n",
    "    df[\"ID\"] = df[\"ID\"].astype(str)\n",
    "\n",
    "    preds_map = _load_cache(cache_path)\n",
    "    done = set(preds_map.keys())\n",
    "\n",
    "    print(f\"{desc} | Resuming with {len(done)} cached / {len(df)} total\")\n",
    "    t0 = perf_counter()\n",
    "    processed = 0\n",
    "\n",
    "    for _, r in tqdm(df.iterrows(), total=len(df), desc=desc, leave=True):\n",
    "        _id = str(r[\"ID\"])\n",
    "        if _id in done:\n",
    "            continue\n",
    "        mwe = r[\"MWE\"]\n",
    "        ctx = pack_context(r.get(\"Previous\",\"\"), r.get(\"Target\",\"\"), r.get(\"Next\",\"\"), mwe)\n",
    "        prompt = make_zero_shot_prompt(mwe, ctx)\n",
    "        yhat = generate_label_from_prompt(prompt)\n",
    "        preds_map[_id] = int(yhat)\n",
    "        _append_one(cache_path, (_id, int(yhat)))\n",
    "        processed += 1\n",
    "\n",
    "    elapsed = perf_counter() - t0\n",
    "    print(f\"{desc} | New computed: {processed} | Cached at start: {len(done)} | Total: {len(df)} | \"\n",
    "          f\"Elapsed: {elapsed:.1f}s | {elapsed/max(1,processed):.3f}s/example (new only)\")\n",
    "\n",
    "    yhat = [preds_map[str(i)] for i in df[\"ID\"]]\n",
    "    return yhat\n",
    "\n",
    "# ---- DEV (zero-shot EN) ----\n",
    "train_0s_en, dev_0s_en = load_train_dev(language=\"EN\", oneshot=False)\n",
    "\n",
    "cache_dev_0s = OUT_DIR / \"cache_llm_zeroshot_dev_en.csv\"\n",
    "yhat_dev_0s = progressive_predict_zero_shot(dev_0s_en, cache_dev_0s, desc=\"Zero-shot EN (dev)\")\n",
    "ytrue_dev_0s = dev_0s_en[\"Label\"].tolist()\n",
    "f1_0s = f1_score(ytrue_dev_0s, yhat_dev_0s, average=\"macro\")\n",
    "print(f\"[LLM Zero-shot EN] Dev macro-F1: {f1_0s:.4f}\")\n",
    "print(classification_report(ytrue_dev_0s, yhat_dev_0s, digits=4))\n",
    "print(confusion_matrix(ytrue_dev_0s, yhat_dev_0s))\n",
    "\n",
    "# ---- EVAL (zero-shot EN) ----\n",
    "eval_en = load_eval(language=\"EN\")\n",
    "cache_eval_0s = OUT_DIR / \"cache_llm_zeroshot_eval_en.csv\"\n",
    "yhat_eval_0s = progressive_predict_zero_shot(eval_en, cache_eval_0s, desc=\"Zero-shot EN (eval)\")\n",
    "\n",
    "sub_0s = pd.DataFrame({\n",
    "    \"ID\": eval_en[\"ID\"].astype(str),\n",
    "    \"Language\": eval_en[\"Language\"],\n",
    "    \"Setting\": [\"zero_shot\"] * len(eval_en),\n",
    "    \"Label\": yhat_eval_0s\n",
    "})\n",
    "sub_0s_path = OUT_DIR / \"eval_submission_en_llm_zeroshot.csv\"\n",
    "sub_0s.to_csv(sub_0s_path, index=False)\n",
    "print(f\"Wrote {sub_0s_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5acc76c",
   "metadata": {},
   "source": [
    "## One-shot (EN): build exemplars, dev eval + eval submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816ad88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 09 - One-shot (EN): build exemplars, dev eval + eval submission (progress + RESUME, immediate cache)\n",
    "\n",
    "from time import perf_counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def progressive_predict_one_shot(df: pd.DataFrame,\n",
    "                                 oneshot_index: dict,\n",
    "                                 global_pool: dict,\n",
    "                                 cache_path: Path,\n",
    "                                 desc: str = \"One-shot EN\") -> list:\n",
    "    df = df.copy()\n",
    "    df[\"ID\"] = df[\"ID\"].astype(str)\n",
    "\n",
    "    preds_map = _load_cache(cache_path)\n",
    "    done = set(preds_map.keys())\n",
    "\n",
    "    print(f\"{desc} | Resuming with {len(done)} cached / {len(df)} total\")\n",
    "    t0 = perf_counter()\n",
    "    processed = 0\n",
    "\n",
    "    for _, r in tqdm(df.iterrows(), total=len(df), desc=desc, leave=True):\n",
    "        _id = str(r[\"ID\"])\n",
    "        if _id in done:\n",
    "            continue\n",
    "\n",
    "        mwe = r[\"MWE\"]\n",
    "        ctx = pack_context(r.get(\"Previous\",\"\"), r.get(\"Target\",\"\"), r.get(\"Next\",\"\"), mwe)\n",
    "\n",
    "        pos_ctx = oneshot_index.get(mwe, {}).get(1, {}).get(\"context\")\n",
    "        neg_ctx = oneshot_index.get(mwe, {}).get(0, {}).get(\"context\")\n",
    "        if pos_ctx is None:\n",
    "            pos_ctx = global_pool[1][\"context\"]\n",
    "        if neg_ctx is None:\n",
    "            neg_ctx = global_pool[0][\"context\"]\n",
    "\n",
    "        prompt = make_one_shot_prompt(mwe, pos_ctx, neg_ctx, ctx)\n",
    "        yhat = generate_label_from_prompt(prompt)\n",
    "        preds_map[_id] = int(yhat)\n",
    "        _append_one(cache_path, (_id, int(yhat)))\n",
    "        processed += 1\n",
    "\n",
    "    elapsed = perf_counter() - t0\n",
    "    print(f\"{desc} | New computed: {processed} | Cached at start: {len(done)} | Total: {len(df)} | \"\n",
    "          f\"Elapsed: {elapsed:.1f}s | {elapsed/max(1,processed):.3f}s/example (new only)\")\n",
    "\n",
    "    yhat = [preds_map[str(i)] for i in df[\"ID\"]]\n",
    "    return yhat\n",
    "\n",
    "train_1s_en, dev_1s_en = load_train_dev(language=\"EN\", oneshot=True)\n",
    "oneshot_index = build_oneshot_index(train_1s_en)\n",
    "global_pool = pick_global_oneshot_fallback(train_1s_en)\n",
    "\n",
    "cache_dev_1s = OUT_DIR / \"cache_llm_oneshot_dev_en.csv\"\n",
    "yhat_dev_1s = progressive_predict_one_shot(dev_1s_en, oneshot_index, global_pool, cache_dev_1s, desc=\"One-shot EN (dev)\")\n",
    "ytrue_dev_1s = dev_1s_en[\"Label\"].tolist()\n",
    "f1_1s = f1_score(ytrue_dev_1s, yhat_dev_1s, average=\"macro\")\n",
    "print(f\"[LLM One-shot EN] Dev macro-F1: {f1_1s:.4f}\")\n",
    "print(classification_report(ytrue_dev_1s, yhat_dev_1s, digits=4))\n",
    "print(confusion_matrix(ytrue_dev_1s, yhat_dev_1s))\n",
    "\n",
    "eval_en = load_eval(language=\"EN\")\n",
    "cache_eval_1s = OUT_DIR / \"cache_llm_oneshot_eval_en.csv\"\n",
    "yhat_eval_1s = progressive_predict_one_shot(eval_en, oneshot_index, global_pool, cache_eval_1s, desc=\"One-shot EN (eval)\")\n",
    "\n",
    "sub_1s = pd.DataFrame({\n",
    "    \"ID\": eval_en[\"ID\"].astype(str),\n",
    "    \"Language\": eval_en[\"Language\"],\n",
    "    \"Setting\": [\"zero_shot\"] * len(eval_en),\n",
    "    \"Label\": yhat_eval_1s\n",
    "})\n",
    "sub_1s_path = OUT_DIR / \"eval_submission_en_llm_oneshot.csv\"\n",
    "sub_1s.to_csv(sub_1s_path, index=False)\n",
    "print(f\"Wrote {sub_1s_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cbb2b7",
   "metadata": {},
   "source": [
    "## Save run metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37b773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 - Save run metadata\n",
    "\n",
    "with open(OUT_DIR / \"run_en_llm.txt\", \"w\") as f:\n",
    "    f.write(f\"MODEL_NAME={MODEL_NAME}\\n\")\n",
    "    f.write(f\"ZERO_SHOT_DEV_F1={f1_0s:.4f}\\n\")\n",
    "    f.write(f\"ONE_SHOT_DEV_F1={f1_1s:.4f}\\n\")\n",
    "    f.write(\"CPU_ONLY=True\\n\")\n",
    "print(\"Saved run metadata.\")\n",
    "\n",
    "print(\"Resume cache files (delete to start fresh):\")\n",
    "for p in [\n",
    "    OUT_DIR / \"cache_llm_zeroshot_dev_en.csv\",\n",
    "    OUT_DIR / \"cache_llm_zeroshot_eval_en.csv\",\n",
    "    OUT_DIR / \"cache_llm_oneshot_dev_en.csv\",\n",
    "    OUT_DIR / \"cache_llm_oneshot_eval_en.csv\",\n",
    "]:\n",
    "    print(f\"- {p}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
