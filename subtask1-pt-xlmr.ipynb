{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d074b74c",
      "metadata": {
        "id": "d074b74c"
      },
      "source": [
        "## Setup: single switch for CPU/GPU, deterministic settings, seeds, paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "02e5e7f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02e5e7f6",
        "outputId": "9f053433-d553-44b2-9bca-3ffa549c7f5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# 01 - Setup: device, imports, seeds, paths, hyperparams\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "# ---- Choose device in ONE place: \"cpu\" or \"cuda\"\n",
        "SELECT_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ---- Environment toggles\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "if SELECT_DEVICE == \"cpu\":\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "import torch\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "device = torch.device(SELECT_DEVICE)\n",
        "if device.type == \"cpu\":\n",
        "    torch.set_num_threads(max(1, os.cpu_count() // 2))\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "# ---- Paths\n",
        "DATA_DIR = Path(\"SemEval_2022_Task2-idiomaticity/SubTaskA\")\n",
        "TRAIN_ONE_SHOT = DATA_DIR / \"Data\" / \"train_one_shot.csv\"\n",
        "TRAIN_ZERO_SHOT = DATA_DIR / \"Data\" / \"train_zero_shot.csv\"\n",
        "DEV = DATA_DIR / \"Data\" / \"dev.csv\"\n",
        "DEV_GOLD = DATA_DIR / \"Data\" / \"dev_gold.csv\"\n",
        "EVAL = DATA_DIR / \"Data\" / \"eval.csv\"\n",
        "\n",
        "OUT_DIR = Path(\"outputs_pt_xlmr\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---- Model & training hyperparams\n",
        "MODEL_NAME = \"xlm-roberta-base\"   # can switch to xlm-roberta-large if you have headroom\n",
        "MAX_LEN = 256\n",
        "\n",
        "# zero-shot (full FT)\n",
        "ZS_EPOCHS = 2\n",
        "ZS_LR = 2e-5\n",
        "ZS_WARMUP_RATIO = 0.06\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "# one-shot (head-only FT + focal)\n",
        "OS_EPOCHS = 12\n",
        "OS_LR = 1e-4\n",
        "OS_WARMUP_RATIO = 0.1\n",
        "\n",
        "# batch sizes\n",
        "BATCH_SIZE = 8                     # eval/diagnostics\n",
        "BATCH_SIZE_MICRO = 8               # training micro-batch\n",
        "GRAD_ACCUM_STEPS = 2               # effective batch = micro * accum\n",
        "\n",
        "# pin_memory toggle for CUDA\n",
        "PIN_MEMORY = (device.type == \"cuda\")\n",
        "\n",
        "# ----- Colab VRAM helpers (no-ops on CPU)\n",
        "def cuda_mem():\n",
        "    if device.type == \"cuda\":\n",
        "        alloc = torch.cuda.memory_allocated() / (1024**3)\n",
        "        reserv = torch.cuda.memory_reserved() / (1024**3)\n",
        "        print(f\"CUDA allocated: {alloc:.2f} GiB | reserved: {reserv:.2f} GiB\")\n",
        "\n",
        "def free_vram():\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "        import gc; gc.collect()\n",
        "        print(\"VRAM cleanup done.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b292cbc",
      "metadata": {
        "id": "3b292cbc"
      },
      "source": [
        "## IO helpers and data preparation for Subtask A (PT only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bcca7eda",
      "metadata": {
        "id": "bcca7eda"
      },
      "outputs": [],
      "source": [
        "# 02 - IO helpers and data preparation for Subtask A (PT only)\n",
        "\n",
        "def load_any_csv(path: Path) -> pd.DataFrame:\n",
        "    return pd.read_csv(path, sep=None, engine=\"python\", dtype=str)\n",
        "\n",
        "def ensure_label_int(df: pd.DataFrame, col=\"Label\") -> pd.DataFrame:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].astype(int)\n",
        "    return df\n",
        "\n",
        "def mark_first_case_insensitive(text: str, needle: str, ltag: str=\"<mwe>\", rtag: str=\"</mwe>\") -> str:\n",
        "    if not isinstance(text, str) or not isinstance(needle, str):\n",
        "        return text\n",
        "    low_t = text.lower()\n",
        "    low_n = needle.lower()\n",
        "    idx = low_t.find(low_n)\n",
        "    if idx == -1:\n",
        "        return text\n",
        "    return text[:idx] + ltag + text[idx:idx+len(needle)] + rtag + text[idx+len(needle):]\n",
        "\n",
        "def build_input(prev: str, target: str, nxt: str, mwe: str, sep_token: str) -> str:\n",
        "    target_marked = mark_first_case_insensitive(target, mwe, \"<mwe>\", \"</mwe>\")\n",
        "    prev = \"\" if pd.isna(prev) else prev\n",
        "    nxt = \"\" if pd.isna(nxt) else nxt\n",
        "    return f\"{prev} {sep_token} {target_marked} {sep_token} {nxt}\".strip()\n",
        "\n",
        "def prepare_supervised_frame(train_path: Path, dev_path: Path, dev_gold_path: Path, language=\"PT\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    train_df = load_any_csv(train_path)\n",
        "    dev_df = load_any_csv(dev_path)\n",
        "    gold_df = load_any_csv(dev_gold_path)\n",
        "\n",
        "    train_df.columns = [c.strip() for c in train_df.columns]\n",
        "    dev_df.columns = [c.strip() for c in dev_df.columns]\n",
        "    gold_df.columns = [c.strip() for c in gold_df.columns]\n",
        "\n",
        "    train_df = train_df[train_df[\"Language\"] == language].copy()\n",
        "    dev_df = dev_df[dev_df[\"Language\"] == language].copy()\n",
        "\n",
        "    dev_gold = gold_df[gold_df[\"Language\"] == language][[\"ID\", \"Label\"]].copy()\n",
        "    dev_gold[\"ID\"] = dev_gold[\"ID\"].astype(str)\n",
        "    dev_df[\"ID\"] = dev_df[\"ID\"].astype(str)\n",
        "    dev_labeled = dev_df.merge(dev_gold, on=\"ID\", how=\"left\")\n",
        "    dev_labeled = ensure_label_int(dev_labeled, \"Label\")\n",
        "\n",
        "    train_df = ensure_label_int(train_df, \"Label\")\n",
        "    return train_df, dev_labeled\n",
        "\n",
        "def prepare_eval_frame(eval_path: Path, language=\"PT\") -> pd.DataFrame:\n",
        "    eval_df = load_any_csv(eval_path)\n",
        "    eval_df.columns = [c.strip() for c in eval_df.columns]\n",
        "    eval_df = eval_df[eval_df[\"Language\"] == language].copy()\n",
        "    return eval_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22d4f14d",
      "metadata": {
        "id": "22d4f14d"
      },
      "source": [
        "## Dataset & collate for XLM-R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "81c128fc",
      "metadata": {
        "id": "81c128fc"
      },
      "outputs": [],
      "source": [
        "# 03 - Dataset & collate for XLM-R\n",
        "\n",
        "class IdiomDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, tokenizer, max_len: int, is_infer: bool=False):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.is_infer = is_infer\n",
        "        self.sep = tokenizer.sep_token if tokenizer.sep_token is not None else \"</s>\"\n",
        "        self.texts = []\n",
        "        self.labels = []\n",
        "\n",
        "        for _, row in self.df.iterrows():\n",
        "            prev = row.get(\"Previous\", \"\")\n",
        "            target = row.get(\"Target\", \"\")\n",
        "            nxt = row.get(\"Next\", \"\")\n",
        "            mwe = row.get(\"MWE\", \"\")\n",
        "            text = build_input(prev, target, nxt, mwe, self.sep)\n",
        "            self.texts.append(text)\n",
        "            if not is_infer:\n",
        "                self.labels.append(int(row[\"Label\"]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\"text\": self.texts[idx]}\n",
        "        if not self.is_infer:\n",
        "            item[\"label\"] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "def collate_fn(batch, tokenizer, max_len: int, is_infer: bool=False):\n",
        "    texts = [b[\"text\"] for b in batch]\n",
        "    enc = tokenizer(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    if not is_infer:\n",
        "        labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n",
        "        return enc, labels\n",
        "    return enc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24e24940",
      "metadata": {
        "id": "24e24940"
      },
      "source": [
        "## Model, training, evaluation (fp32 only; gradient accumulation on GPU to keep identical updates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c68a046e",
      "metadata": {
        "id": "c68a046e"
      },
      "outputs": [],
      "source": [
        "# 04 - Model, training, evaluation (fp32; accumulation; + probs for thresholding)\n",
        "\n",
        "def ensure_mwe_tokens(tokenizer, model=None):\n",
        "    add_list = []\n",
        "    for t in [\"<mwe>\", \"</mwe>\"]:\n",
        "        if t not in tokenizer.get_vocab():\n",
        "            add_list.append(t)\n",
        "    if add_list:\n",
        "        tokenizer.add_special_tokens({\"additional_special_tokens\": add_list})\n",
        "        if model is not None:\n",
        "            model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "def build_model_and_tokenizer(model_name: str, move_to_device: bool = False):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    # DO NOT add tokens here; call ensure_mwe_tokens later once per run\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=2,\n",
        "        low_cpu_mem_usage=True,\n",
        "        torch_dtype=torch.float32\n",
        "    )\n",
        "    if move_to_device:\n",
        "        model.to(device)\n",
        "    return model, tokenizer\n",
        "\n",
        "def _to_device_batch(batch):\n",
        "    enc, labels = batch\n",
        "    enc = {k: v.to(device, non_blocking=(device.type==\"cuda\")) for k, v in enc.items()}\n",
        "    labels = labels.to(device, non_blocking=(device.type==\"cuda\"))\n",
        "    return enc, labels\n",
        "\n",
        "def run_epoch(model, loader, tokenizer, optimizer, scheduler, train_mode: bool, accum_steps: int = 1):\n",
        "    model.train() if train_mode else model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    total_loss = 0.0\n",
        "\n",
        "    if train_mode:\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    step_count = 0\n",
        "    for batch in loader:\n",
        "        enc, labels = _to_device_batch(batch)\n",
        "        out = model(**enc, labels=labels)\n",
        "        loss, logits = out.loss, out.logits\n",
        "\n",
        "        if train_mode:\n",
        "            (loss / accum_steps).backward()\n",
        "            step_count += 1\n",
        "            if step_count % accum_steps == 0:\n",
        "                optimizer.step()\n",
        "                if scheduler is not None:\n",
        "                    scheduler.step()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "        preds = torch.argmax(logits, dim=-1).detach().cpu().tolist()\n",
        "        labs = labels.detach().cpu().tolist()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labs)\n",
        "\n",
        "    if train_mode and (step_count % accum_steps != 0):\n",
        "        optimizer.step()\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    avg_loss = total_loss / max(1, len(all_labels))\n",
        "    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "    return avg_loss, macro_f1, all_labels, all_preds\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def run_epoch_weighted(model, loader, tokenizer, optimizer, scheduler, train_mode: bool,\n",
        "                       class_weights: Optional[torch.Tensor]=None, accum_steps: int = 1):\n",
        "    model.train() if train_mode else model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    total_loss = 0.0\n",
        "\n",
        "    if train_mode:\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    step_count = 0\n",
        "    for batch in loader:\n",
        "        enc, labels = _to_device_batch(batch)\n",
        "        logits = model(**enc).logits\n",
        "        loss = F.cross_entropy(logits, labels, weight=class_weights)\n",
        "\n",
        "        if train_mode:\n",
        "            (loss / accum_steps).backward()\n",
        "            step_count += 1\n",
        "            if step_count % accum_steps == 0:\n",
        "                optimizer.step()\n",
        "                if scheduler is not None:\n",
        "                    scheduler.step()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "        preds = torch.argmax(logits, dim=-1).detach().cpu().tolist()\n",
        "        labs = labels.detach().cpu().tolist()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labs)\n",
        "\n",
        "    if train_mode and (step_count % accum_steps != 0):\n",
        "        optimizer.step()\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    avg_loss = total_loss / max(1, len(all_labels))\n",
        "    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "    return avg_loss, macro_f1, all_labels, all_preds\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict(model, loader, tokenizer):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    for batch in loader:\n",
        "        enc = batch\n",
        "        enc = {k: v.to(device, non_blocking=(device.type==\"cuda\")) for k, v in enc.items()}\n",
        "        logits = model(**enc).logits\n",
        "        preds = torch.argmax(logits, dim=-1).cpu().tolist()\n",
        "        all_preds.extend(preds)\n",
        "    return all_preds\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_probs(model, loader, tokenizer) -> np.ndarray:\n",
        "    model.eval()\n",
        "    probs = []\n",
        "    for batch in loader:\n",
        "        enc = batch\n",
        "        enc = {k: v.to(device, non_blocking=(device.type==\"cuda\")) for k, v in enc.items()}\n",
        "        logits = model(**enc).logits\n",
        "        p = torch.softmax(logits, dim=-1)[:, 1].detach().cpu().numpy()\n",
        "        probs.append(p)\n",
        "    return np.concatenate(probs, axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c8c5f38",
      "metadata": {
        "id": "3c8c5f38"
      },
      "source": [
        "## Data loaders factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "526917e8",
      "metadata": {
        "id": "526917e8"
      },
      "outputs": [],
      "source": [
        "# 05 - Data loaders factory\n",
        "\n",
        "def make_loaders(train_df: pd.DataFrame, dev_df: pd.DataFrame, tokenizer, max_len: int, batch_size_micro: int):\n",
        "    train_ds = IdiomDataset(train_df, tokenizer, max_len, is_infer=False)\n",
        "    dev_ds = IdiomDataset(dev_df, tokenizer, max_len, is_infer=False)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size_micro,\n",
        "        shuffle=True,\n",
        "        pin_memory=(device.type == \"cuda\"),\n",
        "        collate_fn=lambda b: collate_fn(b, tokenizer, max_len, is_infer=False)\n",
        "    )\n",
        "    dev_loader = DataLoader(\n",
        "        dev_ds,\n",
        "        batch_size=batch_size_micro,\n",
        "        shuffle=False,\n",
        "        pin_memory=(device.type == \"cuda\"),\n",
        "        collate_fn=lambda b: collate_fn(b, tokenizer, max_len, is_infer=False)\n",
        "    )\n",
        "    return train_loader, dev_loader\n",
        "\n",
        "def make_infer_loader(df: pd.DataFrame, tokenizer, max_len: int, batch_size_micro: int):\n",
        "    ds = IdiomDataset(df, tokenizer, max_len, is_infer=True)\n",
        "    loader = DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size_micro,\n",
        "        shuffle=False,\n",
        "        pin_memory=(device.type == \"cuda\"),\n",
        "        collate_fn=lambda b: collate_fn(b, tokenizer, max_len, is_infer=True)\n",
        "    )\n",
        "    return loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb6bb555",
      "metadata": {
        "id": "eb6bb555"
      },
      "source": [
        "## Training runner (fp32, accumulation to match CPU effective batch & update schedule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "db4838a7",
      "metadata": {
        "id": "db4838a7"
      },
      "outputs": [],
      "source": [
        "# 06 - Training runner (fp32, accumulation; supports class-weighted loss)\n",
        "\n",
        "import math\n",
        "\n",
        "def _num_update_steps_per_epoch(n_examples: int, effective_batch: int) -> int:\n",
        "    return math.ceil(n_examples / effective_batch)\n",
        "\n",
        "def train_and_eval(\n",
        "    train_df: pd.DataFrame,\n",
        "    dev_df: pd.DataFrame,\n",
        "    run_name: str,\n",
        "    model_name: str = MODEL_NAME,\n",
        "    max_len: int = MAX_LEN,\n",
        "    batch_size_micro: int = BATCH_SIZE_MICRO,\n",
        "    epochs: int = EPOCHS,\n",
        "    lr: float = LR,\n",
        "    weight_decay: float = WEIGHT_DECAY,\n",
        "    warmup_ratio: float = WARMUP_RATIO,\n",
        "    accum_steps: int = GRAD_ACCUM_STEPS,\n",
        "    class_weights: Optional[torch.Tensor] = None\n",
        "):\n",
        "    ckpt_dir = OUT_DIR / f\"ckpt_{run_name}\"\n",
        "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    model, tokenizer = build_model_and_tokenizer(model_name, move_to_device=False)\n",
        "    model.to(device)\n",
        "\n",
        "    train_loader, dev_loader = make_loaders(train_df, dev_df, tokenizer, max_len, batch_size_micro)\n",
        "\n",
        "    n_train = len(train_df)\n",
        "    updates_per_epoch = _num_update_steps_per_epoch(n_train, batch_size_micro * accum_steps)\n",
        "    total_update_steps = max(1, epochs * updates_per_epoch)\n",
        "    warmup_steps = max(1, int(warmup_ratio * total_update_steps))\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, foreach=False)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_update_steps\n",
        "    )\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_path = ckpt_dir / \"best.pt\"\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        if class_weights is None:\n",
        "            tr_loss, tr_f1, _, _ = run_epoch(\n",
        "                model, train_loader, tokenizer, optimizer, scheduler, train_mode=True, accum_steps=accum_steps\n",
        "            )\n",
        "        else:\n",
        "            tr_loss, tr_f1, _, _ = run_epoch_weighted(\n",
        "                model, train_loader, tokenizer, optimizer, scheduler, train_mode=True,\n",
        "                class_weights=class_weights, accum_steps=accum_steps\n",
        "            )\n",
        "\n",
        "        dv_loss, dv_f1, y_true, y_pred = run_epoch(\n",
        "            model, dev_loader, tokenizer, optimizer=None, scheduler=None, train_mode=False, accum_steps=1\n",
        "        )\n",
        "        print(f\"[{run_name}] Epoch {epoch}/{epochs} | Train loss {tr_loss:.4f} F1 {tr_f1:.4f} | Dev loss {dv_loss:.4f} F1 {dv_f1:.4f}\")\n",
        "\n",
        "        if dv_f1 > best_f1:\n",
        "            best_f1 = dv_f1\n",
        "            torch.save({\"model_state\": model.state_dict(), \"tokenizer\": tokenizer.get_vocab(), \"f1\": best_f1}, best_path)\n",
        "\n",
        "        if device.type == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"[{run_name}] Best dev macro-F1: {best_f1:.4f}\")\n",
        "    return best_path, tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU/VRAM cleanup helper â€” run this before (re)building models on GPU\n",
        "def free_vram():\n",
        "    import gc, torch\n",
        "    # delete any torch models/tensors we kept around in globals()\n",
        "    for k, v in list(globals().items()):\n",
        "        try:\n",
        "            import torch.nn as nn\n",
        "            if isinstance(v, nn.Module):\n",
        "                del globals()[k]\n",
        "        except Exception:\n",
        "            pass\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            del globals()[k]\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "    print(\"VRAM cleanup done.\")\n",
        "\n",
        "# Optional: print quick CUDA memory stats\n",
        "def cuda_mem():\n",
        "    import torch\n",
        "    if torch.cuda.is_available():\n",
        "        a = torch.cuda.memory_allocated() / (1024**3)\n",
        "        r = torch.cuda.memory_reserved() / (1024**3)\n",
        "        print(f\"CUDA allocated: {a:.2f} GiB | reserved: {r:.2f} GiB\")\n",
        "    else:\n",
        "        print(\"CUDA not available.\")\n"
      ],
      "metadata": {
        "id": "I6jz8znIj-gh"
      },
      "id": "I6jz8znIj-gh",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "89a48a68",
      "metadata": {
        "id": "89a48a68"
      },
      "source": [
        "## One-shot (PT) with class-weighted loss (fp32), eval submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b993579f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b993579f",
        "outputId": "904046b9-3d68-4bef-a6da-e971fcca77e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA allocated: 0.00 GiB | reserved: 0.00 GiB\n",
            "VRAM cleanup done.\n",
            "CUDA allocated: 0.00 GiB | reserved: 0.00 GiB\n",
            "[PT one-shot] Train label counts: Counter({0: 28, 1: 25}) | Dev label counts: Counter({0: 154, 1: 119})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[oneshot_pt_weighted] Epoch 1/6 | Train loss 0.7015 F1 0.3457 | Dev loss 0.6853 F1 0.3607\n",
            "[oneshot_pt_weighted] Epoch 2/6 | Train loss 0.7009 F1 0.3457 | Dev loss 0.6850 F1 0.3607\n",
            "[oneshot_pt_weighted] Epoch 3/6 | Train loss 0.6883 F1 0.3788 | Dev loss 0.6852 F1 0.3607\n",
            "[oneshot_pt_weighted] Epoch 4/6 | Train loss 0.6887 F1 0.4421 | Dev loss 0.6856 F1 0.3607\n",
            "[oneshot_pt_weighted] Epoch 5/6 | Train loss 0.6881 F1 0.4940 | Dev loss 0.6857 F1 0.3607\n",
            "[oneshot_pt_weighted] Epoch 6/6 | Train loss 0.6864 F1 0.4500 | Dev loss 0.6858 F1 0.3607\n",
            "[oneshot_pt_weighted] Best dev macro-F1: 0.3607\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[oneshot_pt_weighted] Dev positive rate (probs>=0.5): 0.000\n",
            "[oneshot_pt_weighted] Tuned threshold on dev: th=0.425, macro-F1=0.4619\n",
            "Wrote outputs_pt_xlmr/eval_submission_pt_oneshot_weighted.csv\n"
          ]
        }
      ],
      "source": [
        "# 07 - One-shot (PT) with class-weighted loss + dev threshold tuning, eval submission\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    cuda_mem(); free_vram(); cuda_mem()\n",
        "\n",
        "train_1s_df, dev_1s_df = prepare_supervised_frame(TRAIN_ONE_SHOT, DEV, DEV_GOLD, language=\"PT\")\n",
        "\n",
        "# Sanity: label balance\n",
        "cnt_train_1s = Counter(train_1s_df[\"Label\"].astype(int).tolist())\n",
        "cnt_dev_1s   = Counter(dev_1s_df[\"Label\"].astype(int).tolist())\n",
        "print(\"[PT one-shot] Train label counts:\", cnt_train_1s, \"| Dev label counts:\", cnt_dev_1s)\n",
        "\n",
        "# Class weights\n",
        "num0, num1 = cnt_train_1s.get(0, 1), cnt_train_1s.get(1, 1)\n",
        "total = num0 + num1\n",
        "w0 = total / (2.0 * num0)\n",
        "w1 = total / (2.0 * num1)\n",
        "class_weights_1s = torch.tensor([w0, w1], dtype=torch.float32, device=device)\n",
        "\n",
        "# Build fresh model/tokenizer; add <mwe> tokens exactly once\n",
        "model_1s, tok_1s = build_model_and_tokenizer(MODEL_NAME, move_to_device=False)\n",
        "ensure_mwe_tokens(tok_1s, model_1s)\n",
        "model_1s.to(device)\n",
        "\n",
        "train_loader_1s, dev_loader_1s = make_loaders(train_1s_df, dev_1s_df, tok_1s, MAX_LEN, BATCH_SIZE_MICRO)\n",
        "\n",
        "EPOCHS_1S = 6\n",
        "LR_1S = 1.5e-5\n",
        "WARMUP_RATIO_1S = 0.1\n",
        "ACCUM_1S = GRAD_ACCUM_STEPS\n",
        "\n",
        "n_train_1s = len(train_1s_df)\n",
        "updates_per_epoch_1s = math.ceil(n_train_1s / (BATCH_SIZE_MICRO * ACCUM_1S))\n",
        "total_update_steps_1s = max(1, EPOCHS_1S * updates_per_epoch_1s)\n",
        "warmup_steps_1s = max(1, int(WARMUP_RATIO_1S * total_update_steps_1s))\n",
        "\n",
        "optimizer_1s = torch.optim.AdamW(model_1s.parameters(), lr=LR_1S, weight_decay=WEIGHT_DECAY, foreach=False)\n",
        "scheduler_1s = get_linear_schedule_with_warmup(optimizer_1s, num_warmup_steps=warmup_steps_1s, num_training_steps=total_update_steps_1s)\n",
        "\n",
        "best_f1 = -1.0\n",
        "best_dir_1s = OUT_DIR / \"ckpt_oneshot_pt_xlmr_weighted\"\n",
        "best_dir_1s.mkdir(parents=True, exist_ok=True)\n",
        "best_file_1s = best_dir_1s / \"best.pt\"\n",
        "\n",
        "for epoch in range(1, EPOCHS_1S + 1):\n",
        "    tr_loss, tr_f1, _, _ = run_epoch_weighted(\n",
        "        model_1s, train_loader_1s, tok_1s, optimizer_1s, scheduler_1s,\n",
        "        train_mode=True, class_weights=class_weights_1s, accum_steps=ACCUM_1S\n",
        "    )\n",
        "    dv_loss, dv_f1, y_true, y_pred = run_epoch(\n",
        "        model_1s, dev_loader_1s, tok_1s, optimizer_1s, scheduler_1s,\n",
        "        train_mode=False, accum_steps=1\n",
        "    )\n",
        "    print(f\"[oneshot_pt_weighted] Epoch {epoch}/{EPOCHS_1S} | Train loss {tr_loss:.4f} F1 {tr_f1:.4f} | Dev loss {dv_loss:.4f} F1 {dv_f1:.4f}\")\n",
        "    if dv_f1 > best_f1:\n",
        "        best_f1 = dv_f1\n",
        "        torch.save({\"model_state\": model_1s.state_dict(), \"f1\": best_f1, \"vocab_size\": len(tok_1s)}, best_file_1s)\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"[oneshot_pt_weighted] Best dev macro-F1: {best_f1:.4f}\")\n",
        "\n",
        "# Reload best for threshold tuning / inference (ensure identical vocab)\n",
        "model_1s_eval = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "ensure_mwe_tokens(tok_1s, model_1s_eval)\n",
        "state = torch.load(best_file_1s, map_location=str(device))\n",
        "# If vocab expanded during training, match it before loading weights\n",
        "if \"vocab_size\" in state:\n",
        "    vs = state[\"vocab_size\"]\n",
        "    if model_1s_eval.get_input_embeddings().weight.shape[0] != vs:\n",
        "        model_1s_eval.resize_token_embeddings(vs)\n",
        "model_1s_eval.load_state_dict(state[\"model_state\"])\n",
        "model_1s_eval.to(device)\n",
        "model_1s_eval.eval()\n",
        "\n",
        "# ---- Dev threshold tuning with a wider grid\n",
        "dev_loader_1s_eval = make_infer_loader(dev_1s_df, tok_1s, MAX_LEN, BATCH_SIZE_MICRO)\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_probs_inferloader(model, loader) -> np.ndarray:\n",
        "    model.eval()\n",
        "    probs = []\n",
        "    for enc in loader:\n",
        "        enc = {k: v.to(device, non_blocking=(device.type==\"cuda\")) for k, v in enc.items()}\n",
        "        p = torch.softmax(model(**enc).logits, dim=-1)[:, 1].detach().cpu().numpy()\n",
        "        probs.append(p)\n",
        "    return np.concatenate(probs, axis=0)\n",
        "\n",
        "dev_probs = predict_probs_inferloader(model_1s_eval, dev_loader_1s_eval)\n",
        "dev_true = dev_1s_df[\"Label\"].astype(int).to_numpy()\n",
        "print(f\"[oneshot_pt_weighted] Dev positive rate (probs>=0.5): {(dev_probs>=0.5).mean():.3f}\")\n",
        "\n",
        "def _tune_threshold(probs, ytrue):\n",
        "    grid = np.linspace(0.05, 0.95, 37)  # wider sweep\n",
        "    best_th, best_f1 = 0.5, -1.0\n",
        "    for th in grid:\n",
        "        yhat = (probs >= th).astype(int)\n",
        "        f1 = f1_score(ytrue, yhat, average=\"macro\")\n",
        "        if f1 > best_f1:\n",
        "            best_f1, best_th = f1, th\n",
        "    return best_th, best_f1\n",
        "\n",
        "best_th_1s, best_th_f1_1s = _tune_threshold(dev_probs, dev_true)\n",
        "print(f\"[oneshot_pt_weighted] Tuned threshold on dev: th={best_th_1s:.3f}, macro-F1={best_th_f1_1s:.4f}\")\n",
        "\n",
        "# ---- Eval predictions\n",
        "eval_pt_df = prepare_eval_frame(EVAL, language=\"PT\")\n",
        "eval_loader_1s = make_infer_loader(eval_pt_df, tok_1s, MAX_LEN, BATCH_SIZE_MICRO)\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_with_threshold_inferloader(model, loader, th: float) -> list:\n",
        "    outs = []\n",
        "    for enc in loader:\n",
        "        enc = {k: v.to(device, non_blocking=(device.type==\"cuda\")) for k, v in enc.items()}\n",
        "        p1 = torch.softmax(model(**enc).logits, dim=-1)[:, 1]\n",
        "        outs.extend((p1 >= th).int().cpu().tolist())\n",
        "    return outs\n",
        "\n",
        "eval_preds_1s = predict_with_threshold_inferloader(model_1s_eval, eval_loader_1s, best_th_1s)\n",
        "\n",
        "sub_1s = pd.DataFrame({\n",
        "    \"ID\": eval_pt_df[\"ID\"].astype(str),\n",
        "    \"Language\": eval_pt_df[\"Language\"],\n",
        "    \"Setting\": [\"zero_shot\"] * len(eval_pt_df),\n",
        "    \"Label\": eval_preds_1s\n",
        "})\n",
        "sub_path_1s = OUT_DIR / \"eval_submission_pt_oneshot_weighted.csv\"\n",
        "sub_1s.to_csv(sub_path_1s, index=False)\n",
        "print(f\"Wrote {sub_path_1s}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bf592dd",
      "metadata": {
        "id": "5bf592dd"
      },
      "source": [
        "## Zero-shot (PT): load, train, eval, and write eval predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a78f5092",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "a78f5092",
        "outputId": "0e76eb3b-9fac-49de-e24b-08878e27fe66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA allocated: 5.19 GiB | reserved: 5.25 GiB\n",
            "VRAM cleanup done.\n",
            "CUDA allocated: 4.16 GiB | reserved: 4.19 GiB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[zeroshot_pt_xlmr] Epoch 1/2 | Train loss 0.6804 F1 0.5463 | Dev loss 0.7009 F1 0.3607\n",
            "[zeroshot_pt_xlmr] Epoch 2/2 | Train loss 0.6607 F1 0.4674 | Dev loss 0.7000 F1 0.3607\n",
            "[zeroshot_pt_xlmr] Best dev macro-F1: 0.3607\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for XLMRobertaForSequenceClassification:\n\tsize mismatch for roberta.embeddings.word_embeddings.weight: copying a param with shape torch.Size([250002, 768]) from checkpoint, the shape in current model is torch.Size([250004, 768]).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2399330231.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_zeroshot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mmodel_zeroshot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_token_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mmodel_zeroshot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_state\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mmodel_zeroshot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mmodel_zeroshot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2623\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2624\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2625\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2626\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for XLMRobertaForSequenceClassification:\n\tsize mismatch for roberta.embeddings.word_embeddings.weight: copying a param with shape torch.Size([250002, 768]) from checkpoint, the shape in current model is torch.Size([250004, 768])."
          ]
        }
      ],
      "source": [
        "# 08 - Zero-shot (PT): weighted training + dev threshold tuning, eval submission\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    cuda_mem(); free_vram(); cuda_mem()\n",
        "\n",
        "train_0s_df, dev_0s_df = prepare_supervised_frame(TRAIN_ZERO_SHOT, DEV, DEV_GOLD, language=\"PT\")\n",
        "\n",
        "from collections import Counter\n",
        "cnt0s = Counter(train_0s_df[\"Label\"].astype(int).tolist())\n",
        "n0, n1 = cnt0s.get(0, 1), cnt0s.get(1, 1)\n",
        "tot = n0 + n1\n",
        "w0_0s = tot / (2.0 * n0)\n",
        "w1_0s = tot / (2.0 * n1)\n",
        "class_weights_0s = torch.tensor([w0_0s, w1_0s], dtype=torch.float32, device=device)\n",
        "\n",
        "best_zeroshot_path, zeroshot_tokenizer = train_and_eval(\n",
        "    train_df=train_0s_df,\n",
        "    dev_df=dev_0s_df,\n",
        "    run_name=\"zeroshot_pt_xlmr\",\n",
        "    batch_size_micro=BATCH_SIZE_MICRO,\n",
        "    epochs=EPOCHS,\n",
        "    lr=LR,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    accum_steps=GRAD_ACCUM_STEPS,\n",
        "    class_weights=class_weights_0s\n",
        ")\n",
        "\n",
        "# After training, when reloading best for tuning/eval:\n",
        "model_zeroshot = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "ensure_mwe_tokens(zeroshot_tokenizer, model_zeroshot)\n",
        "state = torch.load(OUT_DIR / \"ckpt_zeroshot_pt_xlmr\" / \"best.pt\", map_location=str(device))\n",
        "if \"vocab_size\" in state:\n",
        "    vs = state[\"vocab_size\"]\n",
        "    if model_zeroshot.get_input_embeddings().weight.shape[0] != vs:\n",
        "        model_zeroshot.resize_token_embeddings(vs)\n",
        "model_zeroshot.load_state_dict(state[\"model_state\"])\n",
        "model_zeroshot.to(device)\n",
        "model_zeroshot.eval()\n",
        "\n",
        "dev_loader_0s = DataLoader(\n",
        "    IdiomDataset(dev_0s_df, zeroshot_tokenizer, MAX_LEN, is_infer=False),\n",
        "    batch_size=BATCH_SIZE_MICRO,\n",
        "    shuffle=False,\n",
        "    pin_memory=(device.type == \"cuda\"),\n",
        "    collate_fn=lambda b: collate_fn(b, zeroshot_tokenizer, MAX_LEN, is_infer=False)\n",
        ")\n",
        "dev_probs_0s = predict_probs(model_zeroshot, dev_loader_0s, zeroshot_tokenizer)\n",
        "dev_true_0s = dev_0s_df[\"Label\"].astype(int).to_numpy()\n",
        "\n",
        "best_th_0s, best_th_f1_0s = _tune_threshold(dev_probs_0s, dev_true_0s)\n",
        "print(f\"[zeroshot_pt_xlmr] Tuned threshold on dev: th={best_th_0s:.3f}, macro-F1={best_th_f1_0s:.4f}\")\n",
        "\n",
        "# Eval with tuned threshold\n",
        "eval_pt_df = prepare_eval_frame(EVAL, language=\"PT\")\n",
        "eval_loader_0s = make_infer_loader(eval_pt_df, zeroshot_tokenizer, MAX_LEN, BATCH_SIZE_MICRO)\n",
        "\n",
        "eval_preds_0s = predict_with_threshold(model_zeroshot, eval_loader_0s, zeroshot_tokenizer, best_th_0s)\n",
        "\n",
        "sub_0s = pd.DataFrame({\n",
        "    \"ID\": eval_pt_df[\"ID\"].astype(str),\n",
        "    \"Language\": eval_pt_df[\"Language\"],\n",
        "    \"Setting\": [\"zero_shot\"] * len(eval_pt_df),\n",
        "    \"Label\": eval_preds_0s\n",
        "})\n",
        "sub_path_0s = OUT_DIR / \"eval_submission_pt_zeroshot.csv\"\n",
        "sub_0s.to_csv(sub_path_0s, index=False)\n",
        "print(f\"Wrote {sub_path_0s}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08c35219",
      "metadata": {
        "id": "08c35219"
      },
      "source": [
        "## Dev-set diagnostics: confusion matrix & report for best runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52fa515a",
      "metadata": {
        "id": "52fa515a"
      },
      "outputs": [],
      "source": [
        "# 09 - Dev-set diagnostics: confusion matrix & report for best runs (robust to vocab size)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def _resize_model_to_ckpt_vocab(model, state_dict):\n",
        "    # Works for XLM-R; key name uses \"roberta.embeddings.word_embeddings.weight\"\n",
        "    emb_key = \"roberta.embeddings.word_embeddings.weight\"\n",
        "    if emb_key not in state_dict:\n",
        "        # fallback for other arch names if needed\n",
        "        for k in state_dict.keys():\n",
        "            if k.endswith(\"embeddings.word_embeddings.weight\"):\n",
        "                emb_key = k\n",
        "                break\n",
        "    expected_vocab = state_dict[emb_key].shape[0]\n",
        "    current_vocab = model.get_input_embeddings().weight.shape[0]\n",
        "    if expected_vocab != current_vocab:\n",
        "        model.resize_token_embeddings(expected_vocab)\n",
        "\n",
        "def eval_on_dev(best_ckpt_path: Path, tokenizer, dev_df: pd.DataFrame, tag: str):\n",
        "    # Fresh model (fp32)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "\n",
        "    # Load checkpoint state\n",
        "    state = torch.load(best_ckpt_path, map_location=str(device))\n",
        "    sd = state[\"model_state\"]\n",
        "\n",
        "    # Ensure model embedding size matches checkpoint BEFORE load_state_dict\n",
        "    _resize_model_to_ckpt_vocab(model, sd)\n",
        "\n",
        "    # Optional: make sure tokenizer has the markers\n",
        "    vocab = tokenizer.get_vocab()\n",
        "    if (\"<mwe>\" not in vocab) or (\"</mwe>\" not in vocab):\n",
        "        tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<mwe>\", \"</mwe>\"]})\n",
        "        # If we just changed tokenizer, we won't change model size again here,\n",
        "        # because the checkpoint dictates the embedding size we've already set.\n",
        "\n",
        "    # Load weights and move to device\n",
        "    model.load_state_dict(sd, strict=True)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    dev_loader = DataLoader(\n",
        "        IdiomDataset(dev_df, tokenizer, MAX_LEN, is_infer=False),\n",
        "        batch_size=BATCH_SIZE_MICRO,\n",
        "        shuffle=False,\n",
        "        pin_memory=(device.type == \"cuda\"),\n",
        "        collate_fn=lambda b: collate_fn(b, tokenizer, MAX_LEN, is_infer=False)\n",
        "    )\n",
        "\n",
        "    # accum_steps=1 for eval\n",
        "    _, f1, y_true, y_pred = run_epoch(\n",
        "        model, dev_loader, tokenizer, optimizer=None, scheduler=None, train_mode=False, accum_steps=1\n",
        "    )\n",
        "    print(f\"[{tag}] Dev macro-F1: {f1:.4f}\")\n",
        "    print(classification_report(y_true, y_pred, digits=4))\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "\n",
        "# Optionally free VRAM between evals on GPU\n",
        "if device.type == \"cuda\":\n",
        "    cuda_mem(); free_vram(); cuda_mem()\n",
        "\n",
        "eval_on_dev(OUT_DIR / \"ckpt_oneshot_pt_xlmr_weighted\" / \"best.pt\", tok_1s, dev_1s_df, \"One-shot PT XLM-R (weighted)\")\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    cuda_mem(); free_vram(); cuda_mem()\n",
        "\n",
        "eval_on_dev(OUT_DIR / \"ckpt_zeroshot_pt_xlmr\" / \"best.pt\", zeroshot_tokenizer, dev_0s_df, \"Zero-shot PT XLM-R\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "217350f3",
      "metadata": {
        "id": "217350f3"
      },
      "source": [
        "## Save final best checkpoints to disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6950641c",
      "metadata": {
        "id": "6950641c"
      },
      "outputs": [],
      "source": [
        "# 10 - Save final best checkpoints to disk\n",
        "\n",
        "final_oneshot_dir = OUT_DIR / \"final_oneshot_pt_xlmr\"\n",
        "final_zeroshot_dir = OUT_DIR / \"final_zeroshot_pt_xlmr\"\n",
        "final_oneshot_dir.mkdir(parents=True, exist_ok=True)\n",
        "final_zeroshot_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "torch.save(torch.load(OUT_DIR / \"ckpt_oneshot_pt_xlmr_weighted\" / \"best.pt\", map_location=str(device)), final_oneshot_dir / \"model.pt\")\n",
        "torch.save(torch.load(OUT_DIR / \"ckpt_zeroshot_pt_xlmr\" / \"best.pt\", map_location=str(device)), final_zeroshot_dir / \"model.pt\")\n",
        "\n",
        "print(\"Saved final checkpoints.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}