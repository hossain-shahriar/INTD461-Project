{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bdc228a",
   "metadata": {},
   "source": [
    "## Setup: CPU/GPU switch, imports, paths, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1667b3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA H100 80GB HBM3 MIG 2g.20gb\n",
      "Downloading NLTK WordNet data...\n",
      "Downloading NLTK stopwords data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/mhossai6/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/mhossai6/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mhossai6/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_SETTING=zero_shot, BATCH_SIZE=16, NUM_EPOCHS=3, LR=2e-05\n",
      "USE_WSD = True\n"
     ]
    }
   ],
   "source": [
    "# 01 - Setup: CPU/GPU switch, imports, paths, config\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "# ---- Device selection ----\n",
    "RUN_DEVICE = \"gpu\"  # \"gpu\" or \"cpu\"\n",
    "\n",
    "if RUN_DEVICE.lower() == \"gpu\" and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.set_num_threads(max(1, os.cpu_count() // 2))\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# ---- Seeds ----\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ---- Metrics ----\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ---- Transformers ----\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "# ---- WSD: NLTK WordNet (custom simple Lesk) ----\n",
    "import nltk\n",
    "\n",
    "# Make sure WordNet + stopwords are available\n",
    "try:\n",
    "    nltk.data.find(\"corpora/wordnet\")\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK WordNet data...\")\n",
    "    nltk.download(\"wordnet\")\n",
    "    nltk.download(\"omw-1.4\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"corpora/stopwords\")\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK stopwords data...\")\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "# ---- Data paths (SemEval SubTaskA) ----\n",
    "DATA_DIR = Path(\"SemEval_2022_Task2-idiomaticity/SubTaskA\")\n",
    "TRAIN_ONE_SHOT = DATA_DIR / \"Data\" / \"train_one_shot.csv\"\n",
    "TRAIN_ZERO_SHOT = DATA_DIR / \"Data\" / \"train_zero_shot.csv\"\n",
    "DEV = DATA_DIR / \"Data\" / \"dev.csv\"\n",
    "DEV_GOLD = DATA_DIR / \"Data\" / \"dev_gold.csv\"\n",
    "EVAL = DATA_DIR / \"Data\" / \"eval.csv\"\n",
    "EVAL_SUB_FMT = DATA_DIR / \"Data\" / \"eval_submission_format.csv\"\n",
    "\n",
    "# ---- Outputs ----\n",
    "OUT_DIR = Path(\"outputs_en_xlmr_wsd\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Model & training config ----\n",
    "MODEL_NAME = \"xlm-roberta-base\"\n",
    "NUM_LABELS = 2\n",
    "\n",
    "# \"zero_shot\" uses train_zero_shot.csv, \"one_shot\" uses train_one_shot.csv\n",
    "TRAIN_SETTING = \"zero_shot\"  # or \"one_shot\"\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "LR = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "BATCH_SIZE_GPU = 16\n",
    "BATCH_SIZE_CPU = 8\n",
    "BATCH_SIZE = BATCH_SIZE_GPU if device.type == \"cuda\" else BATCH_SIZE_CPU\n",
    "\n",
    "print(f\"TRAIN_SETTING={TRAIN_SETTING}, BATCH_SIZE={BATCH_SIZE}, NUM_EPOCHS={NUM_EPOCHS}, LR={LR}\")\n",
    "\n",
    "# Toggle WSD usage (set False for baseline)\n",
    "USE_WSD = True\n",
    "print(\"USE_WSD =\", USE_WSD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3111e552",
   "metadata": {},
   "source": [
    "## Data loading, context formatting, WSD helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee2f025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 - Data loading, context formatting, WSD helpers (EN only)\n",
    "\n",
    "def load_any_csv(path: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(path, sep=None, engine=\"python\", dtype=str)\n",
    "\n",
    "def ensure_label_int(df: pd.DataFrame, col=\"Label\") -> pd.DataFrame:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(int)\n",
    "    return df\n",
    "\n",
    "def mark_first_case_insensitive(text: str, needle: str, ltag=\"<mwe>\", rtag=\"</mwe>\") -> str:\n",
    "    if not isinstance(text, str) or not isinstance(needle, str):\n",
    "        return text\n",
    "    lt = text.lower()\n",
    "    ln = needle.lower()\n",
    "    i = lt.find(ln)\n",
    "    if i == -1:\n",
    "        return text\n",
    "    return text[:i] + ltag + text[i:i+len(needle)] + rtag + text[i+len(needle):]\n",
    "\n",
    "def pack_context(prev: str, target: str, nxt: str, mwe: str) -> str:\n",
    "    prev = \"\" if pd.isna(prev) else prev\n",
    "    nxt = \"\" if pd.isna(nxt) else nxt\n",
    "    target = \"\" if pd.isna(target) else target\n",
    "    tgt_marked = mark_first_case_insensitive(target, mwe)\n",
    "    return f\"Previous: {prev}\\nTarget: {tgt_marked}\\nNext: {nxt}\"\n",
    "\n",
    "# ---- WSD helpers (custom simple Lesk using NLTK WordNet) ----\n",
    "\n",
    "def build_simple_sentence(row) -> str:\n",
    "    prev = row.get(\"Previous\", \"\")\n",
    "    tgt = row.get(\"Target\", \"\")\n",
    "    nxt = row.get(\"Next\", \"\")\n",
    "\n",
    "    def _clean(x):\n",
    "        if isinstance(x, str):\n",
    "            return x\n",
    "        if pd.isna(x):\n",
    "            return \"\"\n",
    "        return str(x)\n",
    "\n",
    "    prev = _clean(prev)\n",
    "    tgt = _clean(tgt)\n",
    "    nxt = _clean(nxt)\n",
    "    return \" \".join([prev, tgt, nxt]).strip()\n",
    "\n",
    "\n",
    "def get_mwe_head(mwe: str) -> str:\n",
    "    if not isinstance(mwe, str):\n",
    "        return \"\"\n",
    "    toks = mwe.split()\n",
    "    return toks[-1] if toks else \"\"\n",
    "\n",
    "\n",
    "def simple_lesk_nltk(context_sentence: str, ambiguous_word: str):\n",
    "    \"\"\"\n",
    "    Very small Lesk-style WSD using NLTK WordNet.\n",
    "    Returns a Synset or None.\n",
    "    \"\"\"\n",
    "    if not context_sentence or not ambiguous_word:\n",
    "        return None\n",
    "\n",
    "    # preprocess context\n",
    "    tokens = [\n",
    "        w.strip(string.punctuation).lower()\n",
    "        for w in context_sentence.split()\n",
    "    ]\n",
    "    context = [w for w in tokens if w and w not in STOP_WORDS]\n",
    "\n",
    "    synsets = wn.synsets(ambiguous_word)\n",
    "    if not synsets:\n",
    "        return None\n",
    "\n",
    "    best_syn = None\n",
    "    max_overlap = 0\n",
    "\n",
    "    for syn in synsets:\n",
    "        sig_tokens = syn.definition().split()\n",
    "        for ex in syn.examples():\n",
    "            sig_tokens += ex.split()\n",
    "\n",
    "        sig_tokens = [\n",
    "            w.strip(string.punctuation).lower()\n",
    "            for w in sig_tokens\n",
    "        ]\n",
    "        signature = [w for w in sig_tokens if w and w not in STOP_WORDS]\n",
    "\n",
    "        overlap = len(set(signature) & set(context))\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_syn = syn\n",
    "\n",
    "    return best_syn\n",
    "\n",
    "\n",
    "def annotate_with_wsd(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds SenseID and SenseGloss columns for EN rows using simple_lesk_nltk.\n",
    "    Operates in-memory; no files written.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    sense_ids = []\n",
    "    sense_glosses = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        sent = build_simple_sentence(row)\n",
    "        mwe = row.get(\"MWE\", \"\") or \"\"\n",
    "        head = get_mwe_head(mwe)\n",
    "\n",
    "        if not sent or not head:\n",
    "            sense_ids.append(\"\")\n",
    "            sense_glosses.append(\"\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            synset = simple_lesk_nltk(sent, head)\n",
    "        except Exception:\n",
    "            synset = None\n",
    "\n",
    "        if synset is None:\n",
    "            sense_ids.append(\"\")\n",
    "            sense_glosses.append(\"\")\n",
    "        else:\n",
    "            sense_ids.append(synset.name())         # e.g. 'bank.n.02'\n",
    "            sense_glosses.append(synset.definition())\n",
    "\n",
    "    df[\"SenseID\"] = sense_ids\n",
    "    df[\"SenseGloss\"] = sense_glosses\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_supervised_frame_en(\n",
    "    train_path: Path,\n",
    "    dev_path: Path,\n",
    "    dev_gold_path: Path,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Prepare EN train/dev frames with optional WSD gloss appended to 'text'.\n",
    "    \"\"\"\n",
    "    train_df = load_any_csv(train_path)\n",
    "    dev_df = load_any_csv(dev_path)\n",
    "    gold_df = load_any_csv(dev_gold_path)\n",
    "\n",
    "    train_df.columns = [c.strip() for c in train_df.columns]\n",
    "    dev_df.columns = [c.strip() for c in dev_df.columns]\n",
    "    gold_df.columns = [c.strip() for c in gold_df.columns]\n",
    "\n",
    "    train_df = train_df[train_df[\"Language\"] == \"EN\"].copy()\n",
    "    dev_df = dev_df[dev_df[\"Language\"] == \"EN\"].copy()\n",
    "\n",
    "    gold = gold_df[gold_df[\"Language\"] == \"EN\"][[\"ID\", \"Label\"]].copy()\n",
    "    gold[\"ID\"] = gold[\"ID\"].astype(str)\n",
    "    dev_df[\"ID\"] = dev_df[\"ID\"].astype(str)\n",
    "    dev_lab = dev_df.merge(gold, on=\"ID\", how=\"left\")\n",
    "\n",
    "    train_df = ensure_label_int(train_df, \"Label\")\n",
    "    dev_lab = ensure_label_int(dev_lab, \"Label\")\n",
    "\n",
    "    if USE_WSD:\n",
    "        print(\"Annotating EN train/dev with WSD (this might take a bit)...\")\n",
    "        train_df = annotate_with_wsd(train_df)\n",
    "        dev_lab = annotate_with_wsd(dev_lab)\n",
    "\n",
    "    def _build_text(row):\n",
    "        ctx = pack_context(\n",
    "            row.get(\"Previous\", \"\"),\n",
    "            row.get(\"Target\", \"\"),\n",
    "            row.get(\"Next\", \"\"),\n",
    "            row.get(\"MWE\", \"\"),\n",
    "        )\n",
    "        if USE_WSD:\n",
    "            gloss = (row.get(\"SenseGloss\", \"\") or \"\").strip()\n",
    "            if gloss:\n",
    "                return ctx + f\"\\n\\nSense gloss (WordNet): {gloss}\"\n",
    "        return ctx\n",
    "\n",
    "    train_df[\"text\"] = train_df.apply(_build_text, axis=1)\n",
    "    dev_lab[\"text\"] = dev_lab.apply(_build_text, axis=1)\n",
    "\n",
    "    return train_df, dev_lab\n",
    "\n",
    "\n",
    "def prepare_eval_frame_en(eval_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare EN eval frame (no labels) with optional WSD.\n",
    "    \"\"\"\n",
    "    df = load_any_csv(eval_path)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    df = df[df[\"Language\"] == \"EN\"].copy()\n",
    "\n",
    "    if USE_WSD:\n",
    "        print(\"Annotating EN eval with WSD (this might take a bit)...\")\n",
    "        df = annotate_with_wsd(df)\n",
    "\n",
    "    def _build_text(row):\n",
    "        ctx = pack_context(\n",
    "            row.get(\"Previous\", \"\"),\n",
    "            row.get(\"Target\", \"\"),\n",
    "            row.get(\"Next\", \"\"),\n",
    "            row.get(\"MWE\", \"\"),\n",
    "        )\n",
    "        if USE_WSD:\n",
    "            gloss = (row.get(\"SenseGloss\", \"\") or \"\").strip()\n",
    "            if gloss:\n",
    "                return ctx + f\"\\n\\nSense gloss (WordNet): {gloss}\"\n",
    "        return ctx\n",
    "\n",
    "    df[\"text\"] = df.apply(_build_text, axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd5d12f",
   "metadata": {},
   "source": [
    "## Dataset, dataloader, XLM-R model + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "573a7d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03 - Dataset, dataloader, XLM-R model + tokenizer\n",
    "\n",
    "class IdiomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer, max_length: int = 256, with_labels: bool = True):\n",
    "        self.texts = df[\"text\"].tolist()\n",
    "        self.with_labels = with_labels\n",
    "        self.labels = df[\"Label\"].tolist() if with_labels and \"Label\" in df.columns else None\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        text = self.texts[idx]\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "        if self.with_labels and self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "\n",
    "def build_model_and_tokenizer(move_to_device: bool = True):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    special_tokens = {\"additional_special_tokens\": [\"<mwe>\", \"</mwe>\"]}\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if move_to_device:\n",
    "        model.to(device)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def make_loader(df: pd.DataFrame, tokenizer, batch_size: int, shuffle: bool, max_length: int = 256, with_labels: bool = True):\n",
    "    ds = IdiomDataset(df, tokenizer, max_length=max_length, with_labels=with_labels)\n",
    "    return torch.utils.data.DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=2 if device.type == \"cuda\" else 0,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14be663a",
   "metadata": {},
   "source": [
    "## Training, prediction utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30ab3409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04 - Training epoch, prediction utilities\n",
    "\n",
    "def run_epoch(\n",
    "    model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    train: bool = True,\n",
    "    class_weights: torch.Tensor = None,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Run one epoch (train or eval). Returns (avg_loss, macro_F1).\n",
    "    If train=False, optimizer and scheduler are not used.\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    if class_weights is not None:\n",
    "        class_weights = class_weights.to(device)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            if class_weights is not None:\n",
    "                log_probs = torch.log_softmax(logits, dim=-1)\n",
    "                loss = torch.nn.functional.nll_loss(\n",
    "                    log_probs,\n",
    "                    labels,\n",
    "                    weight=class_weights,\n",
    "                )\n",
    "            else:\n",
    "                loss = outputs.loss\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        all_y_true.extend(labels.detach().cpu().tolist())\n",
    "        all_y_pred.extend(preds.detach().cpu().tolist())\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(all_y_true))\n",
    "    macro_f1 = f1_score(all_y_true, all_y_pred, average=\"macro\")\n",
    "    return avg_loss, macro_f1\n",
    "\n",
    "\n",
    "def predict(model, dataloader) -> Tuple[List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Returns (y_true, y_pred) for labeled data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            all_y_true.extend(labels.detach().cpu().tolist())\n",
    "            all_y_pred.extend(preds.detach().cpu().tolist())\n",
    "\n",
    "    return all_y_true, all_y_pred\n",
    "\n",
    "\n",
    "def predict_no_labels(model, dataloader) -> List[int]:\n",
    "    \"\"\"\n",
    "    Returns predicted labels for unlabeled data (eval set).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            all_y_pred.extend(preds.detach().cpu().tolist())\n",
    "\n",
    "    return all_y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d15ef70",
   "metadata": {},
   "source": [
    "## Train EN XLM-R (+/- WSD), evaluate on dev + eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13c5754b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing EN train/dev frames...\n",
      "Annotating EN train/dev with WSD (this might take a bit)...\n",
      "EN train size: 3327, EN dev size: 466\n",
      "EN train label counts: {0: 1762, 1: 1565}\n",
      "EN class weights: [0.9440976163450624, 1.0629392971246006]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 22:34:34.700322: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-26 22:34:34.712254: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764225274.721466  482884 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764225274.724153  482884 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764225274.733398  482884 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764225274.733409  482884 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764225274.733411  482884 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764225274.733413  482884 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-26 22:34:34.736680: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/3 (EN zero_shot, USE_WSD=True) ===\n",
      "[EN train] loss=0.6191 macro-F1=0.6297\n",
      "[EN dev]   loss=0.7501 macro-F1=0.7320\n",
      "New best EN dev macro-F1=0.7320 -> saved to outputs_en_xlmr_wsd/xlmr_en_zero_shot_wsd_best.pt\n",
      "\n",
      "=== Epoch 2/3 (EN zero_shot, USE_WSD=True) ===\n",
      "[EN train] loss=0.2104 macro-F1=0.9399\n",
      "[EN dev]   loss=1.0133 macro-F1=0.7939\n",
      "New best EN dev macro-F1=0.7939 -> saved to outputs_en_xlmr_wsd/xlmr_en_zero_shot_wsd_best.pt\n",
      "\n",
      "=== Epoch 3/3 (EN zero_shot, USE_WSD=True) ===\n",
      "[EN train] loss=0.0755 macro-F1=0.9834\n",
      "[EN dev]   loss=1.0156 macro-F1=0.8166\n",
      "New best EN dev macro-F1=0.8166 -> saved to outputs_en_xlmr_wsd/xlmr_en_zero_shot_wsd_best.pt\n",
      "\n",
      "Best EN dev macro-F1 (zero_shot, USE_WSD=True): 0.8166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best checkpoint from: outputs_en_xlmr_wsd/xlmr_en_zero_shot_wsd_best.pt\n",
      "\n",
      "[EN dev final] macro-F1=0.8166\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8072    0.7363    0.7701       182\n",
      "           1     0.8400    0.8873    0.8630       284\n",
      "\n",
      "    accuracy                         0.8283       466\n",
      "   macro avg     0.8236    0.8118    0.8166       466\n",
      "weighted avg     0.8272    0.8283    0.8267       466\n",
      "\n",
      "[[134  48]\n",
      " [ 32 252]]\n",
      "Annotating EN eval with WSD (this might take a bit)...\n",
      "\n",
      "Wrote EN eval submission to: outputs_en_xlmr_wsd/eval_submission_en_xlmr_zero_shot_wsd.csv\n"
     ]
    }
   ],
   "source": [
    "# 05 - Train EN XLM-R (+/- WSD) and evaluate on EN dev + eval\n",
    "\n",
    "if TRAIN_SETTING == \"one_shot\":\n",
    "    train_path = TRAIN_ONE_SHOT\n",
    "elif TRAIN_SETTING == \"zero_shot\":\n",
    "    train_path = TRAIN_ZERO_SHOT\n",
    "else:\n",
    "    raise ValueError(f\"Unknown TRAIN_SETTING: {TRAIN_SETTING}\")\n",
    "\n",
    "print(\"\\nPreparing EN train/dev frames...\")\n",
    "train_en_df, dev_en_df = prepare_supervised_frame_en(\n",
    "    train_path=train_path,\n",
    "    dev_path=DEV,\n",
    "    dev_gold_path=DEV_GOLD,\n",
    ")\n",
    "print(f\"EN train size: {len(train_en_df)}, EN dev size: {len(dev_en_df)}\")\n",
    "\n",
    "# Class weights for EN training\n",
    "label_counts = train_en_df[\"Label\"].value_counts().to_dict()\n",
    "print(\"EN train label counts:\", label_counts)\n",
    "total = sum(label_counts.values())\n",
    "class_weights = [total / (2.0 * label_counts.get(i, 1)) for i in range(NUM_LABELS)]\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(\"EN class weights:\", class_weights)\n",
    "\n",
    "# Build model + tokenizer\n",
    "model, tokenizer = build_model_and_tokenizer(move_to_device=True)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader_en = make_loader(\n",
    "    train_en_df,\n",
    "    tokenizer,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    max_length=256,\n",
    "    with_labels=True,\n",
    ")\n",
    "dev_loader_en = make_loader(\n",
    "    dev_en_df,\n",
    "    tokenizer,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    max_length=256,\n",
    "    with_labels=True,\n",
    ")\n",
    "\n",
    "# Optimizer & scheduler\n",
    "num_training_steps = NUM_EPOCHS * len(train_loader_en)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(WARMUP_RATIO * num_training_steps),\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "best_dev_f1_en = -1.0\n",
    "suffix = \"wsd\" if USE_WSD else \"baseline\"\n",
    "best_path_en = OUT_DIR / f\"xlmr_en_{TRAIN_SETTING}_{suffix}_best.pt\"\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{NUM_EPOCHS} (EN {TRAIN_SETTING}, USE_WSD={USE_WSD}) ===\")\n",
    "    train_loss, train_f1 = run_epoch(\n",
    "        model,\n",
    "        train_loader_en,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        train=True,\n",
    "        class_weights=class_weights_tensor,\n",
    "    )\n",
    "    print(f\"[EN train] loss={train_loss:.4f} macro-F1={train_f1:.4f}\")\n",
    "\n",
    "    dev_loss, dev_f1 = run_epoch(\n",
    "        model,\n",
    "        dev_loader_en,\n",
    "        optimizer=None,\n",
    "        scheduler=None,\n",
    "        train=False,\n",
    "        class_weights=None,\n",
    "    )\n",
    "    print(f\"[EN dev]   loss={dev_loss:.4f} macro-F1={dev_f1:.4f}\")\n",
    "\n",
    "    if dev_f1 > best_dev_f1_en:\n",
    "        best_dev_f1_en = dev_f1\n",
    "        torch.save(model.state_dict(), best_path_en)\n",
    "        print(f\"New best EN dev macro-F1={best_dev_f1_en:.4f} -> saved to {best_path_en}\")\n",
    "\n",
    "print(f\"\\nBest EN dev macro-F1 ({TRAIN_SETTING}, USE_WSD={USE_WSD}): {best_dev_f1_en:.4f}\")\n",
    "\n",
    "# Reload best checkpoint and evaluate final dev + eval\n",
    "model_best, tokenizer_best = build_model_and_tokenizer(move_to_device=True)\n",
    "state = torch.load(best_path_en, map_location=device)\n",
    "model_best.load_state_dict(state)\n",
    "model_best.to(device)\n",
    "model_best.eval()\n",
    "print(f\"Loaded best checkpoint from: {best_path_en}\")\n",
    "\n",
    "# Dev evaluation\n",
    "dev_loader_en_best = make_loader(\n",
    "    dev_en_df,\n",
    "    tokenizer_best,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    max_length=256,\n",
    "    with_labels=True,\n",
    ")\n",
    "ytrue_dev, ypred_dev = predict(model_best, dev_loader_en_best)\n",
    "dev_f1_final = f1_score(ytrue_dev, ypred_dev, average=\"macro\")\n",
    "print(f\"\\n[EN dev final] macro-F1={dev_f1_final:.4f}\")\n",
    "print(classification_report(ytrue_dev, ypred_dev, digits=4))\n",
    "print(confusion_matrix(ytrue_dev, ypred_dev))\n",
    "\n",
    "# Eval predictions\n",
    "eval_en_df = prepare_eval_frame_en(EVAL)\n",
    "eval_loader_en = make_loader(\n",
    "    eval_en_df,\n",
    "    tokenizer_best,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    max_length=256,\n",
    "    with_labels=False,\n",
    ")\n",
    "eval_preds = predict_no_labels(model_best, eval_loader_en)\n",
    "\n",
    "setting_str = TRAIN_SETTING\n",
    "sub_en = pd.DataFrame({\n",
    "    \"ID\": eval_en_df[\"ID\"].astype(str),\n",
    "    \"Language\": eval_en_df[\"Language\"],\n",
    "    \"Setting\": [setting_str] * len(eval_en_df),\n",
    "    \"Label\": eval_preds,\n",
    "})\n",
    "sub_path_en = OUT_DIR / f\"eval_submission_en_xlmr_{setting_str}_{suffix}.csv\"\n",
    "sub_en.to_csv(sub_path_en, index=False)\n",
    "print(f\"\\nWrote EN eval submission to: {sub_path_en}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5f94e2",
   "metadata": {},
   "source": [
    "## Save run metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190e2fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved run metadata.\n",
      "Outputs directory: outputs_en_xlmr_wsd\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 06 - Save run metadata\n",
    "\n",
    "with open(OUT_DIR / f\"run_en_xlmr_{TRAIN_SETTING}_{suffix}.txt\", \"w\") as f:\n",
    "    f.write(f\"MODEL_NAME={MODEL_NAME}\\n\")\n",
    "    f.write(f\"DEVICE={device.type}\\n\")\n",
    "    f.write(f\"TRAIN_SETTING={TRAIN_SETTING}\\n\")\n",
    "    f.write(f\"USE_WSD={USE_WSD}\\n\")\n",
    "    f.write(f\"NUM_EPOCHS={NUM_EPOCHS}\\n\")\n",
    "    f.write(f\"BATCH_SIZE={BATCH_SIZE}\\n\")\n",
    "    f.write(f\"LR={LR}\\n\")\n",
    "    f.write(f\"BEST_EN_DEV_F1={best_dev_f1_en:.4f}\\n\")\n",
    "\n",
    "print(\"Saved run metadata.\")\n",
    "print(\"Outputs directory:\", OUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "nlp"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
