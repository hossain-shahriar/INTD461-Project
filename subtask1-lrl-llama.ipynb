{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e4d019b",
   "metadata": {},
   "source": [
    "## Setup: CPU/GPU, imports, seeds, paths, model choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56794150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA H100 80GB HBM3 MIG 2g.20gb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6029407/mhossai6/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV path: /project/6029407/mhossai6/INTD461-Project/lrl_idioms.csv\n",
      "Model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 01 - Setup: single switch for CPU/GPU, imports, seeds, paths, model choice\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "RUN_DEVICE = \"gpu\"  # \"gpu\" to use CUDA if available, otherwise CPU\n",
    "\n",
    "if RUN_DEVICE.lower() == \"gpu\" and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.set_num_threads(max(1, os.cpu_count() // 2))\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ---- Single CSV with all languages; same folder as this notebook\n",
    "MULTI_CSV = Path(\"lrl_idioms.csv\")\n",
    "\n",
    "# Output directory (shared, with per-language files)\n",
    "OUT_DIR = Path(\"outputs_lrl_llama\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# LLaMA model\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Batch size for batched logits classification\n",
    "BATCH_GEN_ZERO = 8 if device.type == \"cuda\" else 4\n",
    "\n",
    "print(f\"CSV path: {MULTI_CSV.resolve()}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Device: {device.type}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dec849",
   "metadata": {},
   "source": [
    "## Data loading & text utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94edf5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded lrl_idioms.csv (all languages):\n",
      "     ID Language                          MWE  \\\n",
      "0  bn_1       BN                   বৃষ্টির জল   \n",
      "1  bn_2       BN  সংসার সুখের হয় রমণীর গুণে ।   \n",
      "2  bn_3       BN                 যাচ্ছা তাই ।   \n",
      "3  bn_4       BN         উলুবনে মুক্ত ছড়ানো ।   \n",
      "4  bn_5       BN                     নিজের ঘর   \n",
      "\n",
      "                                            Previous  \\\n",
      "0                 আকাশে অনেকক্ষণ ধরে কালো মেঘ জমছিল।   \n",
      "1  ছোট বাসা, অল্প রোজগার—সব মিলিয়ে খুব অভাবের জীব...   \n",
      "2  পাড়ার ক্লাবের অনুষ্ঠানের দায়িত্ব যাঁর হাতে দেও...   \n",
      "3  প্রফেসর গভীর মনোযোগ দিয়ে কঠিন তত্ত্বগুলো বুঝিয়...   \n",
      "4   সারাদিন বাইরে থাকায় সে ভীষণ ক্লান্ত হয়ে গিয়েছিল।   \n",
      "\n",
      "                                              Target  \\\n",
      "0           হঠাৎ টুপটাপ করে বৃষ্টির জল পড়া শুরু হলো।   \n",
      "1  তবু হাসিমুখে সব সামলে রাখত তারা; লোকজন বলত, সং...   \n",
      "2  ফলে মঞ্চ, সাউন্ড, আলো—সব মিলিয়ে শেষ দিন একেবার...   \n",
      "3  কিন্তু ছাত্ররা মোবাইলে ব্যস্ত, কেউ শুনছেই না—এ...   \n",
      "4    বাড়ি ফিরে সে সোজা নিজের ঘরে গিয়ে দরজা বন্ধ করল।   \n",
      "\n",
      "                                                Next  Label  \n",
      "0  কিছুক্ষণের মধ্যেই রাস্তা ভিজে একেবারে চকচকে হয়...      0  \n",
      "1  পাড়ার সবাই ওদের দাম্পত্য দেখে মুগ্ধ হয়ে আশীর্ব...      1  \n",
      "2  অতিথিরা কেমন যেন মুখ চেপে হেসে অনুষ্ঠান শেষ হও...      1  \n",
      "3  শেষে তিনি বিরক্ত হয়ে ক্লাস বন্ধ করে দিয়ে বললেন...      1  \n",
      "4    ব্যাগটা রেখে একটু পরে বিছানায় শুয়ে বিশ্রাম নিল।      0  \n",
      "Total examples (all languages): 268\n",
      "\n",
      "Languages found in dataset: ['BN', 'ML', 'PA']\n"
     ]
    }
   ],
   "source": [
    "# 02 - Data loading & text utilities\n",
    "\n",
    "def load_lrl_csv(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assumes CSV with columns at least:\n",
    "    ID, Language, MWE, Previous, Target, Next, Label\n",
    "\n",
    "    We'll keep Label only for metrics, not for prompts.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, sep=None, engine=\"python\", dtype=str)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    if \"Label\" in df.columns:\n",
    "        df[\"Label\"] = df[\"Label\"].astype(int)\n",
    "    return df\n",
    "\n",
    "def mark_first_case_insensitive(text: str, needle: str,\n",
    "                                ltag=\"<mwe>\", rtag=\"</mwe>\") -> str:\n",
    "    if not isinstance(text, str) or not isinstance(needle, str):\n",
    "        return text\n",
    "    lt = text.lower()\n",
    "    ln = needle.lower()\n",
    "    i = lt.find(ln)\n",
    "    if i == -1:\n",
    "        return text\n",
    "    return text[:i] + ltag + text[i:i+len(needle)] + rtag + text[i+len(needle):]\n",
    "\n",
    "def pack_context(prev: str, target: str, nxt: str, mwe: str) -> str:\n",
    "    \"\"\"\n",
    "    Build the context block for the LLM. Note: NO label is included.\n",
    "    \"\"\"\n",
    "    prev = \"\" if pd.isna(prev) else prev\n",
    "    nxt = \"\" if pd.isna(nxt) else nxt\n",
    "    tgt = mark_first_case_insensitive(target, mwe)\n",
    "    # Keep instructions in English; content can be BN/ML/PA/etc.\n",
    "    return f\"Previous: {prev}\\nTarget: {tgt}\\nNext: {nxt}\"\n",
    "\n",
    "# Load full LRL dataset\n",
    "full_df = load_lrl_csv(MULTI_CSV)\n",
    "print(\"Loaded lrl_idioms.csv (all languages):\")\n",
    "print(full_df.head())\n",
    "print(f\"Total examples (all languages): {len(full_df)}\")\n",
    "\n",
    "# Get unique languages present\n",
    "languages = sorted(full_df[\"Language\"].dropna().unique().tolist())\n",
    "print(\"\\nLanguages found in dataset:\", languages)\n",
    "\n",
    "if len(languages) == 0:\n",
    "    raise ValueError(\"No languages found in lrl_idioms.csv (Language column empty?).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e61715",
   "metadata": {},
   "source": [
    "## Load LLaMA (CPU/GPU) and tokenizer with VRAM-safe setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fb7b962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA allocated: 0.00 GiB | reserved: 0.00 GiB\n",
      "VRAM cleanup done.\n",
      "CUDA allocated: 0.00 GiB | reserved: 0.00 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "2025-12-03 19:11:31.233570: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-03 19:11:31.247201: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764817891.257618 2477136 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764817891.260255 2477136 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764817891.269359 2477136 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764817891.269369 2477136 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764817891.269371 2477136 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764817891.269373 2477136 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-03 19:11:31.272817: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA allocated: 5.98 GiB | reserved: 6.00 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 03 - Load LLM (CPU or GPU) and tokenizer with VRAM-safe setup\n",
    "\n",
    "from getpass import getpass\n",
    "from huggingface_hub import login as hf_login\n",
    "import gc\n",
    "\n",
    "# ----- VRAM helpers (safe on CPU too)\n",
    "def free_vram():\n",
    "    \"\"\"\n",
    "    Basic cleanup: run garbage collection and clear CUDA cache.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    print(\"VRAM cleanup done.\")\n",
    "\n",
    "def cuda_mem():\n",
    "    if torch.cuda.is_available():\n",
    "        a = torch.cuda.memory_allocated() / (1024**3)\n",
    "        r = torch.cuda.memory_reserved() / (1024**3)\n",
    "        print(f\"CUDA allocated: {a:.2f} GiB | reserved: {r:.2f} GiB\")\n",
    "    else:\n",
    "        print(\"CUDA not available.\")\n",
    "\n",
    "# Prefer segmented allocator on CUDA to reduce fragmentation\n",
    "if device.type == \"cuda\":\n",
    "    os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "\n",
    "# ----- Optional Hugging Face login (supports gated repos)\n",
    "hf_token = os.getenv(\"HF_TOKEN\", \"\").strip()\n",
    "if not hf_token:\n",
    "    try:\n",
    "        hf_token = getpass(\"Enter your Hugging Face token (or press Enter to skip): \").strip()\n",
    "    except Exception:\n",
    "        hf_token = input(\"Enter your Hugging Face token (or press Enter to skip): \").strip()\n",
    "if hf_token:\n",
    "    try:\n",
    "        hf_login(token=hf_token)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Hugging Face login failed: {e}\")\n",
    "\n",
    "_token_arg = {\"token\": hf_token} if hf_token else {}\n",
    "\n",
    "# ----- Clean up any previous models before loading a new one\n",
    "if device.type == \"cuda\":\n",
    "    cuda_mem()\n",
    "free_vram()\n",
    "if device.type == \"cuda\":\n",
    "    cuda_mem()\n",
    "\n",
    "# ----- Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    "    **_token_arg\n",
    ")\n",
    "\n",
    "# ----- Choose dtype & placement to avoid OOM\n",
    "if device.type == \"cuda\":\n",
    "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\",      # let HF shard / offload as needed\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        **_token_arg\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float32,\n",
    "        device_map={\"\": \"cpu\"},  # all on CPU\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        **_token_arg\n",
    "    )\n",
    "\n",
    "# ----- Ensure pad token and attention_mask behavior are well-defined\n",
    "if tokenizer.pad_token_id is None:\n",
    "    if tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    else:\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model.eval()\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    cuda_mem()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950f0175",
   "metadata": {},
   "source": [
    "## Zero-shot prompt builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c22dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04 - Minimal zero-shot prompt builder\n",
    "\n",
    "INSTR_ZS = (\n",
    "    \"Decide if the MWE is used idiomatically (figurative) in the Target sentence.\\n\"\n",
    "    \"Output only one digit: 1 (idiomatic) or 0 (literal).\"\n",
    ")\n",
    "\n",
    "def make_zero_shot_prompt(mwe: str, ctx_block: str) -> str:\n",
    "    return (\n",
    "        f\"{INSTR_ZS}\\n\"\n",
    "        f\"MWE: {mwe}\\n\"\n",
    "        f\"{ctx_block}\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a31ede",
   "metadata": {},
   "source": [
    "## Batched next-token logits classification (0/1 only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9cafb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05 - Batched next-token logits classification (0/1 only), CPU/GPU\n",
    "\n",
    "def _apply_chat_or_plain_batch(texts: List[str]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Turn raw prompts into input_ids + attention_mask.\n",
    "    \"\"\"\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        messages_batch = [[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful, concise assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": t}\n",
    "        ] for t in texts]\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages_batch,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "    else:\n",
    "        input_ids = tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        ).input_ids\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "_id0: Optional[int] = None\n",
    "_id1: Optional[int] = None\n",
    "\n",
    "def _candidate_token_id_for_digit(d: str) -> Optional[int]:\n",
    "    ids = tokenizer.encode(d, add_special_tokens=False)\n",
    "    if len(ids) == 1:\n",
    "        return ids[0]\n",
    "    ids = tokenizer.encode(\" \" + d, add_special_tokens=False)\n",
    "    if len(ids) == 1:\n",
    "        return ids[0]\n",
    "    ids = tokenizer.encode(d + \"\\n\", add_special_tokens=False)\n",
    "    if len(ids) == 1:\n",
    "        return ids[0]\n",
    "    return None\n",
    "\n",
    "def _init_digit_ids():\n",
    "    global _id0, _id1\n",
    "    if _id0 is None:\n",
    "        _id0 = _candidate_token_id_for_digit(\"0\")\n",
    "    if _id1 is None:\n",
    "        _id1 = _candidate_token_id_for_digit(\"1\")\n",
    "\n",
    "_init_digit_ids()\n",
    "\n",
    "def classify_prompts_logits(prompts: List[str]) -> List[int]:\n",
    "    enc = _apply_chat_or_plain_batch(prompts)\n",
    "\n",
    "    # Move inputs to the same device as the model\n",
    "    model_device = next(model.parameters()).device\n",
    "    enc = {k: v.to(model_device) for k, v in enc.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits  # [B, T, V]\n",
    "        next_logits = logits[:, -1, :]\n",
    "        if _id0 is not None and _id1 is not None:\n",
    "            logit0 = next_logits[:, _id0]\n",
    "            logit1 = next_logits[:, _id1]\n",
    "            return (logit1 >= logit0).long().detach().cpu().tolist()\n",
    "\n",
    "    # Fallback: generate one token and parse\n",
    "    outs: List[int] = []\n",
    "    with torch.no_grad():\n",
    "        gen = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=1,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    for i in range(gen.size(0)):\n",
    "        cut = enc[\"input_ids\"][i].shape[-1]\n",
    "        new_ids = gen[i][cut:]\n",
    "        text = tokenizer.decode(new_ids, skip_special_tokens=True)\n",
    "        if \"0\" in text and \"1\" in text:\n",
    "            outs.append(1 if text.index(\"1\") < text.index(\"0\") else 0)\n",
    "        elif \"1\" in text:\n",
    "            outs.append(1)\n",
    "        elif \"0\" in text:\n",
    "            outs.append(0)\n",
    "        else:\n",
    "            outs.append(1)\n",
    "    return outs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866cc0a6",
   "metadata": {},
   "source": [
    "## Caching helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22d90d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06 - Caching helpers\n",
    "\n",
    "from time import perf_counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def _load_cache(cache_path: Path) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Load a simple ID -> Label cache from CSV, if it exists.\n",
    "    \"\"\"\n",
    "    if cache_path.exists():\n",
    "        df = pd.read_csv(cache_path, dtype={\"ID\": str, \"Label\": int})\n",
    "        return dict(zip(df[\"ID\"].astype(str), df[\"Label\"].astype(int)))\n",
    "    return {}\n",
    "\n",
    "def _append_one(cache_path: Path, rec: Tuple[str, int]):\n",
    "    \"\"\"\n",
    "    Append a single (ID, Label) record to the cache CSV (creating header if needed).\n",
    "    \"\"\"\n",
    "    _id, _lab = rec\n",
    "    header_needed = not cache_path.exists()\n",
    "    with open(cache_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        if header_needed:\n",
    "            f.write(\"ID,Label\\n\")\n",
    "        f.write(f\"{_id},{int(_lab)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf769d01",
   "metadata": {},
   "source": [
    "## Zero-shot prediction for a single language (batched, resumable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b12a72ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 07 - Zero-shot prediction for a single language (batched, resumable)\n",
    "\n",
    "def progressive_predict_zero_shot_batched_for_language(\n",
    "    df_lang: pd.DataFrame,\n",
    "    language: str,\n",
    "    cache_path: Path,\n",
    "    desc: str = \"Zero-shot\",\n",
    ") -> List[int]:\n",
    "    df = df_lang.copy()\n",
    "    df[\"ID\"] = df[\"ID\"].astype(str)\n",
    "\n",
    "    preds_map = _load_cache(cache_path)\n",
    "    done = set(preds_map.keys())\n",
    "    todo_idx = [i for i, _id in enumerate(df[\"ID\"]) if _id not in done]\n",
    "\n",
    "    print(f\"{desc} [{language}] | Resuming with {len(done)} cached / {len(df)} total\")\n",
    "    t0 = perf_counter()\n",
    "\n",
    "    for start in tqdm(range(0, len(todo_idx), BATCH_GEN_ZERO), desc=f\"{desc} [{language}]\", leave=True):\n",
    "        batch_rows = todo_idx[start:start + BATCH_GEN_ZERO]\n",
    "        prompts: List[str] = []\n",
    "        ids: List[str] = []\n",
    "\n",
    "        for j in batch_rows:\n",
    "            r = df.iloc[j]\n",
    "            _id = r[\"ID\"]\n",
    "            mwe = r[\"MWE\"]\n",
    "            ctx = pack_context(\n",
    "                r.get(\"Previous\", \"\"),\n",
    "                r.get(\"Target\", \"\"),\n",
    "                r.get(\"Next\", \"\"),\n",
    "                mwe,\n",
    "            )\n",
    "            prompts.append(make_zero_shot_prompt(mwe, ctx))\n",
    "            ids.append(_id)\n",
    "\n",
    "        if not prompts:\n",
    "            continue\n",
    "\n",
    "        labels = classify_prompts_logits(prompts)\n",
    "        for _id, lab in zip(ids, labels):\n",
    "            preds_map[_id] = int(lab)\n",
    "            _append_one(cache_path, (_id, int(lab)))\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    elapsed = perf_counter() - t0\n",
    "    print(\n",
    "        f\"{desc} [{language}] | Newly computed: {len(todo_idx)} | Cached at start: {len(done)} | \"\n",
    "        f\"Total: {len(df)} | Elapsed: {elapsed:.1f}s | \"\n",
    "        f\"{(elapsed / max(1, len(todo_idx))):.3f}s/example (new only)\"\n",
    "    )\n",
    "\n",
    "    yhat = [preds_map[str(i)] for i in df[\"ID\"]]\n",
    "    return yhat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce25f3ac",
   "metadata": {},
   "source": [
    "## Run zero-shot evaluation for all languages and collect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52d9d318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing language: BN\n",
      "============================================================\n",
      "Zero-shot LLaMA [BN] | Resuming with 0 cached / 168 total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zero-shot LLaMA [BN]: 100%|██████████| 21/21 [00:05<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot LLaMA [BN] | Newly computed: 168 | Cached at start: 0 | Total: 168 | Elapsed: 5.9s | 0.035s/example (new only)\n",
      "\n",
      "[LLAMA Zero-shot BN] Macro-F1: 0.3940\n",
      "[LLAMA Zero-shot BN] Macro-recall: 0.6024\n",
      "[LLAMA Zero-shot BN] Accuracy: 0.3988\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2887    1.0000    0.4481        41\n",
      "           1     1.0000    0.2047    0.3399       127\n",
      "\n",
      "    accuracy                         0.3988       168\n",
      "   macro avg     0.6444    0.6024    0.3940       168\n",
      "weighted avg     0.8264    0.3988    0.3663       168\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 41   0]\n",
      " [101  26]]\n",
      "\n",
      "Saved predictions for BN to: outputs_lrl_llama/BN_llama_zeroshot_predictions.csv\n",
      "\n",
      "============================================================\n",
      "Processing language: ML\n",
      "============================================================\n",
      "Zero-shot LLaMA [ML] | Resuming with 0 cached / 50 total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zero-shot LLaMA [ML]: 100%|██████████| 7/7 [00:01<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot LLaMA [ML] | Newly computed: 50 | Cached at start: 0 | Total: 50 | Elapsed: 1.4s | 0.028s/example (new only)\n",
      "\n",
      "[LLAMA Zero-shot ML] Macro-F1: 0.3432\n",
      "[LLAMA Zero-shot ML] Macro-recall: 0.5048\n",
      "[LLAMA Zero-shot ML] Accuracy: 0.3600\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3023    0.8667    0.4483        15\n",
      "           1     0.7143    0.1429    0.2381        35\n",
      "\n",
      "    accuracy                         0.3600        50\n",
      "   macro avg     0.5083    0.5048    0.3432        50\n",
      "weighted avg     0.5907    0.3600    0.3011        50\n",
      "\n",
      "Confusion matrix:\n",
      "[[13  2]\n",
      " [30  5]]\n",
      "\n",
      "Saved predictions for ML to: outputs_lrl_llama/ML_llama_zeroshot_predictions.csv\n",
      "\n",
      "============================================================\n",
      "Processing language: PA\n",
      "============================================================\n",
      "Zero-shot LLaMA [PA] | Resuming with 0 cached / 50 total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zero-shot LLaMA [PA]: 100%|██████████| 7/7 [00:01<00:00,  6.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot LLaMA [PA] | Newly computed: 50 | Cached at start: 0 | Total: 50 | Elapsed: 1.1s | 0.023s/example (new only)\n",
      "\n",
      "[LLAMA Zero-shot PA] Macro-F1: 0.3064\n",
      "[LLAMA Zero-shot PA] Macro-recall: 0.3718\n",
      "[LLAMA Zero-shot PA] Accuracy: 0.3400\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3590    0.6364    0.4590        22\n",
      "           1     0.2727    0.1071    0.1538        28\n",
      "\n",
      "    accuracy                         0.3400        50\n",
      "   macro avg     0.3159    0.3718    0.3064        50\n",
      "weighted avg     0.3107    0.3400    0.2881        50\n",
      "\n",
      "Confusion matrix:\n",
      "[[14  8]\n",
      " [25  3]]\n",
      "\n",
      "Saved predictions for PA to: outputs_lrl_llama/PA_llama_zeroshot_predictions.csv\n",
      "\n",
      "Finished all languages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 08 - Run zero-shot evaluation for ALL languages and collect results\n",
    "\n",
    "all_results = []  # will store per-language metric summaries\n",
    "\n",
    "for lang in languages:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Processing language: {lang}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    df_lang = full_df[full_df[\"Language\"] == lang].copy()\n",
    "    if df_lang.empty:\n",
    "        print(f\"No rows for language {lang}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    if \"Label\" not in df_lang.columns:\n",
    "        print(f\"No 'Label' column for language {lang}, skipping (no metrics possible).\")\n",
    "        continue\n",
    "\n",
    "    cache_zero = OUT_DIR / f\"cache_llama_zeroshot_{lang}.csv\"\n",
    "\n",
    "    yhat = progressive_predict_zero_shot_batched_for_language(\n",
    "        df_lang,\n",
    "        language=lang,\n",
    "        cache_path=cache_zero,\n",
    "        desc=\"Zero-shot LLaMA\"\n",
    "    )\n",
    "\n",
    "    ytrue = df_lang[\"Label\"].astype(int).tolist()\n",
    "\n",
    "    f1_macro = f1_score(ytrue, yhat, average=\"macro\")\n",
    "    recall_macro = recall_score(ytrue, yhat, average=\"macro\")\n",
    "    acc = accuracy_score(ytrue, yhat)\n",
    "\n",
    "    print(f\"\\n[LLAMA Zero-shot {lang}] Macro-F1: {f1_macro:.4f}\")\n",
    "    print(f\"[LLAMA Zero-shot {lang}] Macro-recall: {recall_macro:.4f}\")\n",
    "    print(f\"[LLAMA Zero-shot {lang}] Accuracy: {acc:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(ytrue, yhat, digits=4))\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix(ytrue, yhat))\n",
    "\n",
    "    # Save per-language predictions\n",
    "    out_df = df_lang.copy()\n",
    "    out_df[\"PredLabel_ZeroShot\"] = yhat\n",
    "    pred_path = OUT_DIR / f\"{lang}_llama_zeroshot_predictions.csv\"\n",
    "    out_df.to_csv(pred_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"\\nSaved predictions for {lang} to: {pred_path}\")\n",
    "\n",
    "    all_results.append({\n",
    "        \"Language\": lang,\n",
    "        \"NumExamples\": len(df_lang),\n",
    "        \"MacroF1\": f1_macro,\n",
    "        \"MacroRecall\": recall_macro,\n",
    "        \"Accuracy\": acc,\n",
    "    })\n",
    "\n",
    "print(\"\\nFinished all languages.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d677fa1",
   "metadata": {},
   "source": [
    "## Show summary table and save metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb99b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary over all languages =====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>NumExamples</th>\n",
       "      <th>MacroF1</th>\n",
       "      <th>MacroRecall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BN</td>\n",
       "      <td>168</td>\n",
       "      <td>0.393978</td>\n",
       "      <td>0.602362</td>\n",
       "      <td>0.39881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ML</td>\n",
       "      <td>50</td>\n",
       "      <td>0.343186</td>\n",
       "      <td>0.504762</td>\n",
       "      <td>0.36000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PA</td>\n",
       "      <td>50</td>\n",
       "      <td>0.306431</td>\n",
       "      <td>0.371753</td>\n",
       "      <td>0.34000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Language  NumExamples   MacroF1  MacroRecall  Accuracy\n",
       "0       BN          168  0.393978     0.602362   0.39881\n",
       "1       ML           50  0.343186     0.504762   0.36000\n",
       "2       PA           50  0.306431     0.371753   0.34000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved summary CSV to: outputs_lrl_llama/llama_zeroshot_summary_all_languages.csv\n",
      "\n",
      "Resume cache files (delete to start fresh per language):\n",
      "- outputs_lrl_llama/cache_llama_zeroshot_BN.csv\n",
      "- outputs_lrl_llama/cache_llama_zeroshot_ML.csv\n",
      "- outputs_lrl_llama/cache_llama_zeroshot_PA.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 09 - Show summary table and save metadata\n",
    "\n",
    "if all_results:\n",
    "    summary_df = pd.DataFrame(all_results)\n",
    "    print(\"\\n===== Summary over all languages =====\")\n",
    "    display(summary_df)\n",
    "\n",
    "    summary_path = OUT_DIR / \"llama_zeroshot_summary_all_languages.csv\"\n",
    "    summary_df.to_csv(summary_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"\\nSaved summary CSV to: {summary_path}\")\n",
    "else:\n",
    "    print(\"No results collected (check dataset / labels).\")\n",
    "\n",
    "# Save simple run metadata\n",
    "with open(OUT_DIR / \"run_lrl_llama_zeroshot.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"MODEL_NAME={MODEL_NAME}\\n\")\n",
    "    f.write(f\"DEVICE={device.type}\\n\")\n",
    "    f.write(f\"BATCH_GEN_ZERO={BATCH_GEN_ZERO}\\n\")\n",
    "    f.write(f\"DATA={MULTI_CSV}\\n\")\n",
    "    if all_results:\n",
    "        for row in all_results:\n",
    "            f.write(\n",
    "                f\"LANG={row['Language']},N={row['NumExamples']},\"\n",
    "                f\"MacroF1={row['MacroF1']:.4f},\"\n",
    "                f\"MacroRecall={row['MacroRecall']:.4f},\"\n",
    "                f\"Acc={row['Accuracy']:.4f}\\n\"\n",
    "            )\n",
    "\n",
    "print(\"\\nResume cache files (delete to start fresh per language):\")\n",
    "for lang in languages:\n",
    "    cache_path = OUT_DIR / f\"cache_llama_zeroshot_{lang}.csv\"\n",
    "    if cache_path.exists():\n",
    "        print(f\"- {cache_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
