{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "363141e3",
   "metadata": {},
   "source": [
    "## Shared setup: device, imports, paths, utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b25f7cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA H100 80GB HBM3 MIG 2g.20gb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6029407/mhossai6/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE=16, NUM_EPOCHS=3, LR=2e-05\n"
     ]
    }
   ],
   "source": [
    "# 01 - Shared setup: device, imports, paths, utilities\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "# ---- Device selection ----\n",
    "RUN_DEVICE = \"gpu\"  # \"gpu\" or \"cpu\"\n",
    "\n",
    "if RUN_DEVICE.lower() == \"gpu\" and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.set_num_threads(max(1, os.cpu_count() // 2))\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# ---- Seeds ----\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ---- Metrics ----\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ---- HF transformers ----\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "# ---- Data paths (SemEval SubTaskA) ----\n",
    "DATA_DIR = Path(\"SemEval_2022_Task2-idiomaticity/SubTaskA\")\n",
    "TRAIN_ONE_SHOT = DATA_DIR / \"Data\" / \"train_one_shot.csv\"\n",
    "TRAIN_ZERO_SHOT = DATA_DIR / \"Data\" / \"train_zero_shot.csv\"\n",
    "DEV = DATA_DIR / \"Data\" / \"dev.csv\"\n",
    "DEV_GOLD = DATA_DIR / \"Data\" / \"dev_gold.csv\"\n",
    "EVAL = DATA_DIR / \"Data\" / \"eval.csv\"\n",
    "\n",
    "# ---- Outputs ----\n",
    "OUT_DIR = Path(\"outputs_pt_xlmr_en\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Model & training config ----\n",
    "MODEL_NAME = \"xlm-roberta-base\"\n",
    "NUM_LABELS = 2\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "LR = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "BATCH_SIZE_GPU = 16\n",
    "BATCH_SIZE_CPU = 8\n",
    "BATCH_SIZE = BATCH_SIZE_GPU if device.type == \"cuda\" else BATCH_SIZE_CPU\n",
    "\n",
    "print(f\"BATCH_SIZE={BATCH_SIZE}, NUM_EPOCHS={NUM_EPOCHS}, LR={LR}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Data loading & context utilities\n",
    "# =============================================================================\n",
    "\n",
    "def load_any_csv(path: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(path, sep=None, engine=\"python\", dtype=str)\n",
    "\n",
    "def ensure_label_int(df: pd.DataFrame, col=\"Label\") -> pd.DataFrame:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(int)\n",
    "    return df\n",
    "\n",
    "def mark_first_case_insensitive(text: str, needle: str, ltag=\"<mwe>\", rtag=\"</mwe>\") -> str:\n",
    "    if not isinstance(text, str) or not isinstance(needle, str):\n",
    "        return text\n",
    "    lt = text.lower()\n",
    "    ln = needle.lower()\n",
    "    i = lt.find(ln)\n",
    "    if i == -1:\n",
    "        return text\n",
    "    return text[:i] + ltag + text[i:i+len(needle)] + rtag + text[i+len(needle):]\n",
    "\n",
    "def pack_context(prev: str, target: str, nxt: str, mwe: str) -> str:\n",
    "    prev = \"\" if pd.isna(prev) else prev\n",
    "    nxt = \"\" if pd.isna(nxt) else nxt\n",
    "    target = \"\" if pd.isna(target) else target\n",
    "    tgt_marked = mark_first_case_insensitive(target, mwe)\n",
    "    return f\"Previous: {prev}\\nTarget: {tgt_marked}\\nNext: {nxt}\"\n",
    "\n",
    "def prepare_supervised_frame(\n",
    "    train_path: Path,\n",
    "    dev_path: Path,\n",
    "    dev_gold_path: Path,\n",
    "    language: str,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Prepare train/dev frames for a given language:\n",
    "    - Filter by Language column\n",
    "    - Merge dev with gold labels\n",
    "    - Build 'text' column using pack_context\n",
    "    \"\"\"\n",
    "    train_df = load_any_csv(train_path)\n",
    "    dev_df = load_any_csv(dev_path)\n",
    "    gold_df = load_any_csv(dev_gold_path)\n",
    "\n",
    "    # Strip column names\n",
    "    train_df.columns = [c.strip() for c in train_df.columns]\n",
    "    dev_df.columns = [c.strip() for c in dev_df.columns]\n",
    "    gold_df.columns = [c.strip() for c in gold_df.columns]\n",
    "\n",
    "    # Filter by language\n",
    "    train_df = train_df[train_df[\"Language\"] == language].copy()\n",
    "    dev_df = dev_df[dev_df[\"Language\"] == language].copy()\n",
    "\n",
    "    # Merge dev with gold labels\n",
    "    gold = gold_df[gold_df[\"Language\"] == language][[\"ID\", \"Label\"]].copy()\n",
    "    gold[\"ID\"] = gold[\"ID\"].astype(str)\n",
    "    dev_df[\"ID\"] = dev_df[\"ID\"].astype(str)\n",
    "    dev_lab = dev_df.merge(gold, on=\"ID\", how=\"left\")\n",
    "    dev_lab = ensure_label_int(dev_lab, \"Label\")\n",
    "    train_df = ensure_label_int(train_df, \"Label\")\n",
    "\n",
    "    # Build text column\n",
    "    def _build_text(row):\n",
    "        return pack_context(\n",
    "            row.get(\"Previous\", \"\"),\n",
    "            row.get(\"Target\", \"\"),\n",
    "            row.get(\"Next\", \"\"),\n",
    "            row.get(\"MWE\", \"\"),\n",
    "        )\n",
    "\n",
    "    train_df[\"text\"] = train_df.apply(_build_text, axis=1)\n",
    "    dev_lab[\"text\"] = dev_lab.apply(_build_text, axis=1)\n",
    "\n",
    "    return train_df, dev_lab\n",
    "\n",
    "def prepare_eval_frame(eval_path: Path, language: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare eval frame (no labels) for a given language.\n",
    "    \"\"\"\n",
    "    df = load_any_csv(eval_path)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    df = df[df[\"Language\"] == language].copy()\n",
    "\n",
    "    def _build_text(row):\n",
    "        return pack_context(\n",
    "            row.get(\"Previous\", \"\"),\n",
    "            row.get(\"Target\", \"\"),\n",
    "            row.get(\"Next\", \"\"),\n",
    "            row.get(\"MWE\", \"\"),\n",
    "        )\n",
    "\n",
    "    df[\"text\"] = df.apply(_build_text, axis=1)\n",
    "    return df\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset, model, dataloaders\n",
    "# =============================================================================\n",
    "\n",
    "class IdiomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer, max_length: int = 256, with_labels: bool = True):\n",
    "        self.texts = df[\"text\"].tolist()\n",
    "        self.with_labels = with_labels\n",
    "        self.labels = df[\"Label\"].tolist() if with_labels and \"Label\" in df.columns else None\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        text = self.texts[idx]\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "        if self.with_labels and self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "def build_model_and_tokenizer(move_to_device: bool = True):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # Add MWE markers if not present\n",
    "    special_tokens = {\"additional_special_tokens\": [\"<mwe>\", \"</mwe>\"]}\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "    )\n",
    "\n",
    "    # Resize embeddings for new special tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if move_to_device:\n",
    "        model.to(device)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def make_loader(df: pd.DataFrame, tokenizer, batch_size: int, shuffle: bool, max_length: int = 256, with_labels: bool = True):\n",
    "    ds = IdiomDataset(df, tokenizer, max_length=max_length, with_labels=with_labels)\n",
    "    return torch.utils.data.DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=2 if device.type == \"cuda\" else 0,\n",
    "    )\n",
    "\n",
    "# =============================================================================\n",
    "# Training & evaluation functions\n",
    "# =============================================================================\n",
    "\n",
    "def run_epoch(\n",
    "    model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    train: bool = True,\n",
    "    class_weights: torch.Tensor = None,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Run one epoch. Returns (avg_loss, macro_f1).\n",
    "    If train=False, no optimizer.step / scheduler.step is done.\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    if class_weights is not None:\n",
    "        class_weights = class_weights.to(device)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            if class_weights is not None:\n",
    "                log_probs = torch.log_softmax(logits, dim=-1)\n",
    "                loss = torch.nn.functional.nll_loss(\n",
    "                    log_probs,\n",
    "                    labels,\n",
    "                    weight=class_weights,\n",
    "                )\n",
    "            else:\n",
    "                loss = outputs.loss\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        all_y_true.extend(labels.detach().cpu().tolist())\n",
    "        all_y_pred.extend(preds.detach().cpu().tolist())\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(all_y_true))\n",
    "    macro_f1 = f1_score(all_y_true, all_y_pred, average=\"macro\")\n",
    "    return avg_loss, macro_f1\n",
    "\n",
    "def predict(model, dataloader) -> Tuple[List[int], List[int]]:\n",
    "    model.eval()\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            all_y_true.extend(labels.detach().cpu().tolist())\n",
    "            all_y_pred.extend(preds.detach().cpu().tolist())\n",
    "\n",
    "    return all_y_true, all_y_pred\n",
    "\n",
    "def predict_no_labels(model, dataloader) -> List[int]:\n",
    "    model.eval()\n",
    "    all_y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            all_y_pred.extend(preds.detach().cpu().tolist())\n",
    "\n",
    "    return all_y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bb5db7",
   "metadata": {},
   "source": [
    "## EN→PT ZERO-SHOT: train on EN train_zero_shot, test on PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8175647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EN→PT XLM-R (ZERO-SHOT SETTING: train_zero_shot.csv for EN)\n",
      "================================================================================\n",
      "EN (zero-shot) train size: 3327, EN dev size: 466\n",
      "EN (zero-shot) train label counts: {0: 1762, 1: 1565}\n",
      "EN (zero-shot) class weights: [0.9440976163450624, 1.0629392971246006]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 20:30:29.552979: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-26 20:30:39.863389: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764217841.258534 4132549 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764217842.128042 4132549 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764217845.533746 4132549 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764217845.533780 4132549 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764217845.533782 4132549 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764217845.533783 4132549 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-26 20:30:46.411537: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/3 (EN zero-shot training) ===\n",
      "[EN zero-shot train] loss=0.5996 macro-F1=0.6544\n",
      "[EN zero-shot dev]   loss=0.7342 macro-F1=0.7429\n",
      "New best EN dev macro-F1=0.7429 -> saved to outputs_pt_xlmr_en/xlmr_en_zero_shot_best.pt\n",
      "\n",
      "=== Epoch 2/3 (EN zero-shot training) ===\n",
      "[EN zero-shot train] loss=0.1871 macro-F1=0.9478\n",
      "[EN zero-shot dev]   loss=0.9633 macro-F1=0.7972\n",
      "New best EN dev macro-F1=0.7972 -> saved to outputs_pt_xlmr_en/xlmr_en_zero_shot_best.pt\n",
      "\n",
      "=== Epoch 3/3 (EN zero-shot training) ===\n",
      "[EN zero-shot train] loss=0.0737 macro-F1=0.9810\n",
      "[EN zero-shot dev]   loss=1.1441 macro-F1=0.7975\n",
      "New best EN dev macro-F1=0.7975 -> saved to outputs_pt_xlmr_en/xlmr_en_zero_shot_best.pt\n",
      "\n",
      "Best EN dev macro-F1 (zero-shot): 0.7975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded EN zero-shot checkpoint from: outputs_pt_xlmr_en/xlmr_en_zero_shot_best.pt\n",
      "PT dev size: 273, PT eval size: 279\n",
      "\n",
      "[EN-trained XLM-R (zero-shot) -> PT dev] macro-F1=0.6470\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7132    0.6299    0.6690       154\n",
      "           1     0.5839    0.6723    0.6250       119\n",
      "\n",
      "    accuracy                         0.6484       273\n",
      "   macro avg     0.6486    0.6511    0.6470       273\n",
      "weighted avg     0.6569    0.6484    0.6498       273\n",
      "\n",
      "[[97 57]\n",
      " [39 80]]\n",
      "\n",
      "Wrote PT eval submission (EN zero-shot) to: outputs_pt_xlmr_en/eval_submission_pt_xlmr_en_zero_shot.csv\n"
     ]
    }
   ],
   "source": [
    "# 02 - EN→PT ZERO-SHOT: train on EN train_zero_shot, test on PT\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EN→PT XLM-R (ZERO-SHOT SETTING: train_zero_shot.csv for EN)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ---- Prepare EN train/dev (zero-shot setting) ----\n",
    "train_en_0s, dev_en_0s = prepare_supervised_frame(\n",
    "    train_path=TRAIN_ZERO_SHOT,\n",
    "    dev_path=DEV,\n",
    "    dev_gold_path=DEV_GOLD,\n",
    "    language=\"EN\",\n",
    ")\n",
    "\n",
    "print(f\"EN (zero-shot) train size: {len(train_en_0s)}, EN dev size: {len(dev_en_0s)}\")\n",
    "\n",
    "# Class weights for EN zero-shot training\n",
    "label_counts_0s = train_en_0s[\"Label\"].value_counts().to_dict()\n",
    "print(\"EN (zero-shot) train label counts:\", label_counts_0s)\n",
    "total_0s = sum(label_counts_0s.values())\n",
    "class_weights_0s = [total_0s / (2.0 * label_counts_0s.get(i, 1)) for i in range(NUM_LABELS)]\n",
    "class_weights_0s_tensor = torch.tensor(class_weights_0s, dtype=torch.float)\n",
    "print(\"EN (zero-shot) class weights:\", class_weights_0s)\n",
    "\n",
    "# ---- Build model & tokenizer ----\n",
    "model_0s, tok_0s = build_model_and_tokenizer(move_to_device=True)\n",
    "\n",
    "# ---- EN dataloaders ----\n",
    "train_loader_en_0s = make_loader(\n",
    "    train_en_0s,\n",
    "    tok_0s,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    max_length=256,\n",
    "    with_labels=True,\n",
    ")\n",
    "dev_loader_en_0s = make_loader(\n",
    "    dev_en_0s,\n",
    "    tok_0s,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    max_length=256,\n",
    "    with_labels=True,\n",
    ")\n",
    "\n",
    "# ---- Optimizer & scheduler ----\n",
    "num_training_steps_0s = NUM_EPOCHS * len(train_loader_en_0s)\n",
    "optimizer_0s = torch.optim.AdamW(\n",
    "    model_0s.parameters(),\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "scheduler_0s = get_linear_schedule_with_warmup(\n",
    "    optimizer_0s,\n",
    "    num_warmup_steps=int(WARMUP_RATIO * num_training_steps_0s),\n",
    "    num_training_steps=num_training_steps_0s,\n",
    ")\n",
    "\n",
    "best_en_dev_f1_0s = -1.0\n",
    "best_path_en_0s = OUT_DIR / \"xlmr_en_zero_shot_best.pt\"\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{NUM_EPOCHS} (EN zero-shot training) ===\")\n",
    "    train_loss_0s, train_f1_0s = run_epoch(\n",
    "        model_0s,\n",
    "        train_loader_en_0s,\n",
    "        optimizer_0s,\n",
    "        scheduler_0s,\n",
    "        train=True,\n",
    "        class_weights=class_weights_0s_tensor,\n",
    "    )\n",
    "    print(f\"[EN zero-shot train] loss={train_loss_0s:.4f} macro-F1={train_f1_0s:.4f}\")\n",
    "\n",
    "    dev_loss_0s, dev_f1_0s = run_epoch(\n",
    "        model_0s,\n",
    "        dev_loader_en_0s,\n",
    "        optimizer=None,\n",
    "        scheduler=None,\n",
    "        train=False,\n",
    "        class_weights=None,\n",
    "    )\n",
    "    print(f\"[EN zero-shot dev]   loss={dev_loss_0s:.4f} macro-F1={dev_f1_0s:.4f}\")\n",
    "\n",
    "    if dev_f1_0s > best_en_dev_f1_0s:\n",
    "        best_en_dev_f1_0s = dev_f1_0s\n",
    "        torch.save(model_0s.state_dict(), best_path_en_0s)\n",
    "        print(f\"New best EN dev macro-F1={best_en_dev_f1_0s:.4f} -> saved to {best_path_en_0s}\")\n",
    "\n",
    "print(f\"\\nBest EN dev macro-F1 (zero-shot): {best_en_dev_f1_0s:.4f}\")\n",
    "\n",
    "# ---- Reload best EN zero-shot checkpoint ----\n",
    "model_0s_xfer, tok_0s_xfer = build_model_and_tokenizer(move_to_device=True)\n",
    "state_0s = torch.load(best_path_en_0s, map_location=device)\n",
    "model_0s_xfer.load_state_dict(state_0s)\n",
    "model_0s_xfer.to(device)\n",
    "model_0s_xfer.eval()\n",
    "print(f\"Loaded EN zero-shot checkpoint from: {best_path_en_0s}\")\n",
    "\n",
    "# ---- Prepare PT dev + eval ----\n",
    "_, dev_pt_0s = prepare_supervised_frame(\n",
    "    train_path=TRAIN_ZERO_SHOT,  # unused here; function needs it but we ignore train\n",
    "    dev_path=DEV,\n",
    "    dev_gold_path=DEV_GOLD,\n",
    "    language=\"PT\",\n",
    ")\n",
    "eval_pt_0s = prepare_eval_frame(EVAL, language=\"PT\")\n",
    "\n",
    "print(f\"PT dev size: {len(dev_pt_0s)}, PT eval size: {len(eval_pt_0s)}\")\n",
    "\n",
    "dev_loader_pt_0s = make_loader(\n",
    "    dev_pt_0s,\n",
    "    tok_0s_xfer,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    max_length=256,\n",
    "    with_labels=True,\n",
    ")\n",
    "eval_loader_pt_0s = make_loader(\n",
    "    eval_pt_0s,\n",
    "    tok_0s_xfer,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    max_length=256,\n",
    "    with_labels=False,\n",
    ")\n",
    "\n",
    "# ---- Evaluate EN-trained-zero-shot model on PT dev ----\n",
    "ytrue_pt_0s, ypred_pt_0s = predict(model_0s_xfer, dev_loader_pt_0s)\n",
    "pt_dev_f1_0s = f1_score(ytrue_pt_0s, ypred_pt_0s, average=\"macro\")\n",
    "print(f\"\\n[EN-trained XLM-R (zero-shot) -> PT dev] macro-F1={pt_dev_f1_0s:.4f}\")\n",
    "print(classification_report(ytrue_pt_0s, ypred_pt_0s, digits=4))\n",
    "print(confusion_matrix(ytrue_pt_0s, ypred_pt_0s))\n",
    "\n",
    "# ---- Predict on PT eval ----\n",
    "eval_preds_pt_0s = predict_no_labels(model_0s_xfer, eval_loader_pt_0s)\n",
    "\n",
    "sub_pt_0s = pd.DataFrame({\n",
    "    \"ID\": eval_pt_0s[\"ID\"].astype(str),\n",
    "    \"Language\": eval_pt_0s[\"Language\"],\n",
    "    \"Setting\": [\"en_trained_zero_shot\"] * len(eval_pt_0s),\n",
    "    \"Label\": eval_preds_pt_0s,\n",
    "})\n",
    "sub_pt_path_0s = OUT_DIR / \"eval_submission_pt_xlmr_en_zero_shot.csv\"\n",
    "sub_pt_0s.to_csv(sub_pt_path_0s, index=False)\n",
    "print(f\"\\nWrote PT eval submission (EN zero-shot) to: {sub_pt_path_0s}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c648f93b",
   "metadata": {},
   "source": [
    "## EN→PT ONE-SHOT: train on EN train_one_shot, test on PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11fb9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EN→PT XLM-R (ONE-SHOT SETTING: train_one_shot.csv for EN)\n",
      "================================================================================\n",
      "EN (one-shot) train size: 87, EN dev size: 466\n",
      "EN (one-shot) train label counts: {1: 55, 0: 32}\n",
      "EN (one-shot) class weights: [1.359375, 0.7909090909090909]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/3 (EN one-shot training) ===\n",
      "[EN one-shot train] loss=0.7264 macro-F1=0.3830\n",
      "[EN one-shot dev]   loss=0.6682 macro-F1=0.3787\n",
      "New best EN dev macro-F1=0.3787 -> saved to outputs_pt_xlmr_en/xlmr_en_one_shot_best.pt\n",
      "\n",
      "=== Epoch 2/3 (EN one-shot training) ===\n",
      "[EN one-shot train] loss=0.7150 macro-F1=0.4285\n",
      "[EN one-shot dev]   loss=0.6686 macro-F1=0.3787\n",
      "\n",
      "=== Epoch 3/3 (EN one-shot training) ===\n",
      "[EN one-shot train] loss=0.6974 macro-F1=0.4755\n",
      "[EN one-shot dev]   loss=0.6685 macro-F1=0.3787\n",
      "\n",
      "Best EN dev macro-F1 (one-shot): 0.3787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded EN one-shot checkpoint from: outputs_pt_xlmr_en/xlmr_en_one_shot_best.pt\n",
      "PT dev size: 273, PT eval size: 279\n",
      "\n",
      "[EN-trained XLM-R (one-shot) -> PT dev] macro-F1=0.3036\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       154\n",
      "           1     0.4359    1.0000    0.6071       119\n",
      "\n",
      "    accuracy                         0.4359       273\n",
      "   macro avg     0.2179    0.5000    0.3036       273\n",
      "weighted avg     0.1900    0.4359    0.2647       273\n",
      "\n",
      "[[  0 154]\n",
      " [  0 119]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6029407/mhossai6/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/project/6029407/mhossai6/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/project/6029407/mhossai6/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote PT eval submission (EN one-shot) to: outputs_pt_xlmr_en/eval_submission_pt_xlmr_en_one_shot.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 03 - EN→PT ONE-SHOT: train on EN train_one_shot, test on PT\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EN→PT XLM-R (ONE-SHOT SETTING: train_one_shot.csv for EN)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ---- Prepare EN train/dev (one-shot setting) ----\n",
    "train_en_1s, dev_en_1s = prepare_supervised_frame(\n",
    "    train_path=TRAIN_ONE_SHOT,\n",
    "    dev_path=DEV,\n",
    "    dev_gold_path=DEV_GOLD,\n",
    "    language=\"EN\",\n",
    ")\n",
    "\n",
    "print(f\"EN (one-shot) train size: {len(train_en_1s)}, EN dev size: {len(dev_en_1s)}\")\n",
    "\n",
    "# Class weights for EN one-shot training\n",
    "label_counts_1s = train_en_1s[\"Label\"].value_counts().to_dict()\n",
    "print(\"EN (one-shot) train label counts:\", label_counts_1s)\n",
    "total_1s = sum(label_counts_1s.values())\n",
    "class_weights_1s = [total_1s / (2.0 * label_counts_1s.get(i, 1)) for i in range(NUM_LABELS)]\n",
    "class_weights_1s_tensor = torch.tensor(class_weights_1s, dtype=torch.float)\n",
    "print(\"EN (one-shot) class weights:\", class_weights_1s)\n",
    "\n",
    "# ---- Build model & tokenizer ----\n",
    "model_1s, tok_1s = build_model_and_tokenizer(move_to_device=True)\n",
    "\n",
    "# ---- EN dataloaders ----\n",
    "train_loader_en_1s = make_loader(\n",
    "    train_en_1s,\n",
    "    tok_1s,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    max_length=256,\n",
    "    with_labels=True,\n",
    ")\n",
    "dev_loader_en_1s = make_loader(\n",
    "    dev_en_1s,\n",
    "    tok_1s,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    max_length=256,\n",
    "    with_labels=True,\n",
    ")\n",
    "\n",
    "# ---- Optimizer & scheduler ----\n",
    "num_training_steps_1s = NUM_EPOCHS * len(train_loader_en_1s)\n",
    "optimizer_1s = torch.optim.AdamW(\n",
    "    model_1s.parameters(),\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "scheduler_1s = get_linear_schedule_with_warmup(\n",
    "    optimizer_1s,\n",
    "    num_warmup_steps=int(WARMUP_RATIO * num_training_steps_1s),\n",
    "    num_training_steps=num_training_steps_1s,\n",
    ")\n",
    "\n",
    "best_en_dev_f1_1s = -1.0\n",
    "best_path_en_1s = OUT_DIR / \"xlmr_en_one_shot_best.pt\"\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{NUM_EPOCHS} (EN one-shot training) ===\")\n",
    "    train_loss_1s, train_f1_1s = run_epoch(\n",
    "        model_1s,\n",
    "        train_loader_en_1s,\n",
    "        optimizer_1s,\n",
    "        scheduler_1s,\n",
    "        train=True,\n",
    "        class_weights=class_weights_1s_tensor,\n",
    "    )\n",
    "    print(f\"[EN one-shot train] loss={train_loss_1s:.4f} macro-F1={train_f1_1s:.4f}\")\n",
    "\n",
    "    dev_loss_1s, dev_f1_1s = run_epoch(\n",
    "        model_1s,\n",
    "        dev_loader_en_1s,\n",
    "        optimizer=None,\n",
    "        scheduler=None,\n",
    "        train=False,\n",
    "        class_weights=None,\n",
    "    )\n",
    "    print(f\"[EN one-shot dev]   loss={dev_loss_1s:.4f} macro-F1={dev_f1_1s:.4f}\")\n",
    "\n",
    "    if dev_f1_1s > best_en_dev_f1_1s:\n",
    "        best_en_dev_f1_1s = dev_f1_1s\n",
    "        torch.save(model_1s.state_dict(), best_path_en_1s)\n",
    "        print(f\"New best EN dev macro-F1={best_en_dev_f1_1s:.4f} -> saved to {best_path_en_1s}\")\n",
    "\n",
    "print(f\"\\nBest EN dev macro-F1 (one-shot): {best_en_dev_f1_1s:.4f}\")\n",
    "\n",
    "# ---- Reload best EN one-shot checkpoint ----\n",
    "model_1s_xfer, tok_1s_xfer = build_model_and_tokenizer(move_to_device=True)\n",
    "state_1s = torch.load(best_path_en_1s, map_location=device)\n",
    "model_1s_xfer.load_state_dict(state_1s)\n",
    "model_1s_xfer.to(device)\n",
    "model_1s_xfer.eval()\n",
    "print(f\"Loaded EN one-shot checkpoint from: {best_path_en_1s}\")\n",
    "\n",
    "# ---- Prepare PT dev + eval ----\n",
    "_, dev_pt_1s = prepare_supervised_frame(\n",
    "    train_path=TRAIN_ONE_SHOT,  # unused; function signature\n",
    "    dev_path=DEV,\n",
    "    dev_gold_path=DEV_GOLD,\n",
    "    language=\"PT\",\n",
    ")\n",
    "eval_pt_1s = prepare_eval_frame(EVAL, language=\"PT\")\n",
    "\n",
    "print(f\"PT dev size: {len(dev_pt_1s)}, PT eval size: {len(eval_pt_1s)}\")\n",
    "\n",
    "dev_loader_pt_1s = make_loader(\n",
    "    dev_pt_1s,\n",
    "    tok_1s_xfer,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    max_length=256,\n",
    "    with_labels=True,\n",
    ")\n",
    "eval_loader_pt_1s = make_loader(\n",
    "    eval_pt_1s,\n",
    "    tok_1s_xfer,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    max_length=256,\n",
    "    with_labels=False,\n",
    ")\n",
    "\n",
    "# ---- Evaluate EN-trained-one-shot model on PT dev ----\n",
    "ytrue_pt_1s, ypred_pt_1s = predict(model_1s_xfer, dev_loader_pt_1s)\n",
    "pt_dev_f1_1s = f1_score(ytrue_pt_1s, ypred_pt_1s, average=\"macro\")\n",
    "print(f\"\\n[EN-trained XLM-R (one-shot) -> PT dev] macro-F1={pt_dev_f1_1s:.4f}\")\n",
    "print(classification_report(ytrue_pt_1s, ypred_pt_1s, digits=4))\n",
    "print(confusion_matrix(ytrue_pt_1s, ypred_pt_1s))\n",
    "\n",
    "# ---- Predict on PT eval ----\n",
    "eval_preds_pt_1s = predict_no_labels(model_1s_xfer, eval_loader_pt_1s)\n",
    "\n",
    "sub_pt_1s = pd.DataFrame({\n",
    "    \"ID\": eval_pt_1s[\"ID\"].astype(str),\n",
    "    \"Language\": eval_pt_1s[\"Language\"],\n",
    "    \"Setting\": [\"en_trained_one_shot\"] * len(eval_pt_1s),\n",
    "    \"Label\": eval_preds_pt_1s,\n",
    "})\n",
    "sub_pt_path_1s = OUT_DIR / \"eval_submission_pt_xlmr_en_one_shot.csv\"\n",
    "sub_pt_1s.to_csv(sub_pt_path_1s, index=False)\n",
    "print(f\"\\nWrote PT eval submission (EN one-shot) to: {sub_pt_path_1s}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
